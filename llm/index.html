
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../stablediffusion/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.36">
    
    
      
        <title>LLM - AIDIY Wiki</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.06209087.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#21" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AIDIY Wiki" class="md-header__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AIDIY Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  LLM

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../stablediffusion/" class="md-tabs__link">
        
  
    
  
  StableDiffusion

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vit/" class="md-tabs__link">
        
  
    
  
  ViT

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AIDIY Wiki" class="md-nav__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AIDIY Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LLM
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LLM
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 大模型 基础面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 大模型 基础面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#213" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 涌现能力是啥原因?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#214-llm" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.4 大模型LLM的架构介绍?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 大模型 进阶面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 大模型 进阶面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#222-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 什么是 LLMs 复读机问题?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 如何缓解 LLMs 复读机问题?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-bertllamachatglm" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#225" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.5 各个专业领域是否需要各自的大模型来服务?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#226" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.6 如何让大模型处理更长的文本?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 大模型 微调面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 大模型 微调面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.1 如果想要在某个模型基础上做全参数微调，究竟需要多少显存?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-sftllm" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.2 为什么SFT之后感觉LLM傻了?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.3 SFT 指令微调数据如何构建?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#234-continue-pretrain" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.4 领域模型Continue PreTrain 数据选取?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#235" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.5 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#236-continue-pretrain" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.6 领域模型Continue PreTrain ，如何让模型在预训练过程中就学习到更多的知识?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#237-sftchatbase" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.7 进行SFT操作的时候，基座模型选用Chat还是Base?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#238" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.8 领域模型微调 指令&amp;数据输入格式 要求?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2316" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.16 多轮对话任务如何微调模型?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2318" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.18 微调模型需要多大显存?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2319-llmsft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.19 大模型LLM进行SFT操作的时候在学习什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2320-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.20 预训练和SFT操作有什么不同?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2321-oom" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.21 样本量规模增大，训练出现OOM错误?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2322-llmsft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.22 大模型LLM进行SFT 如何对样本进行优化?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2323" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.23 模型参数迭代实验?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 大模型 langchain面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 大模型 langchain面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#242-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.2 LangChain 包含哪些核心概念?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#243-langchain-agent" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.3 什么是LangChain Agent?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#247-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.7 LangChain 包含哪些特点?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#248-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.8 LangChain 存在哪些问题及方法方案?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#249-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.9 LangChain 替代方案?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-llm" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 基于LLM+向量库的文档对话 面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-peft" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 大模型 参数高效微调(PEFT) 面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.6 大模型 参数高效微调(PEFT) 面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#266-adalora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.6 AdaLoRA 的思路是怎么样的?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#267-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.7 LoRA权重是否可以合入原模型?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#268-chatglm-6b-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.8 ChatGLM-6B LoRA后的权重多大?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#269-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.9 LoRA 微调优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2610-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.10 LoRA微调方法为啥能加速训练?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2611-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.11 如何在已有LoRA模型上继续训练?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2612-p-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.12 为什么需要 P-tuning?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2614-p-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.14 P-tuning 优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2615-p-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.15 P-tuning 缺点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2616-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.16 为什么需要 指示微调（Prompt-tuning）?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2617-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.17 指示微调（Prompt-tuning）思路是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2618-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.18 指示微调（Prompt-tuning）优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2619-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.19 指示微调（Prompt-tuning）缺点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2620-prompt-tuning-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.20 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2621-prompt-tuning-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.21 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2622-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.22 提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2623-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.23 为什么需要 前缀微调（Prefix-tuning）?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2624-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.24 前缀微调（Prefix-tuning）思路是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2625-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.25 前缀微调（Prefix-tuning）的优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2626-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.26 前缀微调（Prefix-tuning）的缺点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2627-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.27 为什么 需要 适配器微调（Adapter-tuning）?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2628-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.28 适配器微调（Adapter-tuning）思路?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2629-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.29 适配器微调（Adapter-tuning）特点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2630-adapterfusion" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.30 AdapterFusion 思路 是什么?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 大模型 推理面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.7 大模型 推理面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#273-int8fp16" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.3 推理速度上，int8和fp16比起来怎么样?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#274" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.4 大模型有推理能力吗?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#275" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.5 大模型生成时的参数怎么设置?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#277" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.7 如何让大模型输出合规化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#278" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.8 应用模式变更
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 大模型 评测面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.8 大模型 评测面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#281" class="md-nav__link">
    <span class="md-ellipsis">
      2.8.1 大模型怎么评测?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#282-honest" class="md-nav__link">
    <span class="md-ellipsis">
      2.8.2 大模型的honest原则是如何实现的?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#283" class="md-nav__link">
    <span class="md-ellipsis">
      2.8.3 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 大模型 强化学习面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.9 大模型 强化学习面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#292-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      2.9.2 RLHF 在实践过程中存在哪些不足?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#293" class="md-nav__link">
    <span class="md-ellipsis">
      2.9.3 如何解决 人工产生的偏好数据集成本较高，很难量产问题?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#294-sft-rm-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      2.9.4 如何解决三个阶段的训练（SFT-&gt;RM-&gt;PPO）过程较长，更新迭代较慢问题?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#210" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 大模型 软硬件配置面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.10 大模型 软硬件配置面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2101-ffn" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.1 介绍一下 FFN 块 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2102-gelu" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.2 介绍一下 GeLU 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2103-swish" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.3 介绍一下 Swish 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2104-glu-ffn" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2107-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.7 各LLMs 都使用哪种激活函数?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#211" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 大模型 训练集面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.11 大模型 训练集面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2111-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2.11.1 SFT（有监督微调）的数据集格式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2112-rm" class="md-nav__link">
    <span class="md-ellipsis">
      2.11.2 RM（奖励模型）的数据格式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2113-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      2.11.3 PPO（强化学习）的数据格式?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#212-token" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 Token及模型参数准备篇
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.12 Token及模型参数准备篇">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2122-sfttoken" class="md-nav__link">
    <span class="md-ellipsis">
      2.12.2 SFT需要训练Token数?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#213-alibi-attention-with-linear-biases" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 ALiBi (Attention with Linear Biases)篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#214-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 LLMs 位置编码篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#215" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 长度外推问题篇
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.15 长度外推问题篇">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2152" class="md-nav__link">
    <span class="md-ellipsis">
      2.15.2 长度外推问题 的 解决方法 有哪些?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#216-llms-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 LLMs Tokenizer 篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#217-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 Layer Normalization 篇
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablediffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StableDiffusion
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ViT
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 大模型 基础面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 大模型 基础面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#213" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 涌现能力是啥原因?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#214-llm" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.4 大模型LLM的架构介绍?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 大模型 进阶面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 大模型 进阶面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#222-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 什么是 LLMs 复读机问题?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 如何缓解 LLMs 复读机问题?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-bertllamachatglm" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#225" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.5 各个专业领域是否需要各自的大模型来服务?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#226" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.6 如何让大模型处理更长的文本?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 大模型 微调面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 大模型 微调面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.1 如果想要在某个模型基础上做全参数微调，究竟需要多少显存?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-sftllm" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.2 为什么SFT之后感觉LLM傻了?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.3 SFT 指令微调数据如何构建?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#234-continue-pretrain" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.4 领域模型Continue PreTrain 数据选取?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#235" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.5 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#236-continue-pretrain" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.6 领域模型Continue PreTrain ，如何让模型在预训练过程中就学习到更多的知识?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#237-sftchatbase" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.7 进行SFT操作的时候，基座模型选用Chat还是Base?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#238" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.8 领域模型微调 指令&amp;数据输入格式 要求?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2316" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.16 多轮对话任务如何微调模型?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2318" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.18 微调模型需要多大显存?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2319-llmsft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.19 大模型LLM进行SFT操作的时候在学习什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2320-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.20 预训练和SFT操作有什么不同?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2321-oom" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.21 样本量规模增大，训练出现OOM错误?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2322-llmsft" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.22 大模型LLM进行SFT 如何对样本进行优化?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2323" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.23 模型参数迭代实验?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 大模型 langchain面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 大模型 langchain面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#242-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.2 LangChain 包含哪些核心概念?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#243-langchain-agent" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.3 什么是LangChain Agent?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#247-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.7 LangChain 包含哪些特点?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#248-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.8 LangChain 存在哪些问题及方法方案?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#249-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.4.9 LangChain 替代方案?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-llm" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 基于LLM+向量库的文档对话 面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-peft" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 大模型 参数高效微调(PEFT) 面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.6 大模型 参数高效微调(PEFT) 面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#266-adalora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.6 AdaLoRA 的思路是怎么样的?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#267-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.7 LoRA权重是否可以合入原模型?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#268-chatglm-6b-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.8 ChatGLM-6B LoRA后的权重多大?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#269-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.9 LoRA 微调优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2610-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.10 LoRA微调方法为啥能加速训练?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2611-lora" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.11 如何在已有LoRA模型上继续训练?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2612-p-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.12 为什么需要 P-tuning?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2614-p-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.14 P-tuning 优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2615-p-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.15 P-tuning 缺点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2616-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.16 为什么需要 指示微调（Prompt-tuning）?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2617-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.17 指示微调（Prompt-tuning）思路是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2618-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.18 指示微调（Prompt-tuning）优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2619-prompt-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.19 指示微调（Prompt-tuning）缺点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2620-prompt-tuning-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.20 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2621-prompt-tuning-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.21 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2622-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.22 提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2623-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.23 为什么需要 前缀微调（Prefix-tuning）?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2624-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.24 前缀微调（Prefix-tuning）思路是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2625-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.25 前缀微调（Prefix-tuning）的优点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2626-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.26 前缀微调（Prefix-tuning）的缺点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2627-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.27 为什么 需要 适配器微调（Adapter-tuning）?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2628-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.28 适配器微调（Adapter-tuning）思路?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2629-adapter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.29 适配器微调（Adapter-tuning）特点是什么?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2630-adapterfusion" class="md-nav__link">
    <span class="md-ellipsis">
      2.6.30 AdapterFusion 思路 是什么?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 大模型 推理面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.7 大模型 推理面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#273-int8fp16" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.3 推理速度上，int8和fp16比起来怎么样?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#274" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.4 大模型有推理能力吗?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#275" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.5 大模型生成时的参数怎么设置?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#277" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.7 如何让大模型输出合规化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#278" class="md-nav__link">
    <span class="md-ellipsis">
      2.7.8 应用模式变更
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 大模型 评测面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.8 大模型 评测面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#281" class="md-nav__link">
    <span class="md-ellipsis">
      2.8.1 大模型怎么评测?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#282-honest" class="md-nav__link">
    <span class="md-ellipsis">
      2.8.2 大模型的honest原则是如何实现的?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#283" class="md-nav__link">
    <span class="md-ellipsis">
      2.8.3 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 大模型 强化学习面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.9 大模型 强化学习面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#292-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      2.9.2 RLHF 在实践过程中存在哪些不足?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#293" class="md-nav__link">
    <span class="md-ellipsis">
      2.9.3 如何解决 人工产生的偏好数据集成本较高，很难量产问题?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#294-sft-rm-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      2.9.4 如何解决三个阶段的训练（SFT-&gt;RM-&gt;PPO）过程较长，更新迭代较慢问题?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#210" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 大模型 软硬件配置面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.10 大模型 软硬件配置面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2101-ffn" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.1 介绍一下 FFN 块 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2102-gelu" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.2 介绍一下 GeLU 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2103-swish" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.3 介绍一下 Swish 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2104-glu-ffn" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2107-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.10.7 各LLMs 都使用哪种激活函数?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#211" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 大模型 训练集面
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.11 大模型 训练集面">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2111-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2.11.1 SFT（有监督微调）的数据集格式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2112-rm" class="md-nav__link">
    <span class="md-ellipsis">
      2.11.2 RM（奖励模型）的数据格式?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2113-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      2.11.3 PPO（强化学习）的数据格式?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#212-token" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 Token及模型参数准备篇
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.12 Token及模型参数准备篇">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2122-sfttoken" class="md-nav__link">
    <span class="md-ellipsis">
      2.12.2 SFT需要训练Token数?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#213-alibi-attention-with-linear-biases" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 ALiBi (Attention with Linear Biases)篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#214-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 LLMs 位置编码篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#215" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 长度外推问题篇
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.15 长度外推问题篇">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2152" class="md-nav__link">
    <span class="md-ellipsis">
      2.15.2 长度外推问题 的 解决方法 有哪些?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#216-llms-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 LLMs Tokenizer 篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#217-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 Layer Normalization 篇
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>LLM</h1>

<p><a href="https://github.com/Joining-AI/LLM_Interview_Prepare"><img alt="GitHub stars" src="https://img.shields.io/github/stars/Joining-AI/LLM_Interview_Prepare?style=social" /></a></p>
<h2 id="21">2.1 大模型 基础面<a class="headerlink" href="#21" title="Permanent link">&para;</a></h2>
<p>### 2.1.1 目前主流的开源模型体系有哪些?</p>
<p>目前主流的开源模型体系主要基于Transformer架构，这一架构在自然语言处理（NLP）领域取得了显著成果，并被广泛应用于各种任务和应用中。以下是当前主流的开源模型体系概述：</p>
<ol>
<li><strong>GPT（Generative Pre-trained Transformer）系列</strong>：</li>
<li>由OpenAI发布，包括GPT、GPT-2、GPT-3等模型。</li>
<li>通过在大规模无标签文本上进行预训练，然后在特定任务上进行微调，具有很强的生成能力和语言理解能力。</li>
<li>
<p>GPT系列模型在文本生成、问答系统、对话系统等领域表现出色。</p>
</li>
<li>
<p><strong>BERT（Bidirectional Encoder Representations from Transformers）系列</strong>：</p>
</li>
<li>由Google发布，是一种基于Transformer架构的双向预训练语言模型。</li>
<li>通过在大规模无标签文本上进行预训练，然后在下游任务上进行微调，具有强大的语言理解能力和表征能力。</li>
<li>
<p>BERT系列模型在文本分类、命名实体识别、问答系统等多个NLP任务中取得了显著效果。</p>
</li>
<li>
<p><strong>XLNet</strong>：</p>
</li>
<li>由卡内基梅隆大学（CMU）和Google Brain发布，是一种基于Transformer架构的自回归预训练语言模型。</li>
<li>通过自回归方式预训练，可以建模全局依赖关系，具有更好的语言建模能力和生成能力。</li>
<li>
<p>XLNet在多项NLP任务中展现了强大的性能。</p>
</li>
<li>
<p><strong>RoBERTa</strong>：</p>
</li>
<li>由Facebook发布，是在BERT基础上改进的预训练语言模型。</li>
<li>
<p>通过使用更大规模的数据和更长的训练时间，RoBERTa在多个NLP基准测试上取得了更好的性能。</p>
</li>
<li>
<p><strong>T5（Text-to-Text Transfer Transformer）</strong>：</p>
</li>
<li>由Google发布，是一种基于Transformer架构的多任务预训练语言模型。</li>
<li>通过在大规模数据集上进行预训练，T5可用于多种自然语言处理任务，如文本分类、机器翻译、问答等。</li>
<li>
<p>T5模型展示了强大的多任务处理能力。</p>
</li>
<li>
<p><strong>ChatGLM系列</strong>：</p>
</li>
<li>由清华大学发布，如ChatGLM-6B是一个开源的、支持中英双语问答的对话语言模型。</li>
<li>
<p>基于General Language Model（GLM）架构，具有强大的对话和生成能力。</p>
</li>
<li>
<p><strong>MOSS</strong>：</p>
</li>
<li>一个支持中英双语和多种插件的开源对话语言模型，具有160亿参数。</li>
<li>
<p>MOSS展示了在复杂对话任务中的高效性能。</p>
</li>
<li>
<p><strong>CPM-BEE</strong>：</p>
</li>
<li>完全开源且允许商用的百亿参数中英文基座模型。</li>
<li>
<p>采用Transformer自回归架构，使用万亿级高质量语料进行预训练。</p>
</li>
<li>
<p><strong>FlagOpen（飞智）</strong>：</p>
</li>
<li>由智源研究院发布的大模型技术开源体系，包括大模型算法、模型、数据、工具、评测等重要组成部分。</li>
<li>FlagOpen为研究者提供了丰富的资源和工具，以推动大模型技术的发展。</li>
</ol>
<p>### 2.1.2 prefix LM 和 causal LM 区别是什么?</p>
<p>Prefix LM（前缀语言模型）和Causal LM（因果语言模型）是两种不同类型的语言模型，它们在生成文本的方式、训练目标以及应用场景上存在一些关键区别。</p>
<ol>
<li>
<p>生成文本的方式</p>
</li>
<li>
<p><strong>Prefix LM</strong>：前缀语言模型是一种生成模型，它在生成每个词时都可以考虑之前的上下文信息。在生成时，前缀语言模型会根据给定的前缀（即部分文本序列）预测下一个可能的词。这种模型能够利用完整的上下文信息来生成文本，有助于生成更加准确和连贯的内容。解码器（Decoder）可以访问整个输入序列（包括前缀和之前生成的输出），从而更好地理解上下文。</p>
</li>
<li>
<p><strong>Causal LM</strong>：因果语言模型是一种自回归模型，它在生成文本时只能依赖于之前已经生成的文本，而不能利用未来信息。这种模型使用一种掩码（masking），确保在生成每个词时，只能考虑它之前（包括当前）的词，而不能“看”到未来的词。由于其自回归的特性，Causal LM在生成文本时可以逐步构建上下文，适用于长文本生成和需要逐步推理的场景。</p>
</li>
<li>
<p>训练目标</p>
</li>
<li>
<p><strong>Prefix LM</strong>：在训练时，前缀语言模型可能会使用到整个序列的信息来预测下一个词，这种训练方式使得模型能够更全面地理解上下文。</p>
</li>
<li>
<p><strong>Causal LM</strong>：在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种训练方式使得模型在生成文本时能够逐步累积信息，形成连贯的文本。</p>
</li>
<li>
<p>应用场景</p>
</li>
<li>
<p><strong>Prefix LM</strong>：由于Prefix LM能够利用完整的上下文信息来生成文本，因此它更适合于需要基于已有文本继续生成文本的任务，例如文本补全、续写故事等。</p>
</li>
<li>
<p><strong>Causal LM</strong>：由于其自回归的特性，Causal LM广泛用于需要生成新文本的任务，例如文本摘要、聊天机器人、语言生成等。在这些任务中，模型需要逐步构建文本，并且每一步的生成都依赖于前一步的结果。</p>
</li>
<li>
<p>解码方式</p>
</li>
<li>
<p><strong>Prefix LM</strong>：可以采用非自回归解码，即并行生成所有词，这有助于提高生成速度。</p>
</li>
<li><strong>Causal LM</strong>：则采用自回归解码，即一个词接一个词地生成，这种方式虽然速度较慢，但能够逐步构建上下文，生成更加连贯的文本。</li>
</ol>
<h3 id="213">2.1.3 涌现能力是啥原因?<a class="headerlink" href="#213" title="Permanent link">&para;</a></h3>
<p>以下是一些促成大模型涌现能力的关键因素：</p>
<ol>
<li>
<p><strong>模型规模</strong>：大模型拥有大量的参数，这使得它们能够捕捉和学习数据中的复杂模式和细微差别。</p>
</li>
<li>
<p><strong>数据多样性和量级</strong>：大模型通常在大规模和多样化的数据集上进行训练，这为它们提供了丰富的信息来学习语言的各种方面。</p>
</li>
<li>
<p><strong>注意力机制</strong>：Transformer架构中的注意力机制，特别是多头注意力，使得模型能够同时关注输入序列中的多个部分，这有助于理解上下文和执行复杂的语言任务。</p>
</li>
<li>
<p><strong>深度学习</strong>：大模型的深度（即层数）允许它们学习更高层次的抽象表示，这有助于处理语言的复杂性。</p>
</li>
<li>
<p><strong>预训练任务的设计</strong>：大模型通常使用语言建模作为预训练任务，这迫使模型学习如何生成连贯和有意义的文本。</p>
</li>
<li>
<p><strong>持续学习和微调</strong>：在预训练之后，大模型可以通过微调来适应特定的任务或领域，这一过程可能会激发或增强某些能力。</p>
</li>
<li>
<p><strong>架构创新</strong>：模型架构的创新，如更好的初始化方法、优化算法、正则化技术等，也有助于提高模型的性能和涌现能力。</p>
</li>
<li>
<p><strong>计算资源</strong>：大量的计算资源使得训练这些大模型成为可能，这是实现涌现能力的一个重要前提。</p>
</li>
</ol>
<p>涌现能力的例子包括但不限于：</p>
<ul>
<li><strong>上下文学习</strong>：模型能够通过观察少量示例来学习执行特定任务，而无需针对该任务进行显式训练。</li>
<li><strong>指令遵循</strong>：模型能够理解和遵循自然语言指令，执行相应的任务。</li>
<li><strong>复杂的推理能力</strong>：模型能够执行多步骤的推理，解决需要逻辑思考的问题。</li>
</ul>
<h3 id="214-llm">2.1.4 大模型LLM的架构介绍?<a class="headerlink" href="#214-llm" title="Permanent link">&para;</a></h3>
<p>大模型LLM（Large Language Model，大型语言模型）的架构通常基于Transformer模型，这是一种依赖自注意力机制来处理序列数据的深度学习模型。以下是对LLM架构的详细介绍：</p>
<p>一、基础架构</p>
<p><strong>1. Transformer模型</strong></p>
<ul>
<li><strong>自注意力机制</strong>：Transformer模型的核心是自注意力机制，它允许模型在处理每个单词时考虑到整个序列的上下文。这是通过查询（Q）、键（K）、值（V）三个组件实现的，每个单词都生成这些组件，并通过它们之间的相互作用来计算每个单词的加权表示，从而捕捉序列内部的长距离依赖。</li>
<li><strong>多头注意力</strong>：为了增强模型的表示能力，Transformer模型使用多头注意力，即并行地运行多个自注意力层，每个头学习输入的不同方面。</li>
<li><strong>位置编码</strong>：由于Transformer缺乏对序列顺序的固有感知，位置编码被添加到输入嵌入中，以提供单词在序列中的位置信息。</li>
<li><strong>前馈网络</strong>：每个Transformer层后跟一个前馈网络，通常由两个线性层和一个非线性激活函数组成，用于进一步处理自注意力层的输出。</li>
<li><strong>层标准化和残差连接</strong>：为了提高训练稳定性和解决深层网络中的梯度消失问题，Transformer模型使用层标准化和残差连接。残差连接允许每个子层的输出与其输入相加，然后进行层标准化。</li>
</ul>
<p><strong>2. 编码器-解码器架构</strong></p>
<ul>
<li>在某些LLM中，模型采用编码器-解码器架构。编码器处理输入序列，生成一系列表示，这些表示随后被解码器用来生成输出序列。</li>
<li>解码器的自注意力层会屏蔽未来的序列位置，以避免在生成过程中使用未来的信息。</li>
</ul>
<p>二、模型类型</p>
<p>LLM通常可以分为以下几种类型：</p>
<ul>
<li><strong>自回归模型</strong>：如GPT系列，采用经典的语言模型任务进行预训练，即给出上文，预测下文。这种模型在文本生成任务中表现出色。</li>
<li><strong>自编码模型</strong>：如BERT，采用句子重建的任务进行预训练，即预先通过某种方式破坏句子（如掩码或打乱顺序），然后希望模型将被破坏的部分还原。这种模型在自然语言理解任务中表现优异。</li>
<li><strong>序列到序列模型</strong>：如T5，同时使用了原始的编码器与解码器，适用于文本摘要、机器翻译等任务。</li>
</ul>
<p>三、训练过程</p>
<p>LLM的训练过程通常包括以下几个阶段：</p>
<ul>
<li><strong>数据收集和预处理</strong>：从互联网上收集大量文本数据，并进行清洗、格式化和分词等预处理工作。</li>
<li><strong>模型选择和配置</strong>：选择Transformer等适合的神经网络模型架构，并确定模型的大小（参数数量）和超参数。</li>
<li><strong>模型训练</strong>：在预处理过的文本数据上对选定的模型进行训练，使用反向传播和随机梯度下降等优化算法来调整模型的参数。由于大型模型的计算要求较高，训练通常需要在GPU或TPU等专用硬件上进行。</li>
<li><strong>评估和微调</strong>：使用各种指标对模型的性能进行评估，并根据需要进行微调以提高模型性能的特定方面。</li>
</ul>
<p>四、特点和优势</p>
<ul>
<li><strong>巨大的规模</strong>：LLM通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数，这使得它们能够捕捉更多的语言知识和复杂的语法结构。</li>
<li><strong>预训练和微调</strong>：LLM采用了预训练和微调的学习方法，首先在大规模文本数据上进行预训练以学习通用的语言表示和知识，然后通过微调适应特定任务。</li>
<li><strong>涌现能力</strong>：随着模型规模的扩大和数据量的增加，LLM展现出了一些惊人的涌现能力，即在未直接训练过的任务上表现出色。</li>
</ul>
<h2 id="22">2.2 大模型 进阶面<a class="headerlink" href="#22" title="Permanent link">&para;</a></h2>
<p>### 2.2.1 llama 输入句子长度理论上可以无限长吗?</p>
<p>LLaMA（Large Language Model Meta AI）是一种大型语言模型，它的输入句子长度并不是理论上无限长的，而是受到模型设计和计算资源的限制。以下是一些影响输入长度的因素：</p>
<ol>
<li>
<p><strong>上下文窗口大小</strong>：Transformer架构和LLaMA模型通常有一个固定的上下文窗口大小，例如512个token，这意味着模型在处理时只能考虑这么多的连续token。</p>
</li>
<li>
<p><strong>内存限制</strong>：每个token都需要在模型中分配内存以存储其嵌入表示，因此更长的输入句子会导致更高的内存需求。</p>
</li>
<li>
<p><strong>计算资源</strong>：处理更长的输入句子需要更多的计算资源，包括更多的算力和时间。</p>
</li>
<li>
<p><strong>注意力机制</strong>：Transformer模型使用自注意力机制，处理长序列时，计算复杂度会随着序列长度的增加而增加，这可能导致效率问题。</p>
</li>
<li>
<p><strong>优化策略</strong>：为了有效地训练和使用大型模型，可能需要采用特殊的优化策略，如层次化注意力或稀疏注意力，这些策略可能会限制模型处理的序列长度。</p>
</li>
<li>
<p><strong>实现细节</strong>：具体的实现可能会采用不同的技术来处理长序列，例如滑动窗口注意力机制或生成式解码策略，这些技术可以间接地处理更长的序列，但仍然有长度限制。</p>
</li>
<li>
<p><strong>数据结构</strong>：在某些情况下，模型可能使用特殊的数据结构来存储和处理长序列，但这些数据结构本身也可能有长度限制。</p>
</li>
<li>
<p><strong>模型架构</strong>：LLaMA或其他大型模型的架构设计可能会考虑到效率和实用性，选择一个平衡点来确定最大输入长度。</p>
</li>
</ol>
<h3 id="222-llms">2.2.2 什么是 LLMs 复读机问题?<a class="headerlink" href="#222-llms" title="Permanent link">&para;</a></h3>
<p>LLMs复读机问题指的是大型语言模型（LLMs，Large Language Models）在生成文本时出现的一种现象，即模型倾向于无限地复制输入的文本或者以过度频繁的方式重复相同的句子或短语。这种现象显著降低了模型输出的多样性和创造性，给用户带来了不佳的体验，倾向于重复或过度强调输入数据中的某些模式、观点或信息，而不是提供多样化或平衡的输出。这个问题在以下几个方面表现得尤为明显：</p>
<ol>
<li>
<p><strong>重复信息</strong>：模型可能会重复输入序列中的特定词汇或短语，尤其是在输入中这些词汇或短语出现的频率较高时。</p>
</li>
<li>
<p><strong>强化偏见</strong>：如果训练数据包含某种偏见，模型可能会学习并放大这些偏见，导致输出内容不够中立或公正。</p>
</li>
<li>
<p><strong>缺乏创新</strong>：模型可能倾向于生成与训练数据中相似的句子结构和表达方式，而不是创造新颖或独特的内容。</p>
</li>
<li>
<p><strong>过度依赖上下文</strong>：在某些情况下，模型可能过度依赖于给定的上下文或提示（prompt），导致生成的文本过于贴近这些上下文，缺乏独立性。</p>
</li>
<li>
<p><strong>风格一致性</strong>：模型可能在生成文本时过于模仿训练数据中的风格，而不是展现出多样化的表达风格。</p>
</li>
<li>
<p><strong>信息回声</strong>：在多轮对话或长文本生成中，模型可能会重复之前生成的内容，形成一种信息上的“回声”效应。</p>
</li>
</ol>
<p>### 2.2.3 为什么会出现 LLMs 复读机问题?</p>
<p>具体来说，LLMs复读机问题可以归因于以下几个方面：</p>
<ol>
<li>数据偏差</li>
</ol>
<p>大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。这种数据偏差是导致复读机问题的一个重要原因。</p>
<ol>
<li>训练目标的限制</li>
</ol>
<p>LLMs的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，从而增加了复读机问题的风险。</p>
<ol>
<li>缺乏多样性的训练数据</li>
</ol>
<p>尽管LLMs可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性。这同样会加剧复读机问题的出现。</p>
<ol>
<li>模型架构和生成策略</li>
</ol>
<p>除了数据和训练目标外，模型架构和生成策略也是影响复读机问题的重要因素。例如，某些生成策略可能过于保守，倾向于生成与已知文本相似的输出，从而增加了复读的可能性。</p>
<h3 id="224-llms">2.2.4 如何缓解 LLMs 复读机问题?<a class="headerlink" href="#224-llms" title="Permanent link">&para;</a></h3>
<p>解决LLMs的复读机问题通常需要采取以下措施：</p>
<ul>
<li><strong>多样化训练数据</strong>：确保训练数据的多样性和平衡性，避免模型学习到单一或有偏见的模式。</li>
<li><strong>正则化技术</strong>：使用不同的正则化技术来减少模型对训练数据的过度拟合。</li>
<li><strong>微调策略</strong>：在特定任务上微调模型，以减少复读现象并提高输出的多样性和相关性。</li>
<li><strong>干预和过滤</strong>：在生成过程中实施干预机制，如过滤掉重复或不相关的输出。</li>
<li><strong>评估和反馈</strong>：定期评估模型的输出质量，并根据反馈进行调整。</li>
<li><strong>引入噪声</strong>：在生成文本时引入随机性或噪声，以增加生成文本的多样性。</li>
<li><strong>温度参数调整</strong>：温度参数是用来控制生成文本多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性。</li>
<li><strong>后处理</strong>：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。</li>
</ul>
<p>通过这些方法，可以减少LLMs的复读机问题，提高模型生成文本的多样性和质量。</p>
<h3 id="224-bertllamachatglm">2.2.4 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选?<a class="headerlink" href="#224-bertllamachatglm" title="Permanent link">&para;</a></h3>
<p>选择使用BERT模型还是像LLaMA、ChatGLM这样的大型语言模型（LLMs），取决于具体的任务需求、资源可用性以及期望的性能。以下是一些指导原则，可以帮助你决定在不同情况下选择哪种模型：</p>
<p>使用BERT模型的情况：</p>
<ol>
<li><strong>任务对资源要求较低</strong>：如果你需要一个计算效率较高、资源消耗较少的模型，BERT可能是一个好选择。</li>
<li><strong>NLP基础任务</strong>：BERT擅长处理诸如文本分类、命名实体识别、问答系统等自然语言处理的基础任务。</li>
<li><strong>微调需求</strong>：BERT在微调后能够很好地适应特定任务，如果你需要在特定数据集上进行微调，BERT是一个成熟的选择。</li>
<li><strong>研究和教育目的</strong>：由于BERT的普及和广泛的研究基础，它适合用于教育目的和研究项目。</li>
<li><strong>模型解释性</strong>：如果你需要模型的可解释性，BERT及其变体由于其结构相对简单，通常更容易解释。</li>
</ol>
<p>使用LLaMA、ChatGLM等大型模型的情况：</p>
<ol>
<li><strong>复杂的语言理解</strong>：对于需要深层次语言理解和生成的复杂任务，如长文本生成、创意写作、高级对话系统等，大型模型可能更加适合。</li>
<li><strong>丰富的涌现能力</strong>：大型模型往往展现出更多的涌现能力，即在模型规模增大时出现的新能力，这些能力在小型模型中不常见。</li>
<li><strong>无需微调</strong>：对于没有足够数据进行微调的情况，大型模型可以在不微调的情况下提供较好的性能。</li>
<li><strong>多模态任务</strong>：如果任务涉及多种模态（如文本、图像等），大型模型可能更适合，因为它们可以处理更广泛的输入类型。</li>
<li><strong>资源充足</strong>：如果你有充足的计算资源，大型模型可以提供更高的性能，但也需要更多的计算和存储资源。</li>
</ol>
<p>如何选择：</p>
<ul>
<li><strong>任务需求</strong>：分析你的任务需求，确定是需要一个基础的NLP模型还是需要更高级的语言理解能力。</li>
<li><strong>资源评估</strong>：考虑你的计算资源和预算，大型模型通常需要更多的资源。</li>
<li><strong>性能目标</strong>：确定你期望的性能水平，以及不同模型在特定任务上的表现。</li>
<li><strong>数据可用性</strong>：评估可用于微调的数据量和质量，如果数据有限，可能需要一个能够处理少量数据的模型。</li>
<li><strong>部署环境</strong>：考虑模型部署的环境，包括硬件限制、延迟要求等。</li>
<li><strong>伦理和安全</strong>：评估模型可能产生的偏见和伦理问题，选择能够满足这些要求的模型。</li>
</ul>
<p>最后，实际选择可能还需要根据具体应用场景和实验结果来确定，有时候结合使用多个模型，或者使用一个模型作为另一个模型的补充，可能是最佳解决方案。</p>
<h3 id="225">2.2.5 各个专业领域是否需要各自的大模型来服务?<a class="headerlink" href="#225" title="Permanent link">&para;</a></h3>
<p>关于各个专业领域是否需要各自的大模型来服务，这是一个复杂且多维度的问题。以下是对这一问题的详细分析：</p>
<p>一、专业领域大模型的必要性</p>
<ol>
<li><strong>领域特定知识</strong>：</li>
<li>
<p>不同专业领域拥有各自独特的术语、概念和知识体系。一个针对特定领域训练的大模型能够更准确地理解和生成与该领域相关的文本，从而提高信息的准确性和相关性。</p>
</li>
<li>
<p><strong>数据可用性</strong>：</p>
</li>
<li>
<p>某些领域可能拥有大量的专业文本数据，这为训练专业领域的大模型提供了基础。利用这些数据训练出的模型能够更深入地理解该领域的细节和特征。</p>
</li>
<li>
<p><strong>应用需求</strong>：</p>
</li>
<li>
<p>专业领域内的具体应用需求可能需要定制化的模型来满足。例如，医疗领域的模型可能需要特别关注医学术语和诊断准确性，而法律领域的模型可能更擅长法律条文的解析和应用。</p>
</li>
<li>
<p><strong>性能优化</strong>：</p>
</li>
<li>
<p>专业领域的大模型可以在特定任务上进行优化，以更好地满足该领域的特定需求。这种优化可以提高模型的效率和准确性。</p>
</li>
<li>
<p><strong>合规性和伦理</strong>：</p>
</li>
<li>
<p>某些领域如医疗、金融等，对模型的合规性、隐私保护和伦理要求较高。特定设计的模型可以更好地满足这些要求，确保数据的安全和合规使用。</p>
</li>
<li>
<p><strong>用户接受度</strong>：</p>
</li>
<li>领域专家和用户可能更倾向于使用那些能够理解他们专业术语和概念的模型。这有助于提高用户满意度和信任度。</li>
</ol>
<p>二、通用大模型的优势</p>
<ol>
<li><strong>通用性</strong>：</li>
<li>
<p>通用大模型可以处理各种类型的查询，而不仅仅是特定领域的查询。这使得它们具有更广泛的应用范围。</p>
</li>
<li>
<p><strong>处理未知或罕见查询</strong>：</p>
</li>
<li>
<p>由于通用大模型接受了大量和多样的训练数据，它们可能在处理未知或罕见查询时表现得更好。这种能力对于跨领域的任务尤为重要。</p>
</li>
<li>
<p><strong>维护和更新</strong>：</p>
</li>
<li>通用大模型可能更容易维护和更新。因为它们不需要针对每个领域进行特定的训练和优化，所以更新和维护的成本相对较低。</li>
</ol>
<p>三、综合考量</p>
<ul>
<li>对于那些拥有大量专业数据、高度专业化需求、以及对准确性和合规性要求极高的领域，开发和使用各自的大模型可能是有益的。这些模型能够针对特定领域进行深度优化，提供更高质量的服务。</li>
<li>对于数据较少或需求不是特别专业化的领域，可能可以通过使用通用模型并通过迁移学习等技术来适应特定需求。这样可以减少开发和维护成本，同时利用通用模型的强大能力。</li>
</ul>
<h3 id="226">2.2.6 如何让大模型处理更长的文本?<a class="headerlink" href="#226" title="Permanent link">&para;</a></h3>
<p>处理更长文本的能力对于大型语言模型（LLMs）来说是一个重要的考量，因为许多应用场景，如文档摘要、长篇内容生成或分析等，都需要理解和生成较长的文本序列。以下是一些方法和策略，可以帮助大模型处理更长的文本：</p>
<ol>
<li>
<p><strong>增加上下文窗口</strong>：扩大模型的上下文窗口大小，允许模型一次性处理更多的输入。这通常需要模型架构和硬件支持更大的序列长度。</p>
</li>
<li>
<p><strong>滑动窗口技术</strong>：使用滑动窗口方法处理文本，即每次只处理文本的一部分，然后逐步移动窗口来处理整个文本。</p>
</li>
<li>
<p><strong>层次化注意力机制</strong>：采用层次化或分层的注意力机制，允许模型在不同层次上处理信息，从而更有效地处理长文本。</p>
</li>
<li>
<p><strong>稀疏注意力模式</strong>：使用稀疏注意力模式，如局部感知或相对位置编码，减少计算量，使模型能够处理更长的序列。</p>
</li>
<li>
<p><strong>缓存和记忆网络</strong>：利用缓存或记忆网络存储和检索长文本中的关键信息，帮助模型在生成时保持连贯性。</p>
</li>
<li>
<p><strong>分块处理</strong>：将长文本分割成小块，分别处理每个块，然后合并结果。这种方法需要确保块与块之间的边界平滑过渡。</p>
</li>
<li>
<p><strong>迭代细化</strong>：通过迭代过程逐步构建输出，每次迭代都在之前的基础上进行细化，直到达到所需的文本长度。</p>
</li>
<li>
<p><strong>条件生成</strong>：在生成文本时，使用条件生成技术，如给定特定的提示或约束条件，指导模型生成特定长度的文本。</p>
</li>
<li>
<p><strong>多阶段处理</strong>：将文本处理任务分解为多个阶段，每个阶段处理文本的一部分，最终将各阶段的结果整合起来。</p>
</li>
<li>
<p><strong>模型并行和数据并行</strong>：采用模型并行和数据并行技术，将模型的不同部分或数据的不同片段分布到多个计算设备上，以处理更长的文本。</p>
</li>
<li>
<p><strong>优化算法</strong>：使用高效的优化算法和学习率调度策略，帮助模型在训练和微调过程中更好地学习长文本的特征。</p>
</li>
<li>
<p><strong>长短期记忆</strong>：利用长短期记忆（LSTM）或门控循环单元（GRU）等循环神经网络结构，帮助模型记住长文本中的关键信息。</p>
</li>
<li>
<p><strong>知识蒸馏</strong>：通过知识蒸馏技术，将大型模型的知识迁移到更小的模型中，使小模型能够处理长文本并保持性能。</p>
</li>
<li>
<p><strong>预训练和微调</strong>：在大量长文本数据上进行预训练，然后在特定任务上进行微调，以提高模型处理长文本的能力。</p>
</li>
</ol>
<h2 id="23">2.3 大模型 微调面<a class="headerlink" href="#23" title="Permanent link">&para;</a></h2>
<h3 id="231">2.3.1 如果想要在某个模型基础上做全参数微调，究竟需要多少显存?<a class="headerlink" href="#231" title="Permanent link">&para;</a></h3>
<p>在某个模型基础上进行全参数微调所需的显存量取决于多个因素，包括模型的大小、批量大小（batch size）、序列长度、以及所使用的硬件（如GPU的显存容量）。以下是一些关键因素和计算显存需求的方法：</p>
<ol>
<li>
<p><strong>模型大小</strong>：模型的参数数量直接影响所需的显存。大型模型如BERT、GPT等有数亿到数千亿参数，需要的显存自然更多。</p>
</li>
<li>
<p><strong>批量大小</strong>：批量大小是指每次训练迭代中同时处理的样本数量。批量大小越大，单次迭代所需的显存越多。</p>
</li>
<li>
<p><strong>序列长度</strong>：模型处理的序列越长，每个样本所需的显存就越多。</p>
</li>
<li>
<p><strong>优化器</strong>：不同的优化器（如Adam、SGD等）需要存储不同数量的状态信息，这也会影响显存的使用。</p>
</li>
<li>
<p><strong>GPU显存</strong>：可用的GPU显存大小限制了可以加载的模型和批量大小。如果显存不足，可能需要减小批量大小或使用模型并行技术。</p>
</li>
<li>
<p><strong>数据类型</strong>：参数和梯度的数据类型（如float32、float16等）也会影响显存需求。使用较低精度的数据类型可以减少显存使用，但可能会影响模型性能。</p>
</li>
<li>
<p><strong>其他因素</strong>：包括激活函数、层的类型、模型的深度等。</p>
</li>
</ol>
<p>要估算所需的显存，可以使用以下简化的公式来估算单次迭代的显存需求：
[ \text{显存需求} \approx (\text{模型参数数量} + \text{优化器状态}) \times \text{数据类型大小} \times \text{批量大小} ]</p>
<p>例如，如果使用float32数据类型，每个参数需要4字节，一个具有1亿参数的模型在批量大小为32的情况下，仅模型参数就需要：
[ 100,000,000 \text{ 参数} \times 4 \text{ 字节/参数} \times 32 \text{ 批量大小} = 128 \text{ MB} ]</p>
<p>这还没有考虑优化器状态、梯度信息、以及其他中间数据的存储需求。实际的显存需求可能会更高。</p>
<p>在实际操作中，可以使用深度学习框架提供的工具来检查模型的显存占用情况，如PyTorch的<code>torch.cuda.memory_allocated()</code>函数，或者TensorFlow的<code>tf.config.experimental.get_memory_info()</code>函数。</p>
<p>如果显存不足，可以考虑以下解决方案：
- 减小批量大小。
- 使用模型并行或数据并行技术。
- 优化模型结构，减少参数数量。
- 使用较低精度的数据类型。
- 使用显存优化技术，如梯度累积或混合精度训练。</p>
<h3 id="232-sftllm">2.3.2 为什么SFT之后感觉LLM傻了?<a class="headerlink" href="#232-sftllm" title="Permanent link">&para;</a></h3>
<p>SFT（Supervised Fine-Tuning，监督式微调）是一种常见的技术，用于使语言模型（如LLMs，大型语言模型）适应特定的任务或领域。然而，在SFT之后，有时可能会感觉LLMs的性能有所下降，或者表现得不如以前"聪明"。这种感觉可能由以下原因引起：</p>
<ol>
<li>
<p><strong>过拟合</strong>：如果微调的数据集不够代表性或规模较小，模型可能会过度学习这些数据中的特定模式和噪声，导致在新数据上泛化能力下降。</p>
</li>
<li>
<p><strong>任务适应性</strong>：微调可能会使模型更专注于特定任务，从而在执行与微调任务不相关的问题时，性能可能不如之前。</p>
</li>
<li>
<p><strong>数据偏差</strong>：微调数据可能包含偏差，这可能导致模型学习到这些偏差，并在生成文本时反映出来，造成输出质量下降。</p>
</li>
<li>
<p><strong>知识遗忘</strong>：在微调过程中，模型可能会遗忘一些在预训练阶段学到的通用知识，这种现象称为灾难性遗忘。</p>
</li>
<li>
<p><strong>微调策略</strong>：微调的策略和参数选择（如学习率、微调步数等）可能不适当，导致模型未能有效学习任务相关的特征。</p>
</li>
<li>
<p><strong>评估偏差</strong>：如果在不恰当的任务或数据集上评估微调后的模型，可能会得到性能下降的印象。确保评估任务与微调任务的相关性很重要。</p>
</li>
<li>
<p><strong>期望过高</strong>：有时，用户对微调后的模型期望过高，而实际上模型可能只是从一种类型的智能转变为另一种更特定领域的智能。</p>
</li>
<li>
<p><strong>模型容量</strong>：如果微调的模型容量不足以捕捉特定任务的复杂性，模型可能无法表现出预期的性能。</p>
</li>
<li>
<p><strong>长期依赖</strong>：语言模型在处理需要长期依赖信息的任务时可能面临挑战，微调可能未能解决这些问题。</p>
</li>
<li>
<p><strong>评估方法</strong>：评估方法可能未能全面捕捉模型的性能。例如，仅使用准确性可能无法反映模型在生成连贯和自然文本方面的能力。</p>
</li>
</ol>
<p>为了提高SFT后LLMs的性能，可以考虑以下策略：</p>
<ul>
<li>使用更大规模、更多样化的微调数据集。</li>
<li>采用数据增强和清洗技术减少数据偏差。</li>
<li>采用适当的微调策略，如学习率调度、早停（early stopping）等。</li>
<li>在多个任务和数据集上评估模型，以获得全面的性能评估。</li>
<li>结合多种微调技术，如多任务学习或元学习。</li>
</ul>
<p>通过这些方法，可以提高LLMs在特定任务上的性能，同时保持其通用的语言理解能力。</p>
<h3 id="233-sft">2.3.3 SFT 指令微调数据如何构建?<a class="headerlink" href="#233-sft" title="Permanent link">&para;</a></h3>
<p>SFT（Supervised Fine-Tuning，监督式微调）指令微调数据的构建是为了让语言模型适应特定的任务或指令集，以下是构建这类数据集的一般步骤：</p>
<ol>
<li>
<p><strong>确定任务类型</strong>：首先明确你希望模型完成的任务类型，如文本分类、情感分析、问答、摘要、翻译等。</p>
</li>
<li>
<p><strong>收集数据</strong>：根据任务类型收集数据。这些数据可以是已经标注好的，也可以是需要进一步标注的原始文本。</p>
</li>
<li>
<p><strong>定义指令集</strong>：为模型定义一组指令词汇，这些指令将指导模型完成特定任务。</p>
</li>
<li>
<p><strong>构建Prompt</strong>：设计有效的Prompt（提示），Prompt是给模型的输入的一部分，用来引导模型执行特定任务。例如，"Summarize the following text:"（总结以下文本：）。</p>
</li>
<li>
<p><strong>标注数据</strong>：如果使用未标注的数据，需要进行标注。标注可以是类别标签、实体标记、问答对、翻译结果等。</p>
</li>
<li>
<p><strong>格式化数据</strong>：将数据和指令结合，形成模型可以理解的输入格式。例如，将Prompt和输入文本拼接在一起。</p>
</li>
<li>
<p><strong>多样性</strong>：确保数据集具有多样性，涵盖不同的领域、风格和复杂度，以提高模型的泛化能力。</p>
</li>
<li>
<p><strong>避免偏差</strong>：检查数据集中的潜在偏差，并采取措施以减少这些偏差对模型性能的影响。</p>
</li>
<li>
<p><strong>划分数据集</strong>：将数据集划分为训练集、验证集和测试集，以评估模型在不同阶段的性能。</p>
</li>
<li>
<p><strong>数据增强</strong>：使用数据增强技术，如同义词替换、句子重组、文本截断等，来增加数据集的多样性和丰富性。</p>
</li>
<li>
<p><strong>质量控制</strong>：对标注的数据进行质量控制，确保数据的准确性和一致性。</p>
</li>
<li>
<p><strong>反馈循环</strong>：在初步微调后，使用测试集或其他评估方法来评估模型性能，根据反馈进一步优化数据集。</p>
</li>
<li>
<p><strong>迭代改进</strong>：微调是一个迭代过程，可能需要多次调整Prompt和数据集来获得最佳性能。</p>
</li>
<li>
<p><strong>考虑任务特定性</strong>：对于特定任务，可能需要收集或生成特定类型的数据，如对话数据、专业领域文本等。</p>
</li>
<li>
<p><strong>使用现有资源</strong>：利用现有的数据集和资源，如公共数据集、API、在线论坛等，可以加速数据集构建过程。</p>
</li>
</ol>
<h3 id="234-continue-pretrain">2.3.4 领域模型Continue PreTrain 数据选取?<a class="headerlink" href="#234-continue-pretrain" title="Permanent link">&para;</a></h3>
<p>领域模型Continue PreTrain（持续预训练）的数据选取是一个复杂而关键的过程，它直接影响模型在特定领域上的性能和泛化能力。以下是关于领域模型Continue PreTrain数据选取的详细分析：</p>
<p>一、数据选取原则</p>
<ol>
<li><strong>领域特定性</strong>：</li>
<li>
<p>选择与特定领域紧密相关的数据，这些数据应包含领域特有的术语、概念和情境。</p>
</li>
<li>
<p><strong>数据覆盖度</strong>：</p>
</li>
<li>
<p>确保数据集覆盖了领域内的各种情况和案例，包括不同的使用场景和用户行为。</p>
</li>
<li>
<p><strong>数据质量</strong>：</p>
</li>
<li>
<p>选取的数据应准确、干净，避免包含错误或不完整的信息，这些都会降低模型训练的效果。</p>
</li>
<li>
<p><strong>数据多样性</strong>：</p>
</li>
<li>
<p>数据集应包含多样化的数据类型（如文本、图像、音频等）和不同的数据来源及风格，以丰富模型的训练环境。</p>
</li>
<li>
<p><strong>数据平衡性</strong>：</p>
</li>
<li>
<p>在分类任务中，确保各类别样本的数量相对均衡，以避免模型对某些类别的过度偏好。</p>
</li>
<li>
<p><strong>数据代表性</strong>：</p>
</li>
<li>
<p>数据集应代表目标用户群体和实际使用情况，以提高模型的泛化能力。</p>
</li>
<li>
<p><strong>数据时效性</strong>：</p>
</li>
<li>
<p>选择最新的数据，特别是在快速变化的领域，以确保模型能够适应当前的趋势和需求。</p>
</li>
<li>
<p><strong>数据合规性</strong>：</p>
</li>
<li>确保数据的收集和使用符合相关的法律法规，包括隐私保护和数据安全。</li>
</ol>
<p>二、数据选取方法</p>
<ol>
<li><strong>收集目标领域数据</strong>：</li>
<li>
<p>从互联网、特定领域的文档、公司内部数据库等多种渠道收集与目标领域紧密相关的数据。</p>
</li>
<li>
<p><strong>领域专家标注</strong>：</p>
</li>
<li>
<p>如果有领域专家可用，可以请他们对领域相关的数据进行标注。标注内容可以包括分类、命名实体识别、关系抽取等任务，以提供有监督学习的训练集。</p>
</li>
<li>
<p><strong>自动化标注</strong>：</p>
</li>
<li>
<p>在没有领域专家或标注成本较高的情况下，可以使用预训练的模型对领域相关数据进行自动化标注，生成伪标签。虽然伪标签的准确性可能不如人工标注，但在一定程度上仍可用于模型的训练。</p>
</li>
<li>
<p><strong>数据平衡</strong>：</p>
</li>
<li>
<p>注意各类别数据的平衡性。如果某个类别的数据样本较少，可以考虑使用数据增强技术或对该类别进行过采样，以平衡各个类别的数据量。</p>
</li>
<li>
<p><strong>数据质量控制</strong>：</p>
</li>
<li>
<p>在选取数据之前，需要对数据的质量进行评估。使用准确性、一致性等质量评估指标来筛选和过滤低质量的数据。</p>
</li>
<li>
<p><strong>数据预处理</strong>：</p>
</li>
<li>对数据进行必要的预处理，如分词、去除停用词、标准化等，以准备好输入模型进行训练。</li>
</ol>
<p>三、数据选取策略</p>
<ol>
<li><strong>持续更新</strong>：</li>
<li>
<p>建立机制以定期更新数据集，以适应领域的变化和发展。</p>
</li>
<li>
<p><strong>反馈循环</strong>：</p>
</li>
<li>
<p>建立反馈机制，根据模型在实际应用中的表现，不断调整和优化数据选取策略。</p>
</li>
<li>
<p><strong>数据去偏</strong>：</p>
</li>
<li>
<p>识别并减少数据集中的偏见，确保模型不会学习到歧视性或不公平的模式。</p>
</li>
<li>
<p><strong>数据增强</strong>：</p>
</li>
<li>
<p>通过技术手段增加数据集的多样性，如图像增强、文本数据的变体生成等，以提高模型的泛化能力。</p>
</li>
<li>
<p><strong>数据集分割</strong>：</p>
</li>
<li>合理分割数据集为训练集、验证集和测试集，以便于模型评估和避免过拟合。</li>
</ol>
<h3 id="235">2.3.5 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?<a class="headerlink" href="#235" title="Permanent link">&para;</a></h3>
<p>领域数据训练后，模型可能会在一定程度上遗忘其通用能力，这种现象被称为"灾难性遗忘"（Catastrophic Forgetting）。以下是一些缓解模型灾难性遗忘的策略：</p>
<ol>
<li>
<p><strong>多任务学习</strong>：在训练过程中同时考虑多个任务，这有助于模型学习到更为通用的特征表示。</p>
</li>
<li>
<p><strong>正则化技术</strong>：使用如L2正则化等技术，帮助模型在适应新数据时保持原有的知识。</p>
</li>
<li>
<p><strong>课程学习</strong>：按照从易到难的顺序进行训练，先使用通用数据训练，再逐步引入领域特定数据。</p>
</li>
<li>
<p><strong>弹性权重共享</strong>：通过共享预训练模型中的权重，减少对原始知识的覆盖。</p>
</li>
<li>
<p><strong>记忆回放</strong>：在训练过程中周期性地重新引入一些原始的通用数据，帮助模型回顾和保持通用知识。</p>
</li>
<li>
<p><strong>增量学习</strong>：采用增量学习的方法，逐步添加新的知识，而不是完全替换旧的知识。</p>
</li>
<li>
<p><strong>知识蒸馏</strong>：将预训练模型的知识通过蒸馏的方式传递给新的模型，即使模型在领域数据上进行微调。</p>
</li>
<li>
<p><strong>中间层冻结</strong>：在微调过程中，冻结模型的某些中间层，这些层通常包含更通用的特征。</p>
</li>
<li>
<p><strong>注意力机制</strong>：通过注意力机制，模型可以更加集中于输入数据中的关键部分，减少对通用知识的遗忘。</p>
</li>
<li>
<p><strong>元学习</strong>：使模型学会如何学习，即通过少量样本快速适应新任务，减少对原有知识的依赖。</p>
</li>
<li>
<p><strong>数据增强</strong>：通过数据增强技术，增加领域数据的多样性，减少对特定数据集的过度拟合。</p>
</li>
<li>
<p><strong>任务无关的正则化</strong>：在训练领域模型时，加入与任务无关的正则化项，以保持模型的通用性。</p>
</li>
<li>
<p><strong>模型容量</strong>：增加模型的容量，使其能够存储更多的知识，减少遗忘。</p>
</li>
<li>
<p><strong>经验回放</strong>：存储预训练期间的经验，并在后续训练中适当地回放这些经验。</p>
</li>
<li>
<p><strong>持续学习</strong>：采用持续学习框架，确保模型在面对新任务时能够保留之前学到的知识。</p>
</li>
<li>
<p><strong>跨领域迁移</strong>：在不同领域之间迁移知识，使模型在适应新领域时不会完全丢弃旧领域的知识。</p>
</li>
<li>
<p><strong>评估和反馈</strong>：定期评估模型的通用能力，并根据反馈调整训练策略。</p>
</li>
</ol>
<h3 id="236-continue-pretrain">2.3.6 领域模型Continue PreTrain ，如何让模型在预训练过程中就学习到更多的知识?<a class="headerlink" href="#236-continue-pretrain" title="Permanent link">&para;</a></h3>
<p>在领域模型Continue PreTrain（继续预训练）过程中，让模型学习到更多的知识是一个综合性的任务，涉及多个方面的优化。以下是一些关键策略和步骤：</p>
<ol>
<li>
<p>数据选择与准备</p>
</li>
<li>
<p><strong>数据多样性</strong>：确保预训练数据集具有高度的多样性和质量，涵盖广泛的主题、风格和格式，以提供丰富的学习材料。这有助于模型在不同场景下都能表现出色。</p>
</li>
<li><strong>大规模数据集</strong>：使用大规模数据集进行训练，确保模型能够接触到更多的信息和知识。大规模数据有助于模型学习到更复杂的模式和关联。</li>
<li><strong>领域特定数据</strong>：选择与特定领域紧密相关的数据，这些数据应包含领域特有的术语、概念和情境。这有助于模型在特定领域内表现出更高的专业性和准确性。</li>
<li>
<p><strong>数据质量</strong>：选取的数据应准确、干净，避免包含错误或不完整的信息。低质量的数据会降低模型训练的效果。</p>
</li>
<li>
<p>模型架构与算法优化</p>
</li>
<li>
<p><strong>模型架构优化</strong>：设计或优化模型架构，如使用Transformer或BERT等先进的模型结构，以提高模型的表示能力和学习能力。</p>
</li>
<li><strong>注意力机制</strong>：利用注意力机制帮助模型集中于输入数据中最重要的部分，提高学习效率。</li>
<li>
<p><strong>正则化和防止过拟合</strong>：应用正则化技术，如Dropout或权重衰减，以防止模型在训练数据上过拟合。</p>
</li>
<li>
<p>学习策略与技巧</p>
</li>
<li>
<p><strong>多模态学习</strong>：结合文本、图像、声音等多种模态的数据进行训练，让模型能够从不同角度学习知识。这有助于提高模型的跨模态理解和生成能力。</p>
</li>
<li><strong>跨领域数据融合</strong>：将不同领域的数据融合在一起进行训练，使模型能够学习到跨领域的通用模式和关联。这有助于模型在更广泛的任务上表现出色。</li>
<li><strong>任务导向的学习</strong>：设计任务导向的学习方案，如语言模型进行问答、摘要、翻译等任务。这有助于模型在特定任务上表现出更高的性能和准确性。</li>
<li>
<p><strong>自监督学习</strong>：利用自监督学习方法，如预测文本中缺失的单词或句子，来提高模型对语言结构的理解。自监督学习可以在没有标注数据的情况下让模型学习到有用的特征。</p>
</li>
<li>
<p>增量与持续学习</p>
</li>
<li>
<p><strong>增量预训练</strong>：在已有的预训练模型基础上，使用领域数据进行增量预训练。这有助于模型在保持通用能力的同时，学习到更多的领域知识。</p>
</li>
<li>
<p><strong>持续学习</strong>：实施持续学习策略，使模型能够随着时间的推移不断吸收新知识。这有助于模型保持与时俱进的能力，适应不断变化的领域需求。</p>
</li>
<li>
<p>特定技术与方法</p>
</li>
<li>
<p><strong>知识注入</strong>：将结构化知识（如知识图谱）注入到模型中，以增强模型对特定事实的记忆和推理能力。</p>
</li>
<li><strong>模型蒸馏</strong>：通过模型蒸馏技术，将大型模型的知识迁移到小型模型。这有助于在资源受限的情况下，仍然能够获得高性能的模型。</li>
<li>
<p><strong>强化学习</strong>：利用强化学习让模型根据反馈信号自我调整，以更好地学习任务。这有助于提高模型的自我优化能力。</p>
</li>
<li>
<p>数据增强与迁移学习</p>
</li>
<li>
<p><strong>数据增强</strong>：通过数据增强技术，如图像旋转、文本同义词替换等，增加数据集的多样性。这有助于模型学习到更多的变体和模式。</p>
</li>
<li><strong>迁移学习</strong>：利用迁移学习，将在一个任务上学到的知识应用到其他相关任务上。这有助于模型在不同任务之间共享知识，提高整体性能。</li>
</ol>
<h3 id="237-sftchatbase">2.3.7 进行SFT操作的时候，基座模型选用Chat还是Base?<a class="headerlink" href="#237-sftchatbase" title="Permanent link">&para;</a></h3>
<p>在进行SFT（Supervised Fine-Tuning，监督式微调）操作时，选择基座模型（Base Model）还是特定领域的优化模型（如Chat-Oriented Model），取决于几个关键因素：</p>
<ol>
<li>
<p><strong>任务类型</strong>：如果任务是聊天或对话相关的，选择一个已经针对对话优化过的模型（如Chat模型）可能更合适。如果任务是更通用的，如文本分类或翻译，基础模型可能更合适。</p>
</li>
<li>
<p><strong>领域特定性</strong>：如果需要模型在特定领域（如医疗、法律等）内表现更好，选择一个已经在该领域内训练过的模型可能会带来更好的性能。</p>
</li>
<li>
<p><strong>资源可用性</strong>：特定优化的模型可能需要更多的资源来获取和部署。如果资源有限，可能需要使用更通用的基础模型。</p>
</li>
<li>
<p><strong>性能要求</strong>：如果对任务的性能要求非常高，并且需要模型展现出特定的行为或风格，特定优化的模型可能更能满足这些要求。</p>
</li>
<li>
<p><strong>微调数据</strong>：如果微调数据集与特定优化模型的训练数据类似，那么使用该模型可能会获得更好的微调效果。</p>
</li>
<li>
<p><strong>开发时间</strong>：如果项目时间紧迫，可能没有足够的时间来训练和微调特定优化的模型，此时基础模型可能是一个更快速的解决方案。</p>
</li>
<li>
<p><strong>可定制性</strong>：基础模型可能提供更高的可定制性，因为它们可以从更多的领域进行学习和适应。</p>
</li>
<li>
<p><strong>先前经验</strong>：如果团队在特定类型的模型（如Chat模型）上有更多经验，可能会倾向于选择这种模型以利用现有知识。</p>
</li>
<li>
<p><strong>成本效益</strong>：评估不同模型选择的成本效益，包括获取、训练、部署和维护的成本。</p>
</li>
<li>
<p><strong>技术生态</strong>：考虑模型选择与现有技术生态的兼容性，以及它们如何与现有的系统和工作流程集成。</p>
</li>
<li>
<p><strong>风险评估</strong>：评估使用特定优化模型可能带来的风险，如模型偏差、过拟合等。</p>
</li>
<li>
<p><strong>长期维护</strong>：考虑模型的长期维护和更新策略，选择能够持续适应新数据和任务变化的模型。</p>
</li>
</ol>
<p>在实际操作中，可能需要根据具体情况进行权衡和测试，以确定哪种模型最适合当前任务和需求。有时，可以先使用基础模型进行初步的微调，然后根据性能评估结果决定是否需要迁移到特定优化的模型。</p>
<h3 id="238">2.3.8 领域模型微调 指令&amp;数据输入格式 要求?<a class="headerlink" href="#238" title="Permanent link">&para;</a></h3>
<p>领域模型微调是指使用预训练的通用语言模型（如BERT、GPT等）对特定领域的数据进行微调，以适应该领域的任务需求。在进行领域模型微调时，指令和数据输入的格式与要求至关重要，它们直接影响到模型训练的效果和最终性能。以下是关于领域模型微调指令和数据输入格式与要求的一些关键要点：</p>
<p>一、指令格式与要求</p>
<ol>
<li><strong>明确性</strong>：指令应清晰、明确地描述任务目标。对于分类任务，指令应指出需要预测的类别；对于生成任务，指令应明确生成内容的类型和格式。</li>
<li><strong>一致性</strong>：指令的表述方式应与模型训练时使用的数据集中的指令保持一致，以避免混淆模型。</li>
<li><strong>简洁性</strong>：指令应尽可能简洁，避免冗长和复杂的描述，以便模型能够快速理解并响应。</li>
</ol>
<p>二、数据输入格式与要求</p>
<ol>
<li>
<p><strong>文本形式</strong>：输入数据应以文本形式提供，每个样本对应一行或按照特定格式组织。</p>
</li>
<li>
<p><strong>分隔符</strong>：</p>
</li>
<li>对于分类任务，每个样本应包含文本和标签，可以使用制表符（\t）、逗号（,）或其他预定义的分隔符将文本和标签分隔开。</li>
<li>对于生成任务，每个样本通常只需包含文本，但也可能需要包含额外的指令或上下文信息。</li>
<li>
<p>对于序列标注任务，每个样本应包含文本和对应的标签序列，同样需要使用分隔符将文本和标签序列分隔开。</p>
</li>
<li>
<p><strong>文件格式</strong>：数据集应以常见的文件格式保存，如文本文件（.txt）、CSV文件（.csv）、JSON文件（.json）等。确保文件格式与模型输入的要求一致。</p>
</li>
<li>
<p><strong>数据预处理</strong>：</p>
</li>
<li>去除文本中的无关字符和噪声，如HTML标签、特殊符号等。</li>
<li>标准化文本格式，如统一使用英文标点符号而非全角符号。</li>
<li>根据目标语言的特点，使用适当的分词工具将文本分割成单词或字符。确保分词器与预训练模型使用的分词器一致。</li>
<li>
<p>将文本转换为模型可接受的序列化形式，通常包括词汇索引或ID。构建或使用预训练模型的词汇表，将文本中的单词映射到唯一的数值ID。</p>
</li>
<li>
<p><strong>数据增强</strong>：为了提高模型的泛化能力，可能需要对数据进行增强。数据增强技术包括同义词替换、随机插入、回译等。</p>
</li>
<li>
<p><strong>数据集划分</strong>：将数据集划分为训练集、验证集和测试集，用于模型的训练、验证和评估。确保数据集的划分反映了真实的数据分布。</p>
</li>
<li>
<p><strong>特殊标记</strong>：对于某些模型，可能需要添加特定的序列开始和结束标记，如BERT的[CLS]和[SEP]。这些特殊标记有助于模型更好地理解序列的结构和任务的上下文。</p>
</li>
<li>
<p><strong>数据编码</strong>：对于需要数值输入的任务，如情感分析，可能需要对数据进行标准化处理。同时，对于某些模型，可能需要将文本转换为特定的编码形式，如TF-IDF、Word2Vec等。</p>
</li>
<li>
<p><strong>数据验证</strong>：在数据输入到模型之前，进行数据验证，确保数据的质量和格式正确。</p>
</li>
</ol>
<p>### 2.3.9 领域模型微调 领域评测集 构建?</p>
<p>领域模型微调过程中的领域评测集构建是评估模型在特定领域性能的关键步骤。评测集应包含一系列精心设计的任务和数据集，以全面、客观地评估模型在目标领域内的表现。以下是一些构建领域评测集的关键要点：</p>
<ol>
<li>明确评测目标</li>
</ol>
<p>首先，需要明确评测的目标，即希望通过评测集评估模型的哪些能力。这些能力可能包括但不限于领域知识理解能力、任务完成质量、生成文本的准确性、流畅性和相关性等。</p>
<ol>
<li>
<p>选择或构建数据集</p>
</li>
<li>
<p>选择现有数据集：</p>
</li>
<li>查找并评估现有的、针对目标领域的公开数据集。这些数据集可能已经过严格的质量控制和标注，可以直接用于评测。</li>
<li>
<p>常见的领域数据集来源包括学术研究机构、开源社区和企业发布的数据集。</p>
</li>
<li>
<p>构建自定义数据集：</p>
</li>
<li>如果现有数据集无法满足评测需求，可以构建自定义的数据集。这通常涉及数据收集、清洗、标注和验证等步骤。</li>
<li>数据收集应确保覆盖目标领域的各个方面，包括常见的任务场景和罕见情况。</li>
<li>
<p>数据标注应由具有领域知识的专家进行，以确保标注的准确性和一致性。</p>
</li>
<li>
<p>设计评测任务</p>
</li>
</ol>
<p>根据评测目标，设计一系列具体的评测任务。这些任务应能够全面反映模型在目标领域内的性能。例如，对于对话系统，可以设计多轮对话任务、意图识别任务和槽位填充任务等；对于文本分类任务，可以设计多个类别的分类任务等。</p>
<ol>
<li>制定评测标准</li>
</ol>
<p>制定明确的评测标准，用于评估模型在评测任务上的表现。评测标准应涵盖多个方面，如准确率、召回率、F1分数、BLEU分数等。同时，应确保评测标准的客观性和可重复性。</p>
<ol>
<li>实施评测</li>
</ol>
<p>使用构建好的评测集和评测标准，对模型进行实际评测。评测过程中应注意以下几点：</p>
<ul>
<li>确保评测环境的一致性，包括硬件和软件配置等。</li>
<li>使用相同的评测参数和设置，以确保评测结果的可比性。</li>
<li>
<p>对评测结果进行详细记录和分析，以发现模型的优点和不足。</p>
</li>
<li>
<p>结果分析和反馈</p>
</li>
</ul>
<p>根据评测结果，对模型在目标领域内的性能进行全面分析。分析模型在不同任务上的表现差异，找出潜在的问题和改进方向。同时，将评测结果反馈给模型开发者，以便他们根据反馈进行模型优化和改进。</p>
<ol>
<li>持续更新和优化</li>
</ol>
<p>随着目标领域的发展和变化，评测集也需要不断更新和优化。可以定期收集新的数据，并对评测集进行扩展和修订。同时，根据最新的评测结果和模型性能表现，对评测标准和评测方法进行相应的调整和优化。</p>
<p>### 2.3.10 领域模型词表扩增是不是有必要的?</p>
<p>领域模型词表扩增，即将特定领域的专有词汇、术语或新词添加到模型的词汇表中，可能是有必要的，具体取决于以下几个因素：</p>
<ol>
<li>
<p><strong>领域特定性</strong>：对于一些专业领域，如医学、法律或技术领域，存在大量专业术语。扩增词表可以帮助模型更好地理解和处理这些术语。</p>
</li>
<li>
<p><strong>术语的频率和重要性</strong>：如果领域中的特定术语频繁出现且对任务至关重要，将这些术语添加到词表中可以提高模型的准确性和可靠性。</p>
</li>
<li>
<p><strong>模型的预训练数据</strong>：如果模型的预训练数据集中缺少当前领域的专业术语，扩增词表可以帮助模型更好地适应领域数据。</p>
</li>
<li>
<p><strong>避免歧义</strong>：领域术语可能在普通语境中有不同含义，扩增词表有助于减少模型在处理领域文本时的歧义。</p>
</li>
<li>
<p><strong>提高召回率</strong>：在信息检索或文本分类任务中，扩增词表可以提高模型对领域相关文本的召回率。</p>
</li>
<li>
<p><strong>改善模型的泛化能力</strong>：通过学习更多领域的特定词汇，模型可能在处理来自该领域的新数据时表现得更好。</p>
</li>
<li>
<p><strong>适应新兴领域</strong>：对于一些快速发展的领域，新技术和术语不断涌现，扩增词表有助于模型适应这些变化。</p>
</li>
<li>
<p><strong>提升模型的可解释性</strong>：在某些情况下，领域专家可能需要理解模型的决策过程，扩增词表并结合领域知识可以提高模型的可解释性。</p>
</li>
<li>
<p><strong>跨领域应用</strong>：如果模型需要在多个领域中应用，扩增词表可以提高模型在不同领域的适应性。</p>
</li>
<li>
<p><strong>数据集的多样性</strong>：如果领域数据集中包含多样化的术语使用，扩增词表有助于模型更好地捕捉这些多样性。</p>
</li>
</ol>
<p>然而，扩增词表也可能带来一些挑战，如增加模型训练的复杂性、可能导致过拟合等。因此，在决定是否扩增词表时，需要权衡以下因素：</p>
<ul>
<li><strong>资源的可用性</strong>：扩增词表可能需要额外的资源来收集、整理和训练数据。</li>
<li><strong>模型性能的平衡</strong>：需要评估扩增词表对模型在特定任务上性能的影响，确保性能提升是显著的。</li>
<li><strong>维护成本</strong>：随着领域的发展，词表可能需要定期更新，这会带来额外的维护成本。</li>
</ul>
<p>### 2.3.11 如何训练自己的大模型?</p>
<p>训练自己的大模型是一个复杂且资源密集型的任务，它涉及到深度学习、大量数据处理、高性能计算等多个方面。以下是一个详细的步骤指南，帮助你开始训练自己的大模型：</p>
<ol>
<li>
<p>学习深度学习基础知识</p>
</li>
<li>
<p><strong>学习内容</strong>：深度学习的基础知识包括神经网络结构（如卷积神经网络CNN、循环神经网络RNN、Transformer等）、损失函数、优化算法（如SGD、Adam等）等。</p>
</li>
<li>
<p><strong>学习方式</strong>：可以通过在线课程（如Coursera、Udemy上的深度学习课程）、教科书（如《深度学习》一书）和教程来学习。</p>
</li>
<li>
<p>熟悉深度学习框架和编程语言</p>
</li>
<li>
<p><strong>深度学习框架</strong>：熟悉并掌握至少一种深度学习框架，如TensorFlow、PyTorch等。</p>
</li>
<li>
<p><strong>编程语言</strong>：熟练掌握Python等编程语言，因为它们是深度学习领域的主流语言。</p>
</li>
<li>
<p>收集和处理数据集</p>
</li>
<li>
<p><strong>数据收集</strong>：根据你的模型应用需求，收集大量的相关数据。这些数据可以是文本、图像、语音等多种形式。</p>
</li>
<li><strong>数据清洗</strong>：对数据进行清洗，去除噪音、无关信息以及个人隐私相关内容。</li>
<li>
<p><strong>数据标注</strong>：对于监督学习任务，需要对数据进行标注，以提供训练所需的标签。</p>
</li>
<li>
<p>获取计算资源</p>
</li>
<li>
<p><strong>硬件资源</strong>：大模型训练需要大量的计算资源，包括高性能的GPU或TPU。如果自己的硬件资源有限，可以考虑使用云计算平台（如AWS、Google Cloud、Azure等）或租赁专用的深度学习服务器。</p>
</li>
<li>
<p><strong>软件工具</strong>：使用适合的数据处理、模型训练和评估的软件工具。</p>
</li>
<li>
<p>选择和构建模型</p>
</li>
<li>
<p><strong>选择预训练模型</strong>：考虑使用已有的预训练模型（如BERT、GPT等），这些模型已经在大规模数据上进行了训练，可以通过微调来适应你的特定任务。</p>
</li>
<li>
<p><strong>自定义模型</strong>：如果没有合适的预训练模型，你可以根据需求自定义模型结构。</p>
</li>
<li>
<p>模型训练</p>
</li>
<li>
<p><strong>数据准备</strong>：将清洗和标注好的数据准备为模型训练所需的格式。</p>
</li>
<li><strong>配置训练参数</strong>：选择合适的超参数，如学习率、批量大小等。</li>
<li>
<p><strong>开始训练</strong>：使用选定的深度学习框架和计算资源开始训练模型。训练过程可能需要大量的时间和计算资源。</p>
</li>
<li>
<p>模型评估和优化</p>
</li>
<li>
<p><strong>性能评估</strong>：使用验证集或测试集评估模型的性能。</p>
</li>
<li>
<p><strong>优化调整</strong>：根据评估结果调整模型结构、超参数或训练策略，以优化模型性能。</p>
</li>
<li>
<p>模型部署和应用</p>
</li>
<li>
<p><strong>模型部署</strong>：将训练好的模型部署到实际应用中。这可能需要使用容器化技术（如Docker）、模型服务框架（如TensorFlow Serving）等。</p>
</li>
<li>
<p><strong>持续监控</strong>：定期监控已部署模型的性能，以确保其在生产环境中表现良好。</p>
</li>
<li>
<p>持续学习和研究</p>
</li>
<li>
<p><strong>跟踪最新进展</strong>：深度学习领域不断演进，保持对最新研究和技术的关注是非常重要的。可以通过阅读学术论文、关注研究者的社交媒体和参与相关研究项目来实现。</p>
</li>
<li>
<p><strong>参与社区</strong>：加入深度学习社区（如GitHub、Stack Overflow、Reddit等），与其他从业者交流经验、分享成果和解决问题。</p>
</li>
<li>
<p>注意事项</p>
</li>
<li>
<p><strong>数据隐私和安全</strong>：在处理敏感数据时，确保遵守相关法规和伦理准则，保护用户隐私。</p>
</li>
<li><strong>成本控制</strong>：合理规划和管理计算资源和资金投入，以控制训练成本。</li>
<li><strong>可解释性和透明度</strong>：对于需要可解释性的应用场景，研究模型解释性技术，以了解模型的决策过程。</li>
</ol>
<p>### 2.3.12 训练中文大模型有啥经验?</p>
<p>训练中文大模型的经验涉及多个方面，以下是一些关键的经验和建议：</p>
<p>一、数据准备与处理</p>
<ol>
<li><strong>数据质量</strong>：确保收集到的中文数据具有高质量和多样性，以覆盖不同的场景和主题。数据质量对大模型的训练效果至关重要。</li>
<li><strong>数据预处理</strong>：</li>
<li><strong>分词</strong>：使用成熟的中文分词工具（如jieba、pkuseg等）对文本进行分词处理，这是中文处理的基本步骤。</li>
<li><strong>去除停用词</strong>：去除文本中的无意义词汇，以减少数据噪音。</li>
<li><strong>词性标注</strong>：对文本中的词汇进行词性标注，有助于模型更好地理解语义。</li>
<li><strong>拼音转换</strong>：在某些场景下，可能需要将汉字转换为拼音，以辅助模型处理发音相关的信息。</li>
<li><strong>数据增强</strong>：考虑到中文数据集可能相对有限，可以使用数据增强技术来扩充数据集。例如，同义词替换、随机插入或删除词语、句子重组等方法都可以生成新的训练样本。</li>
</ol>
<p>二、模型选择与优化</p>
<ol>
<li><strong>预训练模型</strong>：</li>
<li>利用已在大规模中文语料上预训练好的模型（如BERT、GPT等）作为初始模型，然后在目标任务上进行微调。这样可以利用大规模中文语料的信息，提升模型的表达能力和泛化能力。</li>
<li>对于特定的中文任务（如中文分词、命名实体识别、情感分析等），可以使用中文特定的工具或模型来辅助训练。</li>
<li><strong>模型设计</strong>：</li>
<li>采用先进的神经网络结构（如Transformer）作为模型的基础架构，以提高模型的表达能力和计算效率。</li>
<li>根据任务需求进行模型调整，如增加注意力头、使用长短时记忆网络（LSTM）等。</li>
<li><strong>超参数调优</strong>：</li>
<li>选择合适的优化算法和超参数对模型训练至关重要。常用的优化算法包括Adam、Adagrad、RMSProp等。</li>
<li>通过网格搜索、随机搜索或基于优化算法的自动调参方法来寻找最佳的超参数组合。</li>
</ol>
<p>三、训练策略与技巧</p>
<ol>
<li><strong>分阶段训练</strong>：将训练过程分为多个阶段，每个阶段解决不同的问题或优化不同的目标。</li>
<li><strong>稀疏训练</strong>：通过稀疏化技术减少模型参数的数量，降低计算成本和存储需求。</li>
<li><strong>迁移学习</strong>：利用在相关领域或任务上已经训练好的模型，通过迁移学习技术快速适应新的任务或数据。</li>
<li><strong>自监督学习</strong>：利用大量未标注的中文数据进行自监督学习，以提高模型的预训练效果。</li>
</ol>
<p>四、计算资源与部署</p>
<ol>
<li><strong>计算资源</strong>：</li>
<li>训练大模型需要大量的计算资源，包括GPU、内存和存储。可以考虑使用云计算平台或分布式训练来加速训练过程。</li>
<li>合理分配计算资源，避免性能瓶颈，确保训练过程的顺利进行。</li>
<li><strong>模型部署</strong>：</li>
<li>采用合适的部署方法（如TensorFlow Serving、PyTorch等），确保模型在生产环境中的稳定运行。</li>
<li>对模型进行剪枝、量化等操作，以降低模型复杂度、提高部署效果。</li>
</ol>
<p>五、持续学习与改进</p>
<ol>
<li><strong>监控与评估</strong>：在训练过程中实时监控模型的性能指标（如准确率、召回率等），及时发现并解决问题。</li>
<li><strong>知识蒸馏</strong>：通过知识蒸馏技术将大型模型压缩为小型模型，提高模型在实际应用中的性能。</li>
<li><strong>持续学习</strong>：关注深度学习领域的最新研究进展和技术动态，不断学习和引入新的方法和技术来改进模型。</li>
</ol>
<p>### 2.3.13 指令微调的好处?</p>
<p>指令微调（Prompt Tuning）是一种模型微调方法，通过在输入中添加特定的指令或提示来引导模型完成特定任务。以下是指令微调的一些好处：</p>
<ol>
<li>
<p><strong>灵活性</strong>：指令微调允许模型在不同的任务之间灵活切换，只需改变输入的指令而无需重新训练整个模型。</p>
</li>
<li>
<p><strong>效率</strong>：相比于完全从头开始训练模型，指令微调通常需要更少的数据和计算资源。</p>
</li>
<li>
<p><strong>快速适应</strong>：模型可以快速适应新任务，特别是当新任务与预训练任务相似时。</p>
</li>
<li>
<p><strong>避免过拟合</strong>：由于微调的数据量通常较少，指令微调可以减少过拟合的风险。</p>
</li>
<li>
<p><strong>提高泛化能力</strong>：通过精心设计的指令，模型可以更好地理解和泛化任务的上下文。</p>
</li>
<li>
<p><strong>增强可解释性</strong>：指令微调可以帮助提高模型决策过程的可解释性，因为指令直接关联到模型的输出。</p>
</li>
<li>
<p><strong>跨任务迁移</strong>：指令微调有助于模型在不同任务之间迁移知识，提高模型的通用性。</p>
</li>
<li>
<p><strong>减少数据需求</strong>：相比于监督学习，指令微调可能需要更少的标注数据，特别是在使用预训练模型时。</p>
</li>
<li>
<p><strong>简化模型更新</strong>：在新任务出现时，可以通过简单地更改指令来更新模型，而不需要重新训练整个模型。</p>
</li>
<li>
<p><strong>提高模型的实用性</strong>：指令微调可以使模型更容易集成到实际应用中，快速响应业务需求的变化。</p>
</li>
<li>
<p><strong>探索新能力</strong>：通过指令微调，可以探索模型在未见过的任务上的潜在能力。</p>
</li>
<li>
<p><strong>成本效益</strong>：相比于全参数微调，指令微调可能更加经济，因为它需要的计算资源较少。</p>
</li>
<li>
<p><strong>教育和研究</strong>：在教育和研究环境中，指令微调提供了一种快速实验和验证模型能力的方法。</p>
</li>
<li>
<p><strong>个性化</strong>：指令微调可以根据用户的特定需求定制指令，实现个性化的模型行为。</p>
</li>
<li>
<p><strong>创新应用</strong>：指令微调激发了对模型新应用方式的探索，如交互式学习、自动代码生成等。</p>
</li>
</ol>
<p>### 2.3.14 预训练和微调哪个阶段注入知识的?</p>
<p>在机器学习和深度学习中，预训练和微调是两个不同的阶段，它们都可以用于向模型注入知识，但方式和目的不同：</p>
<ol>
<li><strong>预训练阶段</strong>：</li>
<li>目的：在大规模的数据集上训练模型，使其学习到通用的语言表示或特征。</li>
<li>注入知识：这个阶段模型学习到的是广泛的知识，包括词汇、语法、句子结构、常见概念等。</li>
<li>
<p>方法：通常使用无监督或自监督学习任务，如掩码语言模型（MLM）、下一句预测（NSP）等。</p>
</li>
<li>
<p><strong>微调阶段</strong>：</p>
</li>
<li>目的：在特定领域的数据上进一步训练模型，使其适应特定的任务或应用场景。</li>
<li>注入知识：这个阶段模型学习到的是与特定任务或领域相关的专业知识。</li>
<li>方法：通常使用监督学习，将预训练模型作为起点，然后在有限的标注数据上进行微调。</li>
</ol>
<p>预训练阶段注入知识的方式：
- <strong>自监督学习</strong>：通过预测遮蔽（masked）的单词或短语，模型学习语言的内在结构。
- <strong>大规模数据集</strong>：使用大量的文本数据，覆盖广泛的主题和风格，使模型能够学习到丰富的语言特征。</p>
<p>微调阶段注入知识的方式：
- <strong>领域特定数据</strong>：使用特定领域的文本数据，如法律文件、医疗记录等，使模型学习到专业术语和概念。
- <strong>任务特定数据</strong>：针对特定任务（如情感分析、文本分类等）进行训练，使模型能够执行特定任务。
- <strong>少量样本学习</strong>：在数据较少的情况下，通过微调使模型快速适应新任务。</p>
<p>两者的结合：
- 在实际应用中，预训练和微调通常是连续的过程。预训练提供了一个强大的基础模型，然后通过微调将这个模型适应到特定的任务或领域。
- 预训练阶段可以看作是模型的“基础教育”，而微调阶段则是“专业培训”。</p>
<p>特殊情况：
- 在某些情况下，如果特定领域的数据非常充足，也可以考虑在预训练阶段就使用这些领域特定的数据，从而在预训练过程中就注入特定知识。</p>
<p>总的来说，预训练和微调都是向模型注入知识的重要阶段，但它们关注的知识和应用场景不同。预训练侧重于通用知识的学习，而微调侧重于特定任务或领域的专业知识的适应。</p>
<p>### 2.3.15 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调?</p>
<p>在机器学习和人工智能领域，预训练和微调是两种常见的模型训练方法，它们各有优势和适用场景：</p>
<ol>
<li><strong>预训练（Pre-training）</strong>：</li>
<li>预训练通常指的是在一个大型的数据集上训练模型，以便学习通用的语言或模式。</li>
<li>这种方法适用于没有特定领域数据集或者数据集较小的情况。</li>
<li>
<p>预训练模型可以在广泛的任务上表现良好，因为它学习了语言的通用结构和语义信息。</p>
</li>
<li>
<p><strong>微调（Fine-tuning）</strong>：</p>
</li>
<li>微调是在预训练模型的基础上，针对特定领域或任务进行进一步训练的过程。</li>
<li>这种方法适用于有足够特定领域数据的情况。</li>
<li>微调可以帮助模型更好地适应特定任务或领域，提高在该领域的性能。</li>
</ol>
<p>如果想让大模型学习某个特定领域或行业的知识，通常的做法是：</p>
<ul>
<li>首先进行**预训练**，以获得一个在语言理解、模式识别等方面表现良好的基础模型。</li>
<li>然后，根据可用的特定领域数据量，选择是否进行**微调**。如果有足够的领域特定数据，微调可以帮助模型更好地理解和适应该领域的特定需求。</li>
</ul>
<p>在实际操作中，微调通常需要较少的计算资源和时间，因为它是在预训练模型的基础上进行的。此外，微调可以针对不同的任务或领域进行多次，从而使得同一个预训练模型能够适应多种不同的应用场景。</p>
<h3 id="2316">2.3.16 多轮对话任务如何微调模型?<a class="headerlink" href="#2316" title="Permanent link">&para;</a></h3>
<p>多轮对话任务的模型微调是一个复杂但重要的过程，旨在提高模型在连续对话中的理解、生成和响应能力。以下是微调模型以适应多轮对话任务的一般步骤和考虑因素：</p>
<ol>
<li>数据准备</li>
</ol>
<p><strong>收集或生成数据集</strong>：
- 数据应包含多轮对话的完整历史记录，涵盖不同的主题、风格和长度。
- 确保数据集的多样性和代表性，以提高模型的泛化能力。</p>
<p><strong>数据清洗和格式化</strong>：
- 对数据进行清洗，去除噪声和无关信息。
- 将对话数据格式化为模型可以理解的输入形式，如序列对（prompt-response）格式。</p>
<ol>
<li>模型选择</li>
</ol>
<p><strong>选择合适的预训练模型</strong>：
- 选择一个经过广泛验证的预训练语言模型作为基础，如GPT、BERT、XLNet等。
- 根据任务的具体需求选择不同大小和版本的模型。</p>
<ol>
<li>任务特定层</li>
</ol>
<p><strong>添加任务特定层</strong>：
- 在预训练模型的基础上添加处理对话历史、上下文理解和生成回答等任务相关的层。
- 这些层可能包括注意力机制、记忆网络、对话状态跟踪器等。</p>
<ol>
<li>微调过程</li>
</ol>
<p><strong>定义损失函数</strong>：
- 选择适合对话生成任务的损失函数，如交叉熵损失。
- 考虑使用BLEU、ROUGE等评估指标来辅助优化。</p>
<p><strong>优化算法和参数</strong>：
- 使用常见的优化算法（如SGD、Adam）来优化模型参数。
- 仔细调整学习率、批次大小、训练轮数等超参数。</p>
<p><strong>数据增强和对抗训练</strong>：
- 应用数据增强技术来增加训练数据的多样性和丰富性。
- 引入对抗训练来提高模型的鲁棒性和泛化能力。</p>
<ol>
<li>评估和调优</li>
</ol>
<p><strong>评估模型性能</strong>：
- 使用验证集或开发集对微调后的模型进行评估。
- 计算并比较不同模型在多轮对话任务上的准确率、召回率、F1分数等指标。</p>
<p><strong>调优和迭代</strong>：
- 根据评估结果调整模型结构、参数或训练策略。
- 进行多次迭代以逐步提高模型性能。</p>
<ol>
<li>推理和部署</li>
</ol>
<p><strong>部署模型</strong>：
- 将微调后的模型部署到实际应用场景中。
- 确保模型能够在实时或接近实时的环境中高效运行。</p>
<p><strong>监控和反馈</strong>：
- 监控模型在实际应用中的表现，收集用户反馈。
- 根据反馈对模型进行持续优化和改进。</p>
<p>注意事项</p>
<ul>
<li>在微调过程中，需要特别关注对话的上下文理解、连贯性以及个性化回应的能力。</li>
<li>确保模型能够处理和记忆长距离的对话历史信息。</li>
<li>使用具有记忆能力的模型架构（如基于Transformer的模型）来捕捉长距离依赖关系。</li>
</ul>
<p>### 2.3.17 微调后的模型出现能力劣化，灾难性遗忘是怎么回事?</p>
<p>微调后的模型出现能力劣化，特别是灾难性遗忘（Catastrophic Forgetting），是一个在机器学习领域，特别是在深度学习和大模型应用中频繁出现的问题。这种现象通常发生在模型在微调过程中，当模型在新任务上进行训练时，可能会忘记之前学习到的知识，导致在旧任务上的性能显著下降。以下是关于这一问题的详细分析：</p>
<p>一、灾难性遗忘的原因</p>
<ol>
<li><strong>数据分布差异</strong>：</li>
<li>
<p>微调过程中使用的新任务数据与预训练数据或旧任务数据的分布可能存在显著差异。如果新任务的数据分布与预训练数据差异较大，模型可能会过度调整以适应新任务，导致在旧任务上的性能下降。</p>
</li>
<li>
<p><strong>参数更新冲突</strong>：</p>
</li>
<li>
<p>微调过程中，对新任务进行训练时，模型参数可能会被更新，导致之前学习到的知识被覆盖或丢失。新任务的梯度更新可能会与旧任务的梯度更新发生冲突，从而引发灾难性遗忘。</p>
</li>
<li>
<p><strong>优化目标差异</strong>：</p>
</li>
<li>
<p>微调通常会使用新数据集上的特定损失函数进行优化，而不是原始训练时使用的损失函数。这种差异可能导致模型在优化过程中不平衡地调整参数，进一步加剧灾难性遗忘的风险。</p>
</li>
<li>
<p><strong>模型容量与数据量不匹配</strong>：</p>
</li>
<li>大型模型通常有数百万到数十亿的参数，这些参数在微调时会尽可能地调整以最小化损失函数。如果微调数据的覆盖范围不足以涵盖模型之前学习的所有方面，模型可能会在学习新任务时丧失先前任务的能力。</li>
</ol>
<p>二、灾难性遗忘的影响</p>
<ul>
<li><strong>性能下降</strong>：在旧任务上的性能显著下降，模型可能无法准确完成之前训练过的任务。</li>
<li><strong>知识遗忘</strong>：模型忘记之前学习到的知识，包括但不限于语言理解、逻辑推理、情感分析等能力。</li>
<li><strong>应用受限</strong>：由于模型在旧任务上的性能下降，其在实际应用中的可靠性和效果将受到严重影响。</li>
</ul>
<p>三、缓解灾难性遗忘的方法</p>
<ol>
<li><strong>增量学习</strong>：</li>
<li>
<p>通过增量学习的技术，逐步引入新的数据和任务，而不是一次性地对整个模型进行微调。这有助于减少新任务对旧任务性能的冲击。</p>
</li>
<li>
<p><strong>记忆增强</strong>：</p>
</li>
<li>
<p>使用记忆增强的方法，如重放缓冲区（Replay Buffer）或经验回放（Experience Replay），以维持和更新模型对先前任务的记忆。这种方法可以在训练新任务时，通过回顾旧任务的样本来减少遗忘。</p>
</li>
<li>
<p><strong>多任务学习</strong>：</p>
</li>
<li>
<p>利用多任务学习的框架，使模型能够同时学习多个相关任务。通过共享模型参数和优化目标，可以减少灾难性遗忘的风险。</p>
</li>
<li>
<p><strong>迁移学习策略</strong>：</p>
</li>
<li>
<p>选择适合的迁移学习策略，如只微调部分模型层或使用预训练模型的特征提取能力而不是全模型微调。这有助于保留模型在旧任务上的性能。</p>
</li>
<li>
<p><strong>正则化技术</strong>：</p>
</li>
<li>引入正则化项来限制模型参数的变动范围，保护之前学习到的知识不被过度破坏。例如，弹性权重共享（Elastic Weight Consolidation）等方法可以通过正则化来平衡新任务和旧任务之间的优化目标。</li>
</ol>
<h3 id="2318">2.3.18 微调模型需要多大显存?<a class="headerlink" href="#2318" title="Permanent link">&para;</a></h3>
<p>微调模型所需的显存大小取决于多个因素，包括但不限于：</p>
<ol>
<li>
<p><strong>模型的大小</strong>：更大的模型通常需要更多的显存。例如，BERT-base模型的显存需求与BERT-large模型的显存需求就有很大差异。</p>
</li>
<li>
<p><strong>批量大小（Batch Size）</strong>：批量大小越大，需要的显存就越多。在显存有限的情况下，可能需要减小批量大小。</p>
</li>
<li>
<p><strong>序列长度（Sequence Length）</strong>：对于处理长序列的模型，如Transformer架构，序列长度的增加会显著增加显存需求。</p>
</li>
<li>
<p><strong>优化器和激活函数</strong>：某些优化器（如Adam）和激活函数可能会增加显存的使用。</p>
</li>
<li>
<p><strong>并行化和分布式训练</strong>：使用GPU并行化或分布式训练可以减少单个GPU的显存需求，但整体显存需求可能会增加。</p>
</li>
<li>
<p><strong>数据类型</strong>：使用的数据类型（如float32 vs float16）也会影响显存需求。使用较低精度的数据类型可以减少显存使用，但可能会影响模型性能。</p>
</li>
<li>
<p><strong>模型的实现和优化</strong>：不同的实现方式和优化技术可能会影响显存的使用。</p>
</li>
<li>
<p><strong>硬件配置</strong>：使用的GPU型号和数量也会影响显存需求。例如，NVIDIA的V100 GPU具有16GB或32GB的显存，而RTX 3080则有10GB的显存。</p>
</li>
</ol>
<p>通常，微调一个中等大小的模型（如BERT-base）在单个GPU上可能需要6GB到12GB的显存。但是，这只是一个粗略的估计，实际需求可能会根据上述因素有所不同。在实际应用中，最好根据你的具体模型和硬件配置来评估显存需求。如果你的显存不足，可能需要调整批量大小、序列长度或使用模型剪枝、量化等技术来减少显存使用。</p>
<h3 id="2319-llmsft">2.3.19 大模型LLM进行SFT操作的时候在学习什么?<a class="headerlink" href="#2319-llmsft" title="Permanent link">&para;</a></h3>
<p>大模型LLM（Large Language Model）进行SFT（Supervised Fine-Tuning，有监督微调）操作时，主要在学习以下几个方面：</p>
<ol>
<li>特定任务的知识与规则</li>
</ol>
<p>SFT的核心目标是使预训练好的LLM更加适应某一特定任务。因此，在微调过程中，LLM会学习该任务特有的知识和规则。这些知识和规则通常通过有标签的数据集来体现，数据集包含了任务所需的输入和对应的输出。例如，在文本分类任务中，LLM会学习如何将输入文本划分为预定义的类别；在机器翻译任务中，LLM会学习如何将一种语言的文本转换为另一种语言的文本。</p>
<ol>
<li>数据的分布与模式</li>
</ol>
<p>通过SFT，LLM会进一步学习微调数据集的分布和模式。这些数据集可能包含了与预训练数据集不同的特征或分布，因此LLM需要调整其内部参数以更好地捕捉这些新特征。这种学习过程有助于LLM在特定任务上实现更好的性能。</p>
<ol>
<li>输出格式的稳定性</li>
</ol>
<p>在SFT过程中，LLM还会学习如何以稳定的格式输出结果。这对于需要精确输出格式的任务尤为重要，如信息抽取、文本生成等。通过微调，LLM可以学会根据输入数据生成符合特定格式要求的输出，从而提高任务的完成质量。</p>
<ol>
<li>指令理解与对齐</li>
</ol>
<p>对于需要理解并执行指令的任务（如对话系统、推荐系统等），SFT有助于LLM更好地理解用户指令并生成符合用户期望的响应。通过大量的指令-响应数据对进行训练，LLM可以学会将用户输入映射到正确的输出上，并与用户的偏好和需求保持对齐。</p>
<ol>
<li>模型的泛化能力</li>
</ol>
<p>虽然SFT主要关注于提高LLM在特定任务上的性能，但一个好的微调过程也应该能够提升模型的泛化能力。这意味着微调后的LLM应该能够在一定程度上处理未见过的输入数据，并生成合理的输出。这通常要求微调数据集具有足够的多样性和代表性。</p>
<ol>
<li>
<p>细节与注意事项</p>
</li>
<li>
<p><strong>数据质量与筛选</strong>：在SFT过程中，数据的质量和筛选非常重要。高质量的数据集能够显著提升模型的性能和稳定性。</p>
</li>
<li><strong>超参数调整</strong>：微调过程中的超参数（如学习率、批次大小等）对模型性能有显著影响，需要进行细致的调整和优化。</li>
<li><strong>prompt设计</strong>：在SFT中，prompt（即输入指令或示例）的设计也至关重要。良好的prompt能够引导模型更好地理解任务需求并生成准确的输出。</li>
</ol>
<p>综上所述，大模型LLM进行SFT操作时主要在学习特定任务的知识与规则、数据的分布与模式、输出格式的稳定性、指令理解与对齐、模型的泛化能力以及相关的细节与注意事项。这些学习内容共同构成了LLM在特定任务上实现高性能和稳定性的基础。</p>
<h3 id="2320-sft">2.3.20 预训练和SFT操作有什么不同?<a class="headerlink" href="#2320-sft" title="Permanent link">&para;</a></h3>
<p>预训练（Pre-training）和监督式微调（Supervised Fine-Tuning，简称SFT）是深度学习中两种不同的训练阶段，它们在目标、方法和应用上有所区别：</p>
<ol>
<li><strong>目标</strong>：</li>
<li><strong>预训练</strong>：目的是在大规模的数据集上训练模型，使其能够学习到通用的语言表示和模式。这个阶段通常不针对特定的任务。</li>
<li>
<p><strong>SFT</strong>：目的是在预训练的基础上，针对特定的任务或领域进行进一步的训练，以提高模型在该任务上的性能。</p>
</li>
<li>
<p><strong>数据集</strong>：</p>
</li>
<li><strong>预训练</strong>：使用的数据集通常是大规模的、多样化的，可能包括维基百科、书籍、网页等。</li>
<li>
<p><strong>SFT</strong>：使用的数据集是针对特定任务的，可能包括问答对、对话记录或特定领域的文本。</p>
</li>
<li>
<p><strong>训练方式</strong>：</p>
</li>
<li><strong>预训练</strong>：模型在没有明确任务指导的情况下进行训练，例如通过预测下一个词或句子的掩码语言模型（Masked Language Model, MLM）任务。</li>
<li>
<p><strong>SFT</strong>：模型在有明确任务指导的情况下进行训练，例如通过监督学习来优化特定任务的输出。</p>
</li>
<li>
<p><strong>任务类型</strong>：</p>
</li>
<li><strong>预训练</strong>：不涉及特定任务的训练，而是学习语言的通用特性。</li>
<li>
<p><strong>SFT</strong>：涉及特定任务的训练，如文本分类、情感分析、问答系统等。</p>
</li>
<li>
<p><strong>模型参数</strong>：</p>
</li>
<li><strong>预训练</strong>：在预训练阶段，模型的大部分参数都会更新。</li>
<li>
<p><strong>SFT</strong>：在微调阶段，通常只有部分参数会更新，尤其是与任务相关的顶层或特定层的参数。</p>
</li>
<li>
<p><strong>训练时间</strong>：</p>
</li>
<li><strong>预训练</strong>：由于需要处理大量的数据和学习通用的语言表示，预训练通常需要较长的时间。</li>
<li>
<p><strong>SFT</strong>：微调通常在预训练模型的基础上进行，所需时间相对较短。</p>
</li>
<li>
<p><strong>资源需求</strong>：</p>
</li>
<li><strong>预训练</strong>：需要大量的计算资源，如GPU或TPU，以及大量的数据存储空间。</li>
<li>
<p><strong>SFT</strong>：资源需求相对较低，但仍然需要足够的计算能力来处理特定任务的数据。</p>
</li>
<li>
<p><strong>泛化能力</strong>：</p>
</li>
<li><strong>预训练</strong>：模型在预训练后具有较好的泛化能力，能够处理多种语言相关任务。</li>
<li><strong>SFT</strong>：模型在微调后可能在特定任务上表现更好，但泛化能力可能受限于训练数据的多样性。</li>
</ol>
<p>总结来说，预训练是为了构建一个具有通用语言理解能力的模型，而SFT是为了使这个模型在特定任务上达到最佳性能。两者通常结合使用，以实现在特定应用场景中的最优表现。</p>
<h3 id="2321-oom">2.3.21 样本量规模增大，训练出现OOM错误?<a class="headerlink" href="#2321-oom" title="Permanent link">&para;</a></h3>
<p>当样本量规模增大时，训练过程中出现OOM（Out Of Memory，内存溢出）错误是一个常见的问题。OOM错误通常发生在训练大型模型或处理大量数据时，因为系统资源（特别是内存）无法满足当前训练任务的需求。以下是一些可能导致OOM错误的原因以及相应的解决方案：</p>
<p>一、OOM错误的原因</p>
<ol>
<li><strong>模型复杂度过高</strong>：随着样本量的增加，模型可能需要更多的参数来捕捉数据中的复杂关系，这会导致模型占用更多的内存。</li>
<li><strong>批量大小（Batch Size）过大</strong>：批量大小直接影响每次迭代中需要处理的数据量，过大的批量会显著增加内存消耗。</li>
<li><strong>数据加载方式</strong>：如果数据加载方式不当，如一次性将所有数据加载到内存中，而不是分批加载，也会导致内存溢出。</li>
<li><strong>内存泄漏</strong>：程序中可能存在内存泄漏的问题，即已分配的内存未得到及时释放，随着训练的进行，内存消耗逐渐增加，最终导致OOM。</li>
<li><strong>系统资源限制</strong>：训练环境本身的资源限制，如物理内存不足、操作系统限制等，也可能导致OOM错误。</li>
</ol>
<p>二、解决方案</p>
<ol>
<li><strong>优化模型结构</strong>：</li>
<li>尝试简化模型结构，减少模型参数数量。</li>
<li>
<p>使用更高效的模型架构，如卷积神经网络（CNN）、循环神经网络（RNN）的变种等。</p>
</li>
<li>
<p><strong>调整批量大小</strong>：</p>
</li>
<li>减小批量大小，以减少每次迭代中需要处理的数据量。</li>
<li>
<p>可以通过梯度累积（Gradient Accumulation）技术来模拟较大的批量效果，同时保持较小的实际批量大小。</p>
</li>
<li>
<p><strong>优化数据加载方式</strong>：</p>
</li>
<li>采用流式处理或生成器（Generator）的方式来加载数据，以实现数据的分批加载和实时处理。</li>
<li>
<p>使用数据增强技术来扩展数据集，同时保持内存占用在可控范围内。</p>
</li>
<li>
<p><strong>检查和修复内存泄漏</strong>：</p>
</li>
<li>使用内存分析工具（如VisualVM、JProfiler等）来检查内存使用情况，找出内存泄漏的源头。</li>
<li>
<p>修正代码中的内存泄漏问题，确保已分配的内存得到及时释放。</p>
</li>
<li>
<p><strong>升级硬件资源</strong>：</p>
</li>
<li>如果条件允许，可以考虑升级服务器的硬件配置，如增加物理内存、使用更快的CPU等。</li>
<li>
<p>使用分布式训练技术，将训练任务分配到多个节点上并行处理，以分散内存压力。</p>
</li>
<li>
<p><strong>调整JVM参数</strong>：</p>
</li>
<li>如果是Java程序，可以通过调整JVM参数（如-Xmx、-Xms等）来优化内存分配和使用。</li>
<li>
<p>确保JVM有足够的堆内存空间来处理训练任务。</p>
</li>
<li>
<p><strong>使用更高效的库和框架</strong>：</p>
</li>
<li>选择经过优化的深度学习库和框架（如TensorFlow、PyTorch等），这些库和框架通常提供了更高效的内存管理和优化算法。</li>
</ol>
<h3 id="2322-llmsft">2.3.22 大模型LLM进行SFT 如何对样本进行优化?<a class="headerlink" href="#2322-llmsft" title="Permanent link">&para;</a></h3>
<p>大模型LLM（Large Language Model）进行SFT（Structured Fine-Tuning，结构化微调）时，对样本的优化是提升模型性能的关键步骤。以下是一些对样本进行优化的主要策略和方法：</p>
<p>一、样本选择与预处理</p>
<ol>
<li><strong>针对性选择样本</strong>：</li>
<li>根据特定任务或领域的需求，选择具有代表性和多样性的样本。这些样本应覆盖任务的主要方面和边缘情况。</li>
<li>优先选择高质量、标注准确的样本，避免使用错误或模糊的标注数据。</li>
<li>
<p>去除重复、冗余或无效的样本，减少噪声数据的干扰。</p>
</li>
<li>
<p><strong>样本预处理</strong>：</p>
</li>
<li>对样本进行格式化处理，确保输入数据的格式与模型训练要求一致。</li>
<li>使用同义词替换、句子重组、回译等方法增加样本的多样性，提高模型的泛化能力。</li>
</ol>
<p>二、标注与质量控制</p>
<ol>
<li><strong>精细标注</strong>：</li>
<li>对样本进行精细标注，确保标注的准确性和一致性。</li>
<li>使用多轮标注和审核机制，提高标注质量。</li>
<li>
<p>定期对标注数据进行质量检查，及时发现并纠正错误标注。</p>
</li>
<li>
<p><strong>建立反馈机制</strong>：</p>
</li>
<li>允许标注人员或专家对标注结果进行反馈和修正。</li>
</ol>
<p>三、加权与采样策略</p>
<ol>
<li><strong>样本加权</strong>：</li>
<li>
<p>根据样本的重要性和难度对样本进行加权处理，使模型在训练过程中更加关注重要或困难的样本。</p>
</li>
<li>
<p><strong>采样策略</strong>：</p>
</li>
<li>采用合适的采样策略，如随机采样、分层采样等，确保训练过程中样本的均匀性和代表性。</li>
<li>在样本量较大的情况下，可以使用批量采样或在线采样的方式来提高训练效率。</li>
</ol>
<p>四、结合模型特性进行优化</p>
<ol>
<li><strong>分析模型特性</strong>：</li>
<li>
<p>深入了解所使用的LLM模型的特性和优势，以便更好地利用这些特性进行样本优化。</p>
</li>
<li>
<p><strong>优化调整</strong>：</p>
</li>
<li>根据模型特性对样本进行优化调整，如调整样本的输入格式、增加与模型特性相关的特征等。</li>
<li>利用模型的特点来设计更有效的训练策略和损失函数，以提高模型的性能。</li>
</ol>
<p>五、持续迭代与反馈</p>
<ol>
<li><strong>监控模型性能</strong>：</li>
<li>
<p>在训练过程中不断监控模型的性能表现，并根据表现进行样本和训练策略的迭代优化。</p>
</li>
<li>
<p><strong>模型评估与测试</strong>：</p>
</li>
<li>定期对模型进行评估和测试，确保模型在目标任务上的性能持续提升。</li>
<li>建立用户反馈机制，收集用户对模型输出的评价和建议，根据用户反馈对样本和模型进行进一步的优化和调整。</li>
</ol>
<p>六、数据增强</p>
<ol>
<li><strong>生成新样本</strong>：</li>
<li>使用数据增强技术生成更多的训练样本，特别是在样本量不足的情况下。这可以通过多种方法实现，如回译、同义词替换、句子重组等。</li>
</ol>
<h3 id="2323">2.3.23 模型参数迭代实验?<a class="headerlink" href="#2323" title="Permanent link">&para;</a></h3>
<p>模型参数迭代实验是数值分析和机器学习等领域中常见的实验之一，旨在通过迭代法求解模型的参数。以下是对模型参数迭代实验的详细解析：</p>
<p>一、实验目的</p>
<ol>
<li><strong>掌握迭代法的基本原理</strong>：了解迭代法如何通过逐步逼近的方式求解模型参数。</li>
<li><strong>提升问题解决能力</strong>：通过实际操作，增强对线性方程组、非线性方程等复杂问题的求解能力。</li>
<li><strong>理解模型优化过程</strong>：在机器学习领域，通过迭代优化算法调整模型参数，提升模型性能。</li>
</ol>
<p>二、实验原理</p>
<p>迭代法是一种逐步逼近真实解的方法，其基本原理是将复杂问题简化为一系列相对简单的子问题，通过不断迭代求解子问题，最终逼近原问题的解。常见的迭代法包括高斯消去法、Jacobi迭代法、Gauss-Seidel迭代法（G-S迭代法）、SOR迭代法等。</p>
<p>三、实验步骤</p>
<ol>
<li>问题定义</li>
</ol>
<p>首先，需要明确求解的问题，如线性方程组AX=b的解，或机器学习模型中需要优化的参数等。</p>
<ol>
<li>
<p>初始化</p>
</li>
<li>
<p><strong>选择迭代法</strong>：根据问题的性质和求解要求，选择合适的迭代法。</p>
</li>
<li>
<p><strong>设置初始值</strong>：为迭代过程设置合理的初始参数值。</p>
</li>
<li>
<p>迭代过程</p>
</li>
<li>
<p><strong>构建迭代公式</strong>：根据选定的迭代法，构建相应的迭代公式。</p>
</li>
<li>
<p><strong>执行迭代</strong>：按照迭代公式，反复计算参数值，直到满足终止条件（如达到预设的迭代次数、参数变化量小于某个阈值等）。</p>
</li>
<li>
<p>结果分析</p>
</li>
<li>
<p><strong>验证解的正确性</strong>：将迭代得到的解与真实解（如果已知）或理论解进行比较，验证其正确性。</p>
</li>
<li><strong>分析收敛性</strong>：观察迭代过程中参数值的变化趋势，分析其收敛性。</li>
<li><strong>讨论影响因素</strong>：探讨不同初始值、迭代法等因素对结果的影响。</li>
</ol>
<p>四、实验示例</p>
<p>以线性方程组AX=b的求解为例，可以采用Jacobi迭代法或G-S迭代法进行求解。以下是Jacobi迭代法的简要步骤：</p>
<ol>
<li><strong>将系数矩阵A分解为D-L-U</strong>：其中D是A的对角线矩阵，L是A的严格下三角矩阵，U是A的严格上三角矩阵。</li>
<li><strong>初始化解向量X</strong>：通常选择全零向量作为初始解。</li>
<li><strong>构建迭代公式</strong>：X^(k+1) = D^(-1) * (b - (L + U) * X<sup>(k))，其中X</sup>(k)表示第k次迭代的解向量。</li>
<li><strong>执行迭代</strong>：按照迭代公式反复计算X^(k+1)，直到满足终止条件。</li>
<li><strong>输出结果</strong>：当迭代停止时，输出最终的解向量X。</li>
</ol>
<p>五、注意事项</p>
<ol>
<li><strong>选择合适的迭代法</strong>：不同的迭代法适用于不同的问题，需要根据问题的性质和求解要求选择合适的迭代法。</li>
<li><strong>设置合理的初始值</strong>：初始值的选择对迭代过程的收敛性和求解速度有很大影响，需要设置合理的初始值。</li>
<li><strong>监控迭代过程</strong>：在迭代过程中，需要监控参数值的变化趋势和收敛情况，以便及时发现问题并调整迭代策略。</li>
<li><strong>注意精度和稳定性</strong>：在迭代求解过程中，需要注意计算精度和稳定性问题，以避免产生过大误差或导致迭代过程发散。</li>
</ol>
<h2 id="24-langchain">2.4 大模型 langchain面<a class="headerlink" href="#24-langchain" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>概念部分</strong></li>
</ul>
<p>### 2.4.1 什么是LangChain?</p>
<p>LangChain是一个用于开发基于大语言模型（LLM）应用程序的强大框架，旨在帮助开发人员简化与大模型交互、数据检索以及将不同功能模块串联起来以完成复杂任务的过程。它由Lang.AI（语言人工智能）开发，是一个开源框架，目前以Python或JavaScript包的形式提供。以下是LangChain的详细介绍：</p>
<p>一、LangChain的核心价值</p>
<ol>
<li><strong>上下文感知能力</strong>：LangChain能够将语言模型连接到上下文源（如提示说明、少量示例、响应内容等），使得应用程序能够更准确地理解用户意图并给出更加准确的回答。</li>
<li><strong>推理能力</strong>：LangChain依赖于语言模型进行推理，根据提供的上下文来回答问题或采取何种行动。它支持多模态、多模型联合推理，通过联合多个预训练模型，提供更强大的语义理解和生成能力。</li>
<li><strong>模块化与易用性</strong>：LangChain的组件都是模块化且易于使用的，支持组合工具、集成环境与语言模型协同运行。它提供了预制链（Off-the-shelf chains），即预先设计和构建好的、可以直接使用的链集合，大大简化了开发过程。</li>
</ol>
<p>二、LangChain的主要组成部分</p>
<ol>
<li><strong>模型（Models）</strong>：LangChain支持各种模型类型和模型集成，包括Google的LaMDA、Meta的LLaMa、OpenAI的GPT-4等。</li>
<li><strong>提示（Prompts）</strong>：包括提示词管理、提示词优化和提示词序列化。提示是模型的输入，一般通过特定的模板组件构建而成，LangChain提供了预先设计好的提示模板，也支持自定义模板。</li>
<li><strong>内存（Memory）</strong>：在链/代理调用之间保持状态的概念。LangChain提供了标准的内存接口、内存实现以及使用内存的链/代理示例。</li>
<li><strong>索引（Indexes）</strong>：与文本数据结合使用时，语言模型往往更加强大。此模块涵盖了执行此操作的最佳实践。</li>
<li><strong>链（Chains）</strong>：不仅仅是单个LLM调用，还包括一系列调用（无论是调用LLM还是不同的实用工具）。LangChain提供了标准的链接口、许多与其他工具的集成，以及用于常见应用程序的端到端的链调用。</li>
<li><strong>代理（Agents）</strong>：涉及LLM做出行动决策、执行该行动、查看一个观察结果，并重复该过程直到完成。LangChain提供了标准的代理接口、一系列可供选择的代理，以及端到端代理的示例。</li>
</ol>
<p>三、LangChain的应用场景</p>
<p>LangChain框架的应用场景非常广泛，几乎涵盖了所有需要语言模型支持的应用领域，包括但不限于：</p>
<ul>
<li><strong>文档分析和摘要</strong>：利用LangChain的上下文感知和推理能力，对大量文档进行自动分析和摘要，提取关键信息。</li>
<li><strong>聊天机器人</strong>：构建具有自然语言处理能力的聊天机器人，实现与用户的自然对话，提供信息咨询、任务执行等功能。</li>
<li><strong>智能助手与自动化</strong>：在办公、教育、医疗等领域，LangChain可以构建智能助手，帮助用户自动化处理日常任务，如日程安排、邮件撰写、研究报告生成等。</li>
<li><strong>代码生成与辅助编程</strong>：帮助开发人员快速生成代码片段、优化代码结构、提供编程建议等。</li>
<li><strong>内容创作与生成</strong>：辅助作家、编辑、广告商等生成高质量的文章、广告文案、社交媒体内容等。</li>
<li><strong>数据科学与分析</strong>：与数据科学工具集成，提供数据清洗、特征工程、模型解释等支持。</li>
<li><strong>法律与合规</strong>：在法律领域，LangChain可以应用于合同审查、法律文档分析、合规性检查等场景。</li>
</ul>
<p>四、LangChain的安装与使用</p>
<p>LangChain是一个Python库，可以通过pip进行安装。在命令行中运行以下命令即可安装LangChain：</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>langchain
</code></pre></div>
<p>安装完成后，开发者可以在自己的项目中导入LangChain库，并使用其提供的组件和接口来开发基于LLM的应用程序。</p>
<h3 id="242-langchain">2.4.2 LangChain 包含哪些核心概念?<a class="headerlink" href="#242-langchain" title="Permanent link">&para;</a></h3>
<p>LangChain 包含以下核心概念：</p>
<ol>
<li>
<p><strong>组件（Components）</strong>：LangChain 提供了模块化的组件抽象，用于处理语言模型所需的各种功能，例如模型输入/输出、检索、代理等。</p>
</li>
<li>
<p><strong>链（Chains）</strong>：链是将组件以特定方式组装的集合，用于完成特定的用例。它们可以包含对语言模型的一系列调用，以及与其他工具的交互。</p>
</li>
<li>
<p><strong>代理（Agents）</strong>：代理涉及语言模型对要采取的行动做出决策，执行这些行动，观察结果，然后根据观察结果重复该过程，直到任务完成。</p>
</li>
<li>
<p><strong>用例特定链（Use-case specific chains）</strong>：这些是为特定任务或用例定制的链，提供了更高级别的接口，使得开发人员可以更容易地开始特定用例的开发。</p>
</li>
<li>
<p><strong>LangChain 库（LangChain Libraries）</strong>：提供Python和JavaScript的库，包含组件接口和集成，以及基本的运行时环境，用于组合组件。</p>
</li>
<li>
<p><strong>LangChain 模板（LangChain Templates）</strong>：提供一系列易于部署的参考架构，用于快速启动各种任务。</p>
</li>
<li>
<p><strong>LangServe</strong>：一个库，用于将LangChain链部署为REST API，使得链可以通过网络服务的形式被访问。</p>
</li>
<li>
<p><strong>LangSmith</strong>：一个开发者平台，用于调试、测试、评估和监控基于任何大型语言模型（LLM）框架构建的链，并且与LangChain无缝集成。</p>
</li>
<li>
<p><strong>LangChain 表达式语言（LCEL）</strong>：一种声明性的组合链的方式，支持从原型到生产的过程，无需更改代码。</p>
</li>
<li>
<p><strong>内存（Memory）</strong>：指链/代理调用之间保存状态的概念，LangChain提供标准的内存接口和实现。</p>
</li>
<li>
<p><strong>自定义工具（Custom Tools）</strong>：允许开发者定义自己的工具，以提供给代理使用，增强应用程序的功能。</p>
</li>
<li>
<p><strong>错误处理（Error Handling）</strong>：LangChain 提供了错误处理机制，例如 <code>ToolException</code>，以及通过 <code>handle_tool_error</code> 属性来自定义错误处理策略。</p>
</li>
</ol>
<p>这些概念共同构成了LangChain框架的基础，使其成为一个强大且灵活的开发平台，适用于构建各种由语言模型驱动的应用程序。</p>
<h3 id="243-langchain-agent">2.4.3 什么是LangChain Agent?<a class="headerlink" href="#243-langchain-agent" title="Permanent link">&para;</a></h3>
<p>LangChain Agent（LangChain代理）是LangChain框架中的一个核心概念，它代表了一种能够利用语言模型（LLM）和其他工具来执行复杂任务的系统。以下是LangChain Agent的详细解释：</p>
<p>一、定义与功能</p>
<ul>
<li><strong>定义</strong>：在LangChain中，Agent是一种处理自然语言任务的软件实体或模型组件，它能够根据输入的指令和上下文信息，调用不同的工具（如搜索引擎、数据库查询、API调用等）来获取所需的信息，并将这些信息整合起来形成最终的响应。</li>
<li><strong>功能</strong>：Agent在LangChain中扮演着一个协调者和决策者的角色，它能够根据给定的任务和目标，决定使用哪些工具以及如何组合这些工具来达到目的。Agent的设计目的是为了处理那些简单的语言模型可能无法直接解决的问题，尤其是当这些任务涉及到多个步骤或者需要外部数据源的情况。</li>
</ul>
<p>二、核心思想</p>
<ul>
<li><strong>操作选择</strong>：Agent使用语言模型来选择一系列要执行的操作（AgentAction），这些操作代表了代理应采取的具体步骤。</li>
<li><strong>动态选择</strong>：与Chain（链）的固定路径不同，Agent可以根据上下文动态选择工具和执行策略，这使得它能够更灵活地应对各种复杂场景。</li>
</ul>
<p>三、组成与结构</p>
<ul>
<li><strong>工具（Tools）</strong>：Agent由多个工具组成，每个工具负责单一任务，如Web搜索、数据库查询等。工具是Agent执行单个任务的主要组件。</li>
<li><strong>工具包（Toolkits）</strong>：工具包是工具与预定义操作的组合，可以在Agent中使用。它提供了一种便捷的方式来组织和管理工具集合。</li>
<li><strong>推理引擎</strong>：语言模型在Agent中作为推理引擎使用，它根据输入的指令和上下文信息来确定要执行哪些操作以及按什么顺序执行。</li>
</ul>
<p>四、适用场景</p>
<ul>
<li><strong>网络搜索</strong>：Agent可以集成搜索引擎工具，根据用户输入自动搜索相关信息并生成响应。</li>
<li><strong>嵌入式搜索</strong>：在特定领域或应用中嵌入搜索功能，提供更精确的搜索结果。</li>
<li><strong>API集成</strong>：Agent可以调用外部API获取数据或执行特定操作，以完成复杂任务。</li>
</ul>
<p>五、创建与使用</p>
<ul>
<li><strong>定义工具</strong>：首先，需要定义Agent将使用的工具集合，这些工具可以是内置的也可以是自定义的。</li>
<li><strong>初始化执行器</strong>：使用AgentExecutor来初始化Agent的执行环境，包括加载工具和设置执行参数。</li>
<li><strong>设置提示词</strong>：为Agent设置合适的提示词，以引导语言模型的行为和输出。</li>
<li><strong>执行任务</strong>：通过调用AgentExecutor的invoke方法传入输入指令，Agent将执行一系列操作并生成最终响应。</li>
</ul>
<p>### 2.4.4 如何使用LangChain?</p>
<p>使用 LangChain 构建应用程序通常涉及以下步骤：</p>
<ol>
<li>
<p><strong>了解 LangChain 组件</strong>：熟悉 LangChain 提供的各种组件，例如模型 I/O、检索、代理、内存和链。</p>
</li>
<li>
<p><strong>设置环境</strong>：根据你的开发环境和需求，安装 LangChain 库。LangChain 支持 Python 和 JavaScript。</p>
</li>
<li>
<p><strong>定义问题或用例</strong>：明确你想要解决的问题或你想要实现的用例。</p>
</li>
<li>
<p><strong>选择或创建组件</strong>：根据你的用例，选择或创建必要的 LangChain 组件。这可能包括提示模板、模型集成、数据检索器等。</p>
</li>
<li>
<p><strong>构建链（Chains）</strong>：使用 LangChain 的组件来构建链，这些链定义了如何将组件组合起来以完成特定的任务。</p>
</li>
<li>
<p><strong>创建代理（Agents）</strong>：如果需要，创建代理来执行更复杂的任务，这可能包括决策制定和工具使用。</p>
</li>
<li>
<p><strong>实现自定义逻辑</strong>：根据需要实现自定义逻辑，例如自定义函数或工具。</p>
</li>
<li>
<p><strong>测试和迭代</strong>：测试你的应用程序，并根据反馈进行迭代改进。</p>
</li>
<li>
<p><strong>部署</strong>：当你的应用程序准备好后，使用 LangServe 或其他方法将其部署为服务。</p>
</li>
<li>
<p><strong>监控和评估</strong>：使用 LangSmith 或其他工具来监控和评估你的应用程序的性能。</p>
</li>
</ol>
<p>下面是一个简化的使用 LangChain 的示例流程：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 安装 LangChain</span>
<span class="c1"># pip install langchain</span>

<span class="c1"># 导入 LangChain 库</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># 创建语言模型实例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">()</span>

<span class="c1"># 定义一个提示模板</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;What is the capital of </span><span class="si">{country}</span><span class="s2">?&quot;</span>

<span class="c1"># 创建一个提示实例</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">)</span>

<span class="c1"># 构建链，将提示和模型组合起来</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span>

<span class="c1"># 执行链，获取结果</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;France&quot;</span><span class="p">})</span>

<span class="c1"># 打印结果</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div>
<p>这只是一个基础示例。LangChain 的实际使用可能会更复杂，涉及到更多的组件和自定义逻辑。你可以通过阅读 LangChain 的官方文档和教程来获取更详细的指导和高级用法。</p>
<p>### 2.4.5 LangChain 支持哪些功能?</p>
<p>LangChain 支持一系列功能，以帮助开发者构建由语言模型驱动的应用程序。以下是一些主要功能：</p>
<ol>
<li>
<p><strong>语言模型集成</strong>：支持与各种大型语言模型（LLMs）的集成，包括但不限于 OpenAI 的 GPT 系列模型。</p>
</li>
<li>
<p><strong>模块化组件</strong>：提供模块化的组件抽象，包括模型输入/输出、检索、代理、内存等，以支持构建复杂的应用程序。</p>
</li>
<li>
<p><strong>用例特定链</strong>：允许开发者根据特定用例组装组件，形成高效的工作流程。</p>
</li>
<li>
<p><strong>代理（Agents）</strong>：支持创建代理，这些代理能够理解指令、做出决策、执行操作，并根据结果进行迭代。</p>
</li>
<li>
<p><strong>自定义函数和工具</strong>：允许开发者定义和使用自定义函数和工具，以增强应用程序的功能。</p>
</li>
<li>
<p><strong>LangChain 表达式语言（LCEL）</strong>：提供一种声明性的方式来组合链，支持从原型到生产的过程。</p>
</li>
<li>
<p><strong>内存管理</strong>：提供标准的内存接口，允许在链或代理调用之间保存和使用状态。</p>
</li>
<li>
<p><strong>错误处理</strong>：提供错误处理机制，允许开发者自定义错误处理策略。</p>
</li>
<li>
<p><strong>部署支持</strong>：通过 LangServe 支持将 LangChain 链部署为 REST API。</p>
</li>
<li>
<p><strong>开发者平台</strong>：通过 LangSmith 提供开发者平台，支持调试、测试、评估和监控应用程序。</p>
</li>
<li>
<p><strong>模板和示例</strong>：提供易于部署的参考架构和示例，帮助开发者快速启动项目。</p>
</li>
<li>
<p><strong>生态系统集成</strong>：作为丰富的工具生态系统的一部分，LangChain 支持与多种工具和服务的集成。</p>
</li>
<li>
<p><strong>API 参考</strong>：提供完整的 API 参考文档，方便开发者查找和使用 LangChain 的类和方法。</p>
</li>
<li>
<p><strong>安全性和最佳实践</strong>：提供安全最佳实践指南，帮助开发者安全地使用 LangChain 进行开发。</p>
</li>
<li>
<p><strong>社区和支持</strong>：通过 LangChainHub、Discord 等社区资源，提供分享、探索和讨论的平台。</p>
</li>
</ol>
<p>这些功能共同构成了 LangChain 的强大能力，使其成为一个灵活、可扩展的开发框架，适用于各种语言模型驱动的应用程序。</p>
<p>### 2.4.6 什么是LangChain model?</p>
<p>这些模型通常是大型语言模型（LLM）或聊天模型（Chat Model），它们构成了LangChain框架中自然语言处理任务的基础。</p>
<p>LangChain框架支持多种类型的模型，包括但不限于：</p>
<ol>
<li><strong>大型语言模型（LLM）</strong>：</li>
<li>这些模型是许多语言模型应用程序的支柱，它们将文本字符串作为输入并返回文本字符串作为输出。</li>
<li>LLM能够理解和生成自然语言文本，支持多种语言处理任务，如文本生成、文本分类、情感分析等。</li>
<li>
<p>LangChain为LLM提供了通用的接口，降低了开发者的学习成本，使得开发者能够快速地开发复杂的LLM应用。</p>
</li>
<li>
<p><strong>聊天模型（Chat Model）</strong>：</p>
</li>
<li>聊天模型是语言模型的一个变体，它们以语言模型为基础，但具有更结构化的API。</li>
<li>聊天模型不再以简单的文本字符串为输入和输出，而是将聊天信息列表作为输入和输出，这使得它们更适合于构建聊天机器人等应用。</li>
<li>LangChain支持聊天模型，并提供了与聊天模型交互的接口和工具。</li>
</ol>
<p>除了上述两种主要的模型类型外，LangChain还可能支持其他类型的模型，具体取决于框架的版本和更新情况。</p>
<p>LangChain框架通过提供一套工具、组件和接口，帮助开发人员轻松地构建和管理与语言模型的交互。这些工具包括提示模板（PromptTemplate）、输出解析器（Output Parser）等，它们使得开发人员能够根据需要定制和优化模型的输入和输出，以实现特定的自然语言处理任务。</p>
<p>此外，LangChain还支持链（Chain）和代理（Agent）等高级功能，这些功能允许开发人员将多个模型或组件链接在一起，形成复杂的自然语言处理流程。通过链和代理，开发人员可以构建出功能强大的自然语言处理应用程序，如智能问答系统、文档摘要工具、聊天机器人等。</p>
<h3 id="247-langchain">2.4.7 LangChain 包含哪些特点?<a class="headerlink" href="#247-langchain" title="Permanent link">&para;</a></h3>
<p>LangChain作为一个基于语言模型开发应用程序的强大框架，具有多个显著的特点，这些特点使其在自然语言处理（NLP）领域具有广泛的应用和重要的价值。以下是LangChain的主要特点：</p>
<ol>
<li>支持多种大语言模型</li>
</ol>
<p>LangChain支持多种预训练的大型语言模型（LLM）和聊天模型（Chat Model），如GPT-3.5、GPT-4、BERT、Meta的LLaMa等。这使得开发者能够根据需要选择合适的模型，构建出符合应用场景需求的语言处理系统。</p>
<ol>
<li>丰富的工具和库</li>
</ol>
<p>LangChain集成了大量用于处理自然语言任务的工具和库，包括文本清洗、分词、特征提取、模型训练等。这些工具和库都经过精心设计和优化，帮助开发者更加高效地完成大语言模型的构建和训练。</p>
<ol>
<li>模块化设计</li>
</ol>
<p>LangChain采用模块化设计，将不同的功能和组件封装成独立的模块，如模型（Models）、提示（Prompts）、内存（Memory）、链（Chains）、代理（Agents）等。这种设计使得开发者可以轻松地组合和定制这些模块，以构建出符合自己需求的语言处理系统。</p>
<ol>
<li>高度集成和灵活性</li>
</ol>
<p>LangChain提供了丰富的接口和工具，方便开发者将LLM与外部计算和数据来源结合起来，构建端到端的应用程序。它支持多种编程语言和平台，如Python和TypeScript，使得开发者能够根据自己的喜好和项目需求选择合适的开发环境。</p>
<ol>
<li>上下文感知和推理能力</li>
</ol>
<p>LangChain具有强大的上下文感知和推理能力，能够将多个预训练模型进行联合推理，从而提供更强大的语义理解和生成能力。这使得它能够处理复杂的自然语言任务，如对话生成、文本摘要、知识问答等。</p>
<ol>
<li>可视化工具</li>
</ol>
<p>为了方便开发者更好地理解和调试大语言模型，LangChain还提供了一系列的可视化工具，如模型训练过程的可视化、模型输出的可视化等。这些工具可以帮助开发者更好地了解模型的性能和表现，及时发现和解决问题。</p>
<ol>
<li>社区支持和不断更新</li>
</ol>
<p>LangChain是一个开源项目，拥有活跃的社区支持和不断更新的特性。开发者可以通过社区获取帮助和分享经验，同时也可以关注LangChain的更新和扩展，以便更好地利用这个框架来构建自己的语言处理系统。</p>
<h3 id="248-langchain">2.4.8 LangChain 存在哪些问题及方法方案?<a class="headerlink" href="#248-langchain" title="Permanent link">&para;</a></h3>
<p>LangChain 是一个用于构建由语言模型驱动的应用程序的框架，它提供了模块化的组件和用例特定的链，以支持开发者创建智能应用程序。然而，LangChain 也面临一些问题和挑战。</p>
<p>首先，LangChain 的抽象程度被一些用户认为过高，这在初期可能简化开发，但随着需求的复杂化，这种高级抽象可能导致代码难以理解和维护，增加了开发人员在理解和调试上的负担。此外，LangChain 的不灵活性在面对复杂需求时可能成为阻碍生产力和创新的限制因素。</p>
<p>一些团队和技术评论者指出，LangChain 在生产环境中使用时存在问题，例如难以根据特定业务逻辑动态更改代理的可用工具，以及缺乏从外部观察代理状态的方法。这些问题导致一些团队决定停止使用 LangChain，并采用更灵活的方法来构建他们的应用程序。</p>
<p>针对这些问题，LangChain 可能需要进一步的改进和调整，以提高其灵活性和可定制性，从而更好地适应不同开发团队的需求。同时，开发者在考虑使用 LangChain 时，应该权衡其提供的工具和组件的便利性与可能带来的限制和复杂性。</p>
<p>另外，一些用户认为 LangChain 的学习曲线相对较高，特别是对于初次接触大型语言模型和相关技术栈的开发者。而且，集成多个外部系统和工具可能会增加项目的复杂性，管理这些依赖关系和确保它们之间的兼容性可能成为一项挑战。性能与资源消耗也是考虑因素之一，大型语言模型本身在处理复杂任务时可能消耗较多计算资源，LangChain 虽然提供了优化手段，但在某些应用场景下，资源管理和成本控制仍需精细考虑。</p>
<h3 id="249-langchain">2.4.9 LangChain 替代方案?<a class="headerlink" href="#249-langchain" title="Permanent link">&para;</a></h3>
<p>LangChain 是一个流行的框架，用于构建由语言模型驱动的应用程序，但它也有一些替代品。以下是一些 LangChain 的替代方案：</p>
<ol>
<li><strong>Auto-GPT</strong>：一个旨在使 GPT-4 成为自给自足的聊天 AI 的项目，专注于执行代码和命令来解决问题 。</li>
<li><strong>LlamaIndex</strong>：这是一个多功能的数据管理工具，可以从多种来源提取数据，并优化数据格式以供 LLMs 更好地理解 。</li>
<li><strong>Simpleaichat</strong>：一个专为聊天应用设计的 Python 包，简化了代码并保持了功能强大 。</li>
<li><strong>Outlines</strong>：一个允许开发者精确控制文本生成的工具，提供多种生成方法以保证输出符合特定模式 。</li>
<li><strong>BabyAGI</strong>：一个 Python 脚本，使用 AI 管理任务，结合了 OpenAI、LangChain 和一些向量数据库 。</li>
<li><strong>AgentGPT</strong>：为企业设计的解决方案，通过网页浏览器引入自给自足的 AI 代理 。</li>
<li><strong>MetaGPT</strong>：一个多代理框架，目标是运行一个整个的软件开发公司 。</li>
<li><strong>AutoChain</strong>：结合了 LangChain 和 AutoGPT 的创新方法，为开发者提供灵活的框架来创建代理 。</li>
<li><strong>PromptChainer</strong>：帮助创建 AI 驱动的流程，管理 AI 生成的洞察力 。</li>
<li><strong>TikToken</strong> TikToken 是 OpenAI 开发的 Python 库，它提供了一种简单且高效的方法来计算文本字符串中的令牌，无需使用像 LangChain 这样的复杂框架来完成特定任务。TikToken 可以更直接地处理令牌计数问题，提高处理小数据集时的效率。</li>
<li><strong>Deepset Haystack</strong> Deepset Haystack 是一个开源框架，用于使用大型语言模型构建搜索和问答应用程序。它基于 Hugging Face Transformers，提供了多种查询和理解文本数据的工具。Haystack 旨在简化搜索和问答系统的开发过程，并提供了丰富的功能和定制选项，以满足不同应用场景的需求。</li>
</ol>
<p>这些替代品提供了不同的功能和优势，可以根据你的具体需求和偏好选择使用。例如，如果你需要一个能够处理多种数据类型并优化数据格式的工具，可以考虑使用 LlamaIndex 。如果你更倾向于一个简单易用的聊天应用框架，Simpleaichat 可能是一个不错的选择 。</p>
<ul>
<li><strong>基础技术部分</strong></li>
</ul>
<p>### 2.4.11 LangChain 中 Components and Chains 是什么?</p>
<p>在 LangChain 中，"Components"（组件）和 "Chains"（链）是两个核心概念，它们共同构成了构建应用程序的基础。</p>
<p>一、Components（组件）</p>
<p>Components 是 LangChain 中用于执行特定任务的独立模块。它们是可重用的构建块，可以与其他组件结合以创建更复杂的功能。以下是 LangChain 中一些主要类型的组件：</p>
<ol>
<li><strong>Prompt Templates</strong>（提示模板）: 用于生成语言模型的输入提示。</li>
<li><strong>Output Parsers</strong>（输出解析器）: 解析语言模型的输出，以便进一步处理。</li>
<li><strong>Chains</strong>（链）: 将多个组件按顺序执行的集合。</li>
<li><strong>LLMs (Language Models)</strong>（语言模型）: 执行语言理解或生成任务的大型语言模型。</li>
<li><strong>Indexes</strong>（索引）: 用于存储和检索信息的数据库或搜索引擎。</li>
<li><strong>Agents</strong>（代理）: 能够执行任务、做出决策并采取行动的智能体。</li>
<li><strong>Memory</strong>（内存）: 用于在多个交互中保持状态和上下文的组件。</li>
</ol>
<p>二、 Chains（链）</p>
<p>Chains 是 LangChain 中的一系列有序的组件调用，它们共同完成一个特定的任务或工作流程。链可以看作是组件的管道，其中每个组件对数据进行处理，并将其传递给链中的下一个组件。以下是 LangChain 中链的一些关键特点：</p>
<ol>
<li><strong>有序性</strong>: 链中的组件按照特定的顺序执行，数据在组件之间流动。</li>
<li><strong>可定制性</strong>: 开发者可以根据需要选择和组合不同的组件来创建链。</li>
<li><strong>可重用性</strong>: 一旦定义了一个链，它可以在多个地方重用，提高开发效率。</li>
<li><strong>灵活性</strong>: 链可以根据不同的用例进行调整，以适应不同的需求。</li>
</ol>
<p>三、Components 和 Chains 的关系</p>
<p>组件和链之间的关系可以类比于构建模块和结构。组件是构建模块，它们提供了基本的功能；而链则是使用这些模块构建的结构，它们定义了组件如何协同工作以解决更复杂的问题。</p>
<p>在 LangChain 中，开发者可以定义自己的组件，然后将这些组件组合成链，以创建满足特定需求的应用程序。例如，一个简单的问答应用程序可能包括以下组件链：</p>
<ol>
<li><strong>Prompt Template</strong>: 创建一个提示模板，用于向用户提问。</li>
<li><strong>LLM</strong>: 使用语言模型生成答案。</li>
<li><strong>Output Parser</strong>: 解析模型的输出，提取有用的信息。</li>
</ol>
<p>通过这种方式，LangChain 允许开发者以模块化和灵活的方式构建复杂的应用程序。</p>
<p>### 2.4.12 LangChain 中 Prompt Templates and Values 是什么?
在 LangChain 中，"Prompt Templates"（提示模板）和 "Values"（值）是构建与语言模型交互的提示（prompts）的关键元素。</p>
<p>一、Prompt Templates（提示模板）
提示模板是用于生成发送给语言模型的输入文本的蓝图。它们通常包含一些占位符，这些占位符在运行时会被具体的值所替换。提示模板的目的是标准化与语言模型的交互，确保输入的一致性和预期的结构。提示模板可以包含文本、问题或指令，它们被设计为引导语言模型以特定的方式响应。</p>
<p>例如，一个简单的提示模板可能是：
<div class="highlight"><pre><span></span><code>&quot;What is the capital of {country}?&quot;
</code></pre></div>
在这个模板中，<code>{country}</code> 是一个占位符，它将在实际使用时被具体的国家名称所替换。</p>
<p>二、Values（值）
值是提示模板中占位符的具体替换内容。在使用提示模板生成最终的提示文本时，你需要提供相应的值来填充模板中的占位符。这些值可以是动态的，根据用户的输入或其他上下文信息变化。</p>
<p>继续上面的例子，如果用户想知道法国的首都，那么替换后的值将是 "France"，生成的最终提示将是：
<div class="highlight"><pre><span></span><code>&quot;What is the capital of France?&quot;
</code></pre></div></p>
<p>三、 Prompt Templates 和 Values 的关系
提示模板和值之间的关系是模板化和数据填充的关系。模板定义了提示的结构和所需的信息类型，而值提供了模板中所需的具体数据。这种分离允许开发者创建灵活的系统，可以根据不同的情况生成不同的提示，而不需要每次都从头开始编写文本。</p>
<p>在 LangChain 的上下文中，这种机制允许开发者构建复杂的交互式应用程序，其中语言模型可以根据用户提供的信息或应用程序的内部状态生成响应。通过使用提示模板和值，可以确保与语言模型的交互既灵活又一致。</p>
<p>### 2.4.13 LangChain 中 Example Selectors 是什么?</p>
<p>LangChain中的Example Selectors是一个关键组件，它允许用户为模型提供示例输入和输出，以帮助模型学习执行特定的任务。以下是关于Example Selectors的详细解释：</p>
<p>一、定义与功能</p>
<ul>
<li><strong>定义</strong>：Example Selectors是LangChain框架中的一个组件，用于选择和提供示例数据给模型，以便模型能够学习或优化其执行特定任务的能力。</li>
<li><strong>功能</strong>：</li>
<li><strong>训练新模型</strong>：为全新的模型提供训练数据，让模型学习执行给定的任务。</li>
<li><strong>调优现有模型</strong>：对于现有的模型，Example Selectors可以提供新的示例来继续训练和调优模型，提高其在特定任务上的表现。</li>
<li><strong>测试模型</strong>：也可以用来提供测试用例，评估模型在给定任务上的性能。</li>
<li><strong>说明模型能力</strong>：通过Example Selectors，用户可以直观地了解模型的能力范围，看它能处理什么样的输入并给出什么样的输出。</li>
<li><strong>调试模型</strong>：如果模型在某些示例上表现不佳，Example Selectors可以帮助用户定位并解决问题。</li>
</ul>
<p>二、使用场景</p>
<ul>
<li>在与大语言模型（LLMs）交互时，Example Selectors尤其有用。当需要模型从大量示例中学习和生成特定类型的输出时，通过Example Selectors可以高效地选择和提供这些示例。</li>
<li>对于知识密集型任务，如问答系统、文本摘要等，Example Selectors可以帮助模型更好地理解问题背景，并基于相关示例生成更准确的回答。</li>
</ul>
<p>三、实现方式</p>
<ul>
<li>在LangChain中，用户可以通过自定义Example Selector来满足特定需求。自定义Example Selector通常需要继承自BaseExampleSelector，并实现add_example和select_examples两个抽象方法。</li>
<li><strong>add_example</strong>：用于向selector中添加一个示例。</li>
<li><strong>select_examples</strong>：根据输入，从已添加的示例中选出要使用的示例。</li>
<li>通过这种方式，用户可以灵活地控制哪些示例被用于模型的训练、调优或测试过程中。</li>
</ul>
<p>四、优势</p>
<ul>
<li><strong>提高模型性能</strong>：通过提供有针对性的示例数据，可以帮助模型更快地学习和优化其性能。</li>
<li><strong>降低成本</strong>：在使用第三方大语言模型时，通过合理选择和提供示例数据，可以降低因过多无效输入而产生的费用。</li>
<li><strong>增强可解释性</strong>：通过查看模型在特定示例上的表现，用户可以更直观地了解模型的能力和局限性。</li>
</ul>
<p>### 2.4.14 LangChain 中 Output Parsers 是什么?</p>
<p>LangChain 中的 <code>Output Parsers</code> 是一个组件，它用于解析模型生成的输出。在自然语言处理（NLP）任务中，模型可能会生成复杂的输出，例如长文本、列表、表格或其他结构化数据。<code>Output Parsers</code> 的作用是将这些复杂的输出转换成更易于理解和使用的格式。</p>
<p><code>Output Parsers</code> 可以定制化以满足特定的解析需求，例如：</p>
<ul>
<li>从文本中提取特定的信息，如日期、地点、人名等。</li>
<li>将输出转换为JSON或XML等结构化格式。</li>
<li>对输出进行语义解析，以执行进一步的逻辑操作或数据存储。</li>
</ul>
<p>在LangChain框架中，<code>Output Parsers</code> 通常与 <code>Prompts</code> 和 <code>Example Selectors</code> 结合使用，以创建一个完整的NLP工作流。通过精心设计的提示（Prompts），模型可以生成预期的输出，然后 <code>Output Parsers</code> 将这些输出解析成所需的格式。</p>
<p>例如，如果你正在构建一个聊天机器人，你可能需要一个 <code>Output Parser</code> 来解析模型生成的回复，并确保它们是合适的、语法正确的，并且符合对话的上下文。或者，如果你正在构建一个信息提取系统，<code>Output Parser</code> 可能会用于从文本中提取关键信息并将其存储在数据库中。</p>
<p>LangChain 提供了灵活性，允许开发者根据他们的特定需求实现自定义的 <code>Output Parser</code>。这样，无论是简单的文本清理还是复杂的数据提取任务，都可以有效地实现。</p>
<p>### 2.4.15 LangChain 中 Indexes and Retrievers 是什么?</p>
<p>在LangChain框架中，<code>Indexes</code> 和 <code>Retrievers</code> 是两个关键组件，它们用于管理和检索大量的数据或文档。</p>
<p>一、 Indexes
<code>Indexes</code> 是一种数据结构，用于存储和组织信息，以便可以快速检索。在LangChain中，索引通常用于以下目的：
- <strong>存储文档</strong>：将文本、问题和答案等数据存储在索引中，以便于后续检索。
- <strong>快速搜索</strong>：提供快速的搜索功能，以便用户可以迅速找到所需的信息。
- <strong>数据组织</strong>：通过索引，数据可以按照不同的标准（如日期、作者、主题等）进行组织。</p>
<p>二、 Retrievers
<code>Retrievers</code> 是执行检索操作的组件，它们使用索引来查找和提取信息。<code>Retrievers</code> 的功能包括：
- <strong>基于查询的检索</strong>：根据用户的查询，从索引中检索相关的文档或信息。
- <strong>过滤和排序</strong>：对检索结果进行过滤和排序，以提供最相关的结果。
- <strong>高效访问</strong>：确保用户可以高效地访问存储在索引中的信息。</p>
<p>在LangChain中，<code>Indexes</code> 和 <code>Retrievers</code> 通常与以下场景结合使用：
- <strong>问答系统</strong>：在问答系统中，<code>Retrievers</code> 可以根据用户的问题从索引中检索最相关的答案。
- <strong>信息检索</strong>：在需要从大量文档中检索特定信息的场景中，<code>Retrievers</code> 可以快速找到并返回相关信息。
- <strong>知识库管理</strong>：在构建知识库时，<code>Indexes</code> 可以帮助组织和管理知识库中的信息，而 <code>Retrievers</code> 则用于检索知识库中的条目。</p>
<p>LangChain支持多种类型的索引和检索器，包括但不限于基于向量的索引（如使用嵌入向量进行相似性搜索的索引）和传统的基于文本的索引。开发者可以根据具体需求选择合适的索引和检索策略，以实现最优的检索效果。</p>
<p>### 2.4.16 LangChain 中 Chat Message History 是什么?</p>
<p>在LangChain框架中，<code>Chat Message History</code> 指的是在聊天应用或对话系统中，用于存储和管理对话历史记录的组件或机制。这个历史记录包括用户和聊天机器人之间交换的所有消息，它可以用于多种目的：</p>
<ol>
<li>
<p><strong>上下文理解</strong>：通过分析对话历史，聊天机器人可以更好地理解当前对话的上下文，从而提供更准确和相关的响应。</p>
</li>
<li>
<p><strong>个性化体验</strong>：<code>Chat Message History</code> 可以帮助聊天机器人记住用户的偏好和需求，从而提供更加个性化的服务。</p>
</li>
<li>
<p><strong>对话连贯性</strong>：对话历史确保了对话的连贯性，使得聊天机器人能够在对话过程中保持话题的一致性。</p>
</li>
<li>
<p><strong>数据挖掘和分析</strong>：对话历史可以用于分析用户行为，提取常见问题，优化聊天机器人的性能。</p>
</li>
<li>
<p><strong>训练数据</strong>：对话历史可以作为训练数据，帮助改进聊天机器人的模型，使其更加智能。</p>
</li>
<li>
<p><strong>用户行为追踪</strong>：在某些情况下，对话历史还可以用于追踪用户的行为模式，以提供定制化的推荐或服务。</p>
</li>
</ol>
<p>在实现上，<code>Chat Message History</code> 可以是一个简单的列表或数组，其中存储了对话中每条消息的文本、发送者（用户或机器人）、时间戳等信息。更高级的实现可能包括对消息的语义分析、情感分析等，以提供更深层次的理解和响应。</p>
<p>LangChain作为一个灵活的框架，允许开发者根据自己的需求来设计和实现<code>Chat Message History</code>，以支持复杂的对话管理和分析任务。</p>
<p>### 2.4.17 LangChain 中 Agents and Toolkits 是什么?</p>
<p>在LangChain框架中，<code>Agents</code> 和 <code>Toolkits</code> 是两个关键概念，它们共同构成了LangChain生态系统的核心部分。</p>
<p>一、Agents</p>
<p><code>Agents</code> 在LangChain中指的是执行特定任务或操作的智能体。这些智能体可以是独立的，也可以是协作的，它们可以执行以下类型的任务：
- <strong>数据检索</strong>：从数据库或知识库中检索信息。
- <strong>信息提取</strong>：从文本中提取关键信息或实体。
- <strong>自然语言理解</strong>：理解用户的查询或指令的意图。
- <strong>对话管理</strong>：在多轮对话中维护上下文和连贯性。
- <strong>任务执行</strong>：执行特定的业务逻辑或用户请求的任务。</p>
<p><code>Agents</code> 可以被设计为具有不同的能力和权限，它们可以相互通信和协作，以解决复杂的用户需求。</p>
<p>二、Toolkits</p>
<p><code>Toolkits</code> 是LangChain提供的工具集合，它们为开发者提供了一系列的工具和库，以便构建和集成各种智能应用。<code>Toolkits</code> 包括但不限于：
- <strong>数据存储和检索工具</strong>：用于管理大量数据的存储和快速检索。
- <strong>自然语言处理工具</strong>：包括语言模型、文本分析、情感分析等。
- <strong>机器学习工具</strong>：用于训练和部署机器学习模型。
- <strong>对话管理工具</strong>：用于构建和管理对话流程。
- <strong>集成开发环境</strong>：提供代码编辑、调试和部署的工具。</p>
<p><code>Toolkits</code> 旨在简化开发过程，提供标准化的接口和组件，使开发者能够快速构建和部署智能应用。</p>
<p>三、Agents 和 Toolkits 的关系</p>
<p><code>Agents</code> 和 <code>Toolkits</code> 在LangChain中是相互依赖的。<code>Agents</code> 利用 <code>Toolkits</code> 提供的工具和能力来执行任务，而 <code>Toolkits</code> 则为 <code>Agents</code> 提供了执行任务所需的基础设施和支持。这种设计允许LangChain生态系统具有高度的灵活性和可扩展性，开发者可以根据具体需求选择合适的 <code>Agents</code> 和 <code>Toolkits</code> 来构建定制化的解决方案。</p>
<p>通过结合使用 <code>Agents</code> 和 <code>Toolkits</code>，LangChain支持构建复杂的智能系统，这些系统可以处理各种复杂的任务，从简单的信息检索到复杂的对话管理和任务自动化。</p>
<p>### 2.4.18 LangChain 如何调用LLMs生成回复?</p>
<p>LangChain调用LLMs（大语言模型）生成回复的过程相对直接且灵活，主要通过其提供的API接口和工具集来实现。以下是一个概括性的步骤说明：</p>
<ol>
<li>初始化LLM实例</li>
</ol>
<p>首先，你需要根据所使用的LLM（如OpenAI、Cohere、Hugging Face等）初始化一个LLM实例。这通常涉及到设置LLM的API密钥或其他必要的认证信息。</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">openai_api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">)</span>
</code></pre></div>
<ol>
<li>构造提示（Prompt）</li>
</ol>
<p>接下来，你需要构造一个提示（Prompt），这是你将发送给LLM的输入文本。这个提示应该清晰地描述你想要LLM完成的任务或生成的内容。</p>
<div class="highlight"><pre><span></span><code><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;请给我写一首关于夏天的诗。&quot;</span>
</code></pre></div>
<ol>
<li>调用LLM生成回复</li>
</ol>
<p>有了LLM实例和提示之后，你就可以调用LLM的方法来生成回复了。LangChain提供了多种方式来调用LLM，包括直接调用和异步调用等。</p>
<ul>
<li><strong>直接调用</strong>：</li>
</ul>
<p>直接调用LLM的<code>generate</code>或类似方法，并传入提示作为参数。</p>
<div class="highlight"><pre><span></span><code><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">([</span><span class="n">prompt</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</code></pre></div>
<p>注意：<code>generate</code>方法的具体用法可能因LLM的不同而有所差异，上述代码以OpenAI为例。</p>
<ul>
<li><strong>异步调用</strong>：</li>
</ul>
<p>如果你需要同时调用多个LLM或处理大量请求，可以使用异步调用来提高效率。LangChain支持异步调用OpenAI等LLM。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">asyncio</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">async_generate</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">):</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">agenerate</span><span class="p">([</span><span class="n">prompt</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># 假设你已经有了llm实例</span>
<span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
<span class="n">loop</span><span class="o">.</span><span class="n">run_until_complete</span><span class="p">(</span><span class="n">async_generate</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">))</span>
</code></pre></div>
<ol>
<li>处理回复</li>
</ol>
<p>最后，你需要处理LLM生成的回复。这可能包括解析回复内容、提取有用信息、格式化输出等。LangChain提供了输出解析（Output Parsers）等工具来帮助你完成这些任务。</p>
<ol>
<li>附加功能</li>
</ol>
<p>除了基本的调用LLM生成回复外，LangChain还提供了许多附加功能来增强LLM的应用效果，如：</p>
<ul>
<li><strong>缓存</strong>：对于重复的请求，LangChain提供了缓存功能来避免重复调用LLM，从而节省时间和成本。</li>
<li><strong>流式处理</strong>：对于需要长时间生成大量内容的请求，LangChain支持流式处理，可以逐步返回生成的内容，提高用户体验。</li>
<li><strong>自定义LLM</strong>：如果LangChain没有直接支持你想要使用的LLM，你可以通过继承LangChain的<code>LLM</code>基类来自定义LLM接口。</li>
</ul>
<p>### 2.4.19 LangChain 如何修改提示模板?</p>
<p>根据需求，可以通过以下几种方式修改提示模板：</p>
<ol>
<li>
<p><strong>直接修改模板字符串</strong>：
   如果提示模板是以字符串形式直接定义的，可以直接在代码中修改这个字符串。例如，将模板中的某个词替换为另一个词，或者调整句子的结构。</p>
</li>
<li>
<p><strong>使用PromptTemplate类</strong>：
   在LangChain中，通常会使用<code>PromptTemplate</code>类来创建和管理提示模板。可以通过修改<code>PromptTemplate</code>实例的<code>template</code>属性来更新模板内容。例如：
   <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># 假设已有一个PromptTemplate实例</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;product&quot;</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="s2">&quot;What is a good name for a company that makes </span><span class="si">{product}</span><span class="s2">?&quot;</span><span class="p">)</span>

<span class="c1"># 修改模板内容</span>
<span class="n">template</span><span class="o">.</span><span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;Please suggest a name for a company specializing in </span><span class="si">{product}</span><span class="s2"> production.&quot;</span>

<span class="c1"># 使用修改后的模板</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">product</span><span class="o">=</span><span class="s2">&quot;eco-friendly water bottles&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></p>
</li>
</ol>
<p>注意：直接修改<code>template</code>属性可能不是所有情况下都推荐的做法，因为这可能涉及到对模板状态的直接操作，有时可能会导致不可预见的问题。更常见的做法是先创建一个新的<code>PromptTemplate</code>实例，然后替换旧的实例。</p>
<ol>
<li>
<p><strong>通过方法修改</strong>：
   如果<code>PromptTemplate</code>类提供了修改模板内容的方法（虽然标准的<code>PromptTemplate</code>类可能不直接提供这样的方法），那么可以通过调用这些方法来修改模板。但在大多数情况下，直接修改字符串或创建新实例更为直接和简单。</p>
</li>
<li>
<p><strong>使用版本控制</strong>：
   在修改提示模板时，建议使用版本控制工具（如Git）来跟踪和管理模板的变更历史。这有助于在需要时回滚到之前的版本，并确保团队成员之间的同步。</p>
</li>
</ol>
<p>### 2.4.20 LangChain 如何链接多个组件处理一个特定的下游任务?</p>
<p>LangChain 是一个灵活的框架，允许开发者通过链接多个组件来处理特定的下游任务。这种链式调用方式可以充分利用每个组件的能力，形成一个强大的工作流。以下是使用LangChain链接组件处理下游任务的一般步骤：</p>
<ol>
<li>
<p><strong>定义任务需求</strong>：明确你想要完成的下游任务是什么，以及需要哪些组件来实现它。</p>
</li>
<li>
<p><strong>选择组件</strong>：根据任务需求，选择适当的LangChain组件。这可能包括数据检索器、示例选择器、提示模板、语言模型、输出解析器等。</p>
</li>
<li>
<p><strong>配置组件</strong>：为每个组件配置必要的参数和设置。这可能包括API密钥、模型选择、示例数据、模板内容等。</p>
</li>
<li>
<p><strong>设计工作流</strong>：设计一个工作流，明确组件之间的数据流向和交互方式。决定哪些组件是顺序执行的，哪些可以并行处理。</p>
</li>
<li>
<p><strong>实现数据传递</strong>：实现组件之间的数据传递机制。这可能涉及到定义输入输出接口、使用共享数据结构等。</p>
</li>
<li>
<p><strong>编写代码</strong>：根据设计的工作流，编写代码来链接各个组件。使用LangChain提供的API和接口来实现组件的调用和数据传递。</p>
</li>
<li>
<p><strong>测试和调试</strong>：在开发过程中，不断测试每个组件和整个工作流，确保它们按预期工作，并调试任何出现的问题。</p>
</li>
<li>
<p><strong>优化和调整</strong>：根据测试结果，对工作流和组件进行优化和调整，以提高性能和准确性。</p>
</li>
<li>
<p><strong>部署和监控</strong>：将完成的工作流部署到生产环境，并持续监控其性能和稳定性。</p>
</li>
</ol>
<p>以下是一个简化的代码示例，展示了如何在LangChain中链接多个组件来处理一个简单的问答任务：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.agents</span> <span class="kn">import</span> <span class="n">Agent</span>
<span class="kn">from</span> <span class="nn">langchain.retrievers</span> <span class="kn">import</span> <span class="n">MyRetriever</span>
<span class="kn">from</span> <span class="nn">langchain.example_selectors</span> <span class="kn">import</span> <span class="n">MyExampleSelector</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">MyPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">MyLanguageModel</span>
<span class="kn">from</span> <span class="nn">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">MyOutputParser</span>

<span class="c1"># 实例化组件</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">MyRetriever</span><span class="p">()</span>
<span class="n">example_selector</span> <span class="o">=</span> <span class="n">MyExampleSelector</span><span class="p">()</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">MyPromptTemplate</span><span class="p">()</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">MyLanguageModel</span><span class="p">()</span>
<span class="n">output_parser</span> <span class="o">=</span> <span class="n">MyOutputParser</span><span class="p">()</span>

<span class="c1"># 定义一个Agent来协调各个组件</span>
<span class="k">class</span> <span class="nc">MyAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
        <span class="c1"># 使用检索器获取相关信息</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

        <span class="c1"># 使用示例选择器选择相关示例</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">example_selector</span><span class="o">.</span><span class="n">select_examples</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

        <span class="c1"># 使用提示模板生成提示</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>

        <span class="c1"># 使用语言模型生成回复</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

        <span class="c1"># 使用输出解析器解析回复</span>
        <span class="n">parsed_response</span> <span class="o">=</span> <span class="n">output_parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">parsed_response</span>

<span class="c1"># 创建Agent实例并执行任务</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">MyAgent</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;How does the internal combustion engine work?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div>
<p>### 2.4.21 LangChain 如何Embedding &amp; vector store?</p>
<p>LangChain 提供了强大的文本嵌入（Embedding）和向量存储（Vector Store）功能，以支持复杂的自然语言处理（NLP）任务。以下是 LangChain 如何进行文本嵌入和向量存储的详细步骤：</p>
<p>一、 文本嵌入（Embedding）</p>
<ol>
<li>
<p><strong>导入嵌入模型</strong>：
   LangChain 支持多种文本嵌入模型，如 OpenAI Embeddings、Hugging Face Embeddings 等。首先需要导入所需的嵌入模型。例如，使用 OpenAI Embeddings 的代码如下：
   <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
</code></pre></div></p>
</li>
<li>
<p><strong>创建嵌入向量</strong>：
   使用嵌入模型的 <code>embed_query</code> 或 <code>embed_documents</code> 方法为文本创建嵌入向量。这两个方法分别用于嵌入单个查询文本和多个文档文本。例如：
   <div class="highlight"><pre><span></span><code><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;This is a sample text.&quot;</span>
<span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hi there!&quot;</span><span class="p">,</span> <span class="s2">&quot;Oh, hello!&quot;</span><span class="p">,</span> <span class="s2">&quot;What&#39;s your name?&quot;</span><span class="p">]</span>
<span class="n">document_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></p>
</li>
<li>
<p><strong>缓存嵌入</strong>：
   为了提高效率，可以使用 <code>CacheBackedEmbeddings</code> 类来缓存嵌入结果，避免重复计算。例如：
   <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">CacheBackedEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.storage</span> <span class="kn">import</span> <span class="n">LocalFileStore</span>

<span class="n">underlying_embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
<span class="n">store</span> <span class="o">=</span> <span class="n">LocalFileStore</span><span class="p">(</span><span class="s2">&quot;./cache/&quot;</span><span class="p">)</span>
<span class="n">cached_embedder</span> <span class="o">=</span> <span class="n">CacheBackedEmbeddings</span><span class="o">.</span><span class="n">from_bytes_store</span><span class="p">(</span><span class="n">underlying_embeddings</span><span class="p">,</span> <span class="n">store</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">underlying_embeddings</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></p>
</li>
</ol>
<p>二、 向量存储（Vector Store）</p>
<ol>
<li>
<p><strong>选择合适的向量存储</strong>：
   LangChain 支持多种向量存储后端，如 Chroma、FAISS、Qdrant、Weaviate 等。需要根据实际需求选择合适的存储后端。</p>
</li>
<li>
<p><strong>创建向量存储实例</strong>：
   使用选定的向量存储和嵌入模型创建向量存储实例。例如，使用 Chroma 存储嵌入向量的代码如下：
   <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain.docstore.document</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre></div></p>
</li>
</ol>
<p>或者使用 FAISS：
   <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">cached_embedder</span><span class="p">)</span>
</code></pre></div></p>
<ol>
<li><strong>进行相似性搜索</strong>：
   使用向量存储的 <code>similarity_search</code> 方法进行相似性搜索。输入一个查询文本（也转换为嵌入向量），返回与该查询最相似的文档列表。例如：
   <div class="highlight"><pre><span></span><code><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What is this text about?&quot;</span>
<span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">)</span>
</code></pre></div></li>
</ol>
<p>注意：在上面的 FAISS 示例中，由于 FAISS 通常接受文档嵌入而不是查询嵌入作为输入来构建索引，因此在实际应用中，你可能需要先将查询嵌入与文档嵌入存储在同一个向量空间中，或者使用其他方法来处理查询嵌入。</p>
<p>### 2.4.22 LangChain 低效的令牌使用问题</p>
<p>在LangChain或任何使用语言模型（LLMs）的系统中，"低效的令牌使用"可能指的是几种不同的问题，其中最常见的是：</p>
<ol>
<li>
<p><strong>令牌限制导致的截断</strong>：大多数语言模型对输入和输出的令牌数量有限制。如果输入文本超过了这个限制，它将被截断，可能导致模型无法理解完整的上下文。</p>
</li>
<li>
<p><strong>冗余令牌</strong>：在提示（prompts）或输入中使用不必要的令牌，这不仅浪费令牌配额，而且可能使模型难以捕捉到关键信息。</p>
</li>
<li>
<p><strong>无效的令牌分配</strong>：在多轮对话或复杂查询中，令牌可能没有被有效分配，导致某些重要信息被忽略或无法充分表达。</p>
</li>
</ol>
<p>为了解决或优化LangChain中的令牌使用问题，可以考虑以下策略：</p>
<ul>
<li>
<p><strong>精简输入</strong>：确保输入文本尽可能简洁，去除无关紧要的词汇或冗余信息。</p>
</li>
<li>
<p><strong>优化提示设计</strong>：设计清晰、精确的提示，避免不必要的复杂性，确保每个令牌都用于传达重要信息。</p>
</li>
<li>
<p><strong>使用令牌高效的格式</strong>：在多轮对话中，使用列表或表格等格式来组织信息，可以更有效地利用令牌。</p>
</li>
<li>
<p><strong>分步处理</strong>：对于长对话或复杂任务，考虑将其分解为多个步骤，每步使用较少的令牌。</p>
</li>
<li>
<p><strong>动态令牌管理</strong>：根据对话的进展动态调整输入和输出的令牌使用，例如，在确定用户需求后，可以减少冗余的解释和背景信息。</p>
</li>
<li>
<p><strong>利用上下文管理</strong>：在多轮对话中，有效地利用模型的上下文记忆能力，避免重复之前已经提供的信息。</p>
</li>
<li>
<p><strong>测试和评估</strong>：通过测试不同的输入长度和复杂度，评估模型的响应质量和效率，找到最佳的令牌使用平衡点。</p>
</li>
<li>
<p><strong>使用高效的模型</strong>：如果可能，选择更高效的模型，这些模型能够在较少的令牌限制下提供更好的性能。</p>
</li>
<li>
<p><strong>用户反馈循环</strong>：允许用户提供反馈，根据反馈调整令牌的使用策略，以更好地满足用户需求。</p>
</li>
</ul>
<p>请注意，LangChain作为一个框架，提供了工具和接口来帮助开发者优化令牌的使用，但具体的优化策略可能需要根据应用场景和用户需求来定制。</p>
<p>### 2.4.23 LangChain 太多概念容易混淆，过多的“辅助”函数问题</p>
<p>LangChain 作为一个复杂的自然语言处理（NLP）框架，确实包含了大量的概念和“辅助”函数，这些设计初衷是为了提供灵活性和强大的功能，但同时也可能让新用户感到困惑和难以上手。以下是一些解决 LangChain 概念混淆和过多“辅助”函数问题的方法：</p>
<ol>
<li>
<p>清晰的概念划分和文档</p>
</li>
<li>
<p><strong>概念划分</strong>：在文档中明确区分和解释各个概念，包括它们的作用、区别以及相互之间的关系。使用图表、流程图或示例代码来辅助说明。</p>
</li>
<li>
<p><strong>术语标准化</strong>：确保文档中使用的术语是一致的，并对关键术语进行定义和解释。</p>
</li>
<li>
<p>分层次的文档结构</p>
</li>
<li>
<p><strong>入门指南</strong>：提供一个简洁的入门指南，帮助新用户快速了解 LangChain 的基本概念和主要功能。</p>
</li>
<li><strong>高级教程</strong>：对于已经有一定基础的用户，提供详细的高级教程，介绍更复杂的用法和最佳实践。</li>
<li>
<p><strong>参考手册</strong>：包含所有函数、类和方法的详细文档，方便用户查找特定信息。</p>
</li>
<li>
<p>示例和模板</p>
</li>
<li>
<p><strong>示例代码</strong>：提供丰富的示例代码，涵盖不同的使用场景和任务。示例代码应该具有代表性和可复制性，以便用户能够轻松地学习和实践。</p>
</li>
<li>
<p><strong>模板项目</strong>：创建一些模板项目，这些项目展示了如何使用 LangChain 构建不同类型的 NLP 应用。用户可以根据模板项目快速开始自己的项目。</p>
</li>
<li>
<p>社区和支持</p>
</li>
<li>
<p><strong>活跃的社区</strong>：建立一个活跃的社区，让用户能够交流经验、分享知识和解答问题。社区中的专家和用户可以提供宝贵的建议和指导。</p>
</li>
<li>
<p><strong>官方支持</strong>：提供官方支持渠道，如论坛、邮件列表或聊天室，以便用户能够直接获得来自 LangChain 团队或其他专家的帮助。</p>
</li>
<li>
<p>精简和整合“辅助”函数</p>
</li>
<li>
<p><strong>评估需求</strong>：定期评估“辅助”函数的使用情况和反馈，了解哪些函数是真正有用的，哪些可能是多余的。</p>
</li>
<li><strong>整合功能</strong>：对于功能相似或重复的“辅助”函数，考虑进行整合和简化，以减少用户的认知负担。</li>
<li>
<p><strong>提供默认配置</strong>：为常用的“辅助”函数提供合理的默认配置，以减少用户需要设置的参数数量。</p>
</li>
<li>
<p>教育和培训</p>
</li>
<li>
<p><strong>在线课程</strong>：提供免费的在线课程或教程，帮助用户系统地学习 LangChain 的概念和功能。</p>
</li>
<li><strong>工作坊和研讨会</strong>：组织定期的工作坊和研讨会，邀请专家和用户一起探讨 LangChain 的最佳实践和应用场景。</li>
</ol>
<p>通过这些方法，可以逐步解决 LangChain 概念混淆和过多“辅助”函数的问题，提高用户的体验和满意度。同时，这也需要 LangChain 团队和社区的持续努力和支持。</p>
<p>### 2.4.24 LangChain 行为不一致并且隐藏细节问题</p>
<p>在使用LangChain或其他复杂的框架时，可能会遇到行为不一致和隐藏细节的问题。这些问题可能源于多种原因，例如框架的内部逻辑复杂、文档不完整或更新不及时、API的变更等。以下是一些解决和缓解这些问题的策略：</p>
<ol>
<li>
<p><strong>详细阅读文档</strong>：确保你阅读了最新版本的官方文档，以了解所有功能和行为的预期。</p>
</li>
<li>
<p><strong>查看变更日志</strong>：检查框架的变更日志，了解最近的更新和可能影响现有代码的变更。</p>
</li>
<li>
<p><strong>理解内部机制</strong>：尽可能深入理解LangChain的内部工作机制，这有助于预测和解释行为。</p>
</li>
<li>
<p><strong>使用版本控制</strong>：使用版本控制系统（如Git）来跟踪你的代码更改，确保你可以回滚到之前的稳定版本。</p>
</li>
<li>
<p><strong>编写单元测试</strong>：为你的代码编写单元测试，这可以帮助捕捉不一致的行为，并确保代码按预期工作。</p>
</li>
<li>
<p><strong>错误处理</strong>：确保你的代码有健壮的错误处理和日志记录，以便在出现问题时快速定位。</p>
</li>
<li>
<p><strong>社区和论坛</strong>：参与LangChain的社区和论坛，与其他用户交流经验，了解他们如何处理类似的问题。</p>
</li>
</ol>
<p>### 2.4.25 LangChain 缺乏标准的可互操作数据类型问题</p>
<p>在构建像LangChain这样的框架时，确保数据类型的标准化和可互操作性是非常重要的。这有助于简化开发流程、提高代码的可维护性，并允许不同组件或模块之间无缝协作。以下是一些解决或缓解LangChain中缺乏标准可互操作数据类型问题的方法：</p>
<ol>
<li>
<p><strong>定义清晰的数据模型</strong>：为LangChain中使用的数据定义清晰的数据模型和接口。这包括输入、输出以及中间数据的格式。</p>
</li>
<li>
<p><strong>使用标准化的数据格式</strong>：尽可能使用广泛接受的标准化数据格式，如JSON、XML或Protocol Buffers，这些格式具有良好的互操作性。</p>
</li>
<li>
<p><strong>创建抽象层</strong>：在LangChain中实现一个抽象层，以统一不同组件之间的数据交换格式。</p>
</li>
<li>
<p><strong>数据序列化和反序列化</strong>：确保LangChain提供数据序列化和反序列化的工具，以支持不同数据格式之间的转换。</p>
</li>
<li>
<p><strong>API设计</strong>：设计一致的API，这些API可以接收和返回标准化的数据类型。</p>
</li>
<li>
<p><strong>文档和类型定义</strong>：在文档中清晰地定义数据类型和结构，使用类型定义语言（如YAML或OpenAPI）来描述API的输入和输出。</p>
</li>
<li>
<p><strong>类型检查和验证</strong>：在LangChain中实现类型检查和验证机制，确保数据在处理过程中符合预期的类型。</p>
</li>
<li>
<p><strong>使用类型系统</strong>：利用编程语言的类型系统来定义和检查数据类型，减少类型错误和不一致性。</p>
</li>
<li>
<p><strong>提供数据转换工具</strong>：提供工具或函数来帮助开发者在不同数据类型之间进行转换。</p>
</li>
</ol>
<h2 id="25-llm">2.5 基于LLM+向量库的文档对话 面<a class="headerlink" href="#25-llm" title="Permanent link">&para;</a></h2>
<p>### 2.5.1 LLMs 存在模型幻觉问题，请问如何处理?</p>
<p>LLMs（大型语言模型）存在的模型幻觉问题是一个复杂且需要多方面解决的难题。模型幻觉指的是LLMs生成看似合理但实际上不正确、与输入不符、与先前生成的内容矛盾或与已知世界知识不符的内容。为了处理这一问题，可以采取以下策略：</p>
<ol>
<li>
<p>数据清洗和预处理</p>
</li>
<li>
<p><strong>重要性</strong>：在训练LLMs之前，对数据进行仔细的清洗和预处理是至关重要的。</p>
</li>
<li>
<p><strong>措施</strong>：删除不准确、噪声或有偏差的数据，以减少模型幻觉问题的出现。这有助于确保模型在训练过程中接触到的是高质量、一致性的数据。</p>
</li>
<li>
<p>多样化训练数据</p>
</li>
<li>
<p><strong>目的</strong>：为了减少LLMs对特定数据源的依赖和偏好。</p>
</li>
<li>
<p><strong>实施</strong>：尽量使用多样化的训练数据，包括来自不同领域、不同来源和不同观点的数据。这有助于模型获得更全面的语言理解和更广泛的知识基础。</p>
</li>
<li>
<p>引入多样性的生成策略</p>
</li>
<li>
<p><strong>方法</strong>：在生成文本时，可以采用多样性的生成策略来减少模型的倾向性和幻觉问题。</p>
</li>
<li>
<p><strong>示例</strong>：使用温度参数来调整生成的多样性，或者使用抽样和束搜索等不同的生成方法。这些策略可以帮助模型在生成文本时避免陷入固定的模式或偏见。</p>
</li>
<li>
<p>人工审核和后处理</p>
</li>
<li>
<p><strong>作用</strong>：对生成的文本进行人工审核和后处理是一种常用的方法。</p>
</li>
<li>
<p><strong>实施</strong>：通过人工的干预和修正，可以纠正模型幻觉问题，并确保生成的内容准确和可靠。虽然这种方法需要人力投入，但它是目前最有效的保障内容质量的手段之一。</p>
</li>
<li>
<p>引入外部知识和约束</p>
</li>
<li>
<p><strong>方法</strong>：为了提高生成文本的准确性，可以引入外部知识和约束。</p>
</li>
<li>
<p><strong>示例</strong>：结合知识图谱、实体识别或逻辑推理等技术，将先验知识和约束融入到生成过程中。这些方法可以帮助模型在生成文本时参考更广泛、更准确的知识来源，从而减少幻觉问题的发生。</p>
</li>
<li>
<p>改进模型架构和训练算法</p>
</li>
<li>
<p><strong>方向</strong>：研究并改进LLMs的架构和训练算法，以减少幻觉问题的根源。</p>
</li>
<li>
<p><strong>措施</strong>：例如，通过优化模型的注意力机制、引入记忆机制或改进损失函数等方式，提高模型对上下文和全局信息的理解能力，从而减少幻觉的生成。</p>
</li>
<li>
<p>增强模型的上下文理解能力</p>
</li>
<li>
<p><strong>重要性</strong>：LLMs在生成文本时往往依赖于上下文信息。因此，增强模型的上下文理解能力是减少幻觉问题的关键。</p>
</li>
<li>
<p><strong>措施</strong>：可以通过增加模型的层数、改进编码方式或引入额外的上下文表示等方法，提高模型对上下文信息的捕捉和处理能力。</p>
</li>
<li>
<p>持续监测和评估</p>
</li>
<li>
<p><strong>必要性</strong>：由于LLMs的幻觉问题可能随着时间和数据的变化而变化，因此持续监测和评估模型的表现是必要的。</p>
</li>
<li><strong>实施</strong>：建立有效的评估机制，定期测试模型在不同任务和数据集上的表现，并根据评估结果及时调整和优化模型。</li>
</ol>
<p>综上所述，处理LLMs的模型幻觉问题需要综合考虑数据、模型、算法和人工干预等多个方面。通过实施上述策略，可以逐步减少模型幻觉问题的发生，提高LLMs的准确性和可靠性。</p>
<p>### 2.5.2 基于LLM+向量库的文档对话思路是怎么样?</p>
<p>基于LLM（大型语言模型）+向量库的文档对话思路，主要围绕将外部知识库与LLM结合，以提高文档对话的准确性和效率。以下是详细的思路步骤：</p>
<p>一、总体思路</p>
<p>通过将用户知识库内容经过embedding存入向量知识库，然后用户每一次提问也会经过embedding，利用向量相关性算法（如余弦算法）找到最匹配的几个知识库片段。将这些知识库片段作为上下文，与用户问题一起作为prompt提交给LLM，以生成最终的回答。</p>
<p>二、具体步骤</p>
<ol>
<li><strong>加载文件</strong>：</li>
<li>
<p>使用适当的库（如PyMuPDF、docx等）将包含要对话的文档文件（如PDF、Word、TXT等）加载到系统中。</p>
</li>
<li>
<p><strong>文本预处理</strong>：</p>
</li>
<li>
<p>将加载的文件内容读取成纯文本格式，并进行必要的预处理，如去除多余的空格和特殊字符。</p>
</li>
<li>
<p><strong>文本分割</strong>：</p>
</li>
<li>
<p>为了便于向量化处理和提高检索效率，将长文本分割成较小的段落或句子。可以使用自然语言处理技术进行段落或句子的分割，常用的方法包括按段落、句子或固定长度进行分割。</p>
</li>
<li>
<p><strong>文本向量化</strong>：</p>
</li>
<li>
<p>利用预训练的语言模型（如BERT、GPT等）将分割后的文本转换为向量表示。这些向量捕捉了文本的语义信息，便于后续的相似度计算和检索。常用的工具和库包括Transformers、Sentence-Transformers等。</p>
</li>
<li>
<p><strong>问题向量化</strong>：</p>
</li>
<li>
<p>将用户输入的问题也转换为向量表示，以便与文档向量进行匹配。</p>
</li>
<li>
<p><strong>向量检索</strong>：</p>
</li>
<li>
<p>利用相似性度量（如余弦相似度）在文本向量库中找到与问句向量最相似的Top K个向量。这一步可以使用快速检索算法（如FAISS）来提高匹配效率。</p>
</li>
<li>
<p><strong>构造Prompt</strong>：</p>
</li>
<li>
<p>将找到的最相似的文本段落作为上下文信息，与用户的问题一起形成新的Prompt。这个Prompt将作为LLM的输入，以生成最终的回答。</p>
</li>
<li>
<p><strong>生成回答</strong>：</p>
</li>
<li>将构造好的Prompt提交给预训练的大语言模型（LLM），如GPT-3、ChatGPT等，生成最终的回答。</li>
</ol>
<p>三、优化与改进</p>
<ol>
<li><strong>文档切分粒度</strong>：</li>
<li>
<p>文档切分粒度是一个需要仔细把控的问题。切分过细可能导致噪声过多，切分过粗则可能丢失语义信息。因此，需要根据实际情况选择合适的切分粒度，并可能采用语义级别的分割方法，如利用NLP的篇章分析工具或BERT等模型进行语义分割。</p>
</li>
<li>
<p><strong>向量检索效率</strong>：</p>
</li>
<li>
<p>提高向量检索的效率是优化文档对话系统性能的关键。可以使用高效的向量检索工具，如FAISS、Annoy等，来加速相似度计算和向量匹配过程。</p>
</li>
<li>
<p><strong>模型选择与训练</strong>：</p>
</li>
<li>
<p>选择合适的LLM模型对于生成高质量的回答至关重要。同时，根据具体任务的需求，可以对LLM进行微调或训练，以更好地适应特定领域或场景。</p>
</li>
<li>
<p><strong>多模态融合</strong>：</p>
</li>
<li>在某些情况下，文档可能包含多种类型的信息（如文本、图片、表格等）。为了充分利用这些信息，可以考虑将多模态融合技术引入文档对话系统中，以提高系统的综合性能。</li>
</ol>
<p>通过以上步骤和优化措施，可以构建一个基于LLM+向量库的文档对话系统，该系统能够高效地处理用户提问，并生成准确、有用的回答。</p>
<p>### 2.5.3 基于LLM+向量库的文档对话核心技术是什么?</p>
<p>基于LLM（大型语言模型）+向量库的文档对话核心技术主要涉及到**文本嵌入（Text Embedding）**和**高效检索（Efficient Retrieval）**两个方面。这两个方面共同构成了支撑文档对话系统的关键技术。</p>
<ol>
<li>文本嵌入（Text Embedding）</li>
</ol>
<p>文本嵌入技术是将文本数据转换为固定长度的向量表示，这些向量能够捕捉文本的语义信息，使得在向量空间中相似的文本具有相近的距离。在文档对话系统中，文本嵌入技术被用于将用户提问和文档内容转换为向量，以便进行后续的相似度计算和检索。</p>
<p>常用的文本嵌入方法包括：</p>
<ul>
<li><strong>预训练语言模型</strong>：如BERT、GPT等，这些模型在大量文本数据上进行预训练，能够学习到文本的丰富语义特征，并将这些特征编码到向量中。</li>
<li>
<p><strong>句子嵌入模型</strong>：如Sentence-BERT（SBERT）、Universal Sentence Encoder（USE）等，这些模型专门设计用于生成句子的向量表示，能够在保持较高精度的同时提高计算效率。</p>
</li>
<li>
<p>高效检索（Efficient Retrieval）</p>
</li>
</ul>
<p>在文档对话系统中，面对庞大的文档库，如何实现快速、准确的检索是一个关键问题。高效检索技术能够帮助系统从大量文档中快速找到与用户提问最相关的文档片段，作为后续LLM生成回答的上下文。</p>
<p>常用的高效检索方法包括：</p>
<ul>
<li><strong>向量检索工具</strong>：如FAISS、Annoy等，这些工具利用近似最近邻搜索（Approximate Nearest Neighbor, ANN）算法，在向量空间中快速找到与用户提问向量最相似的文档向量。</li>
<li>
<p><strong>倒排索引</strong>：对于文本数据，还可以使用倒排索引技术来加速检索过程。倒排索引将文档中的词汇映射到包含该词汇的文档列表，通过查询词汇可以快速定位到相关文档。</p>
</li>
<li>
<p>核心技术综合应用</p>
</li>
</ul>
<p>在文档对话系统中，文本嵌入和高效检索技术通常被综合应用：</p>
<ol>
<li><strong>文档向量化</strong>：首先，将文档库中的文档内容进行向量化处理，生成文档向量库。</li>
<li><strong>问题向量化</strong>：当用户提问时，将问题也进行向量化处理，生成问题向量。</li>
<li><strong>向量检索</strong>：在文档向量库中使用高效检索技术找到与问题向量最相似的Top K个文档向量。</li>
<li><strong>Prompt构造</strong>：将检索到的文档片段作为上下文信息，与用户提问一起构造新的Prompt。</li>
<li><strong>回答生成</strong>：将Prompt输入到LLM中，生成最终的回答。</li>
</ol>
<p>通过这种方式，文档对话系统能够充分利用LLM的生成能力和向量库的检索能力，实现高效、准确的文档对话。</p>
<p>### 2.5.4 基于LLM+向量库的文档对话 prompt 模板如何构建?</p>
<p>基于LLM（大型语言模型）+向量库的文档对话prompt模板的构建，是一个结合文本嵌入、高效检索和LLM生成能力的复杂过程。以下是一个构建此类prompt模板的基本框架和步骤：</p>
<p>一、构建思路</p>
<ol>
<li><strong>明确对话目的</strong>：</li>
<li>
<p>确定文档对话的具体目标，如回答问题、提供信息、执行指令等。</p>
</li>
<li>
<p><strong>分析用户提问</strong>：</p>
</li>
<li>
<p>解析用户提问的意图和关键词，理解用户需要的信息类型。</p>
</li>
<li>
<p><strong>检索相关文档</strong>：</p>
</li>
<li>
<p>利用向量库和高效检索技术，找到与用户提问最相关的文档片段。</p>
</li>
<li>
<p><strong>构造上下文Prompt</strong>：</p>
</li>
<li>
<p>将检索到的文档片段作为上下文信息，与用户提问一起构造出完整的Prompt。</p>
</li>
<li>
<p><strong>提交给LLM生成回答</strong>：</p>
</li>
<li>将构造好的Prompt提交给LLM，利用LLM的生成能力生成最终的回答。</li>
</ol>
<p>二、Prompt模板示例</p>
<p>假设用户提问：“请告诉我关于人工智能在医疗领域的应用有哪些？”</p>
<ol>
<li>
<p>检索相关文档</p>
</li>
<li>
<p>使用向量库和高效检索技术，找到与“人工智能 医疗领域 应用”相关的文档片段。</p>
</li>
<li>
<p>构造上下文Prompt</p>
</li>
</ol>
<div class="highlight"><pre><span></span><code>问题：请告诉我关于人工智能在医疗领域的应用有哪些？

上下文信息：
（以下是从向量库中检索到的相关文档片段，作为上下文信息）

* 人工智能在医疗领域的应用日益广泛，包括但不限于辅助诊断、个性化治疗方案制定、患者监测等。
* 通过深度学习算法，AI能够分析大量的医学影像数据，帮助医生更准确地诊断疾病。
* 在个性化治疗方面，AI可以根据患者的基因信息和病情，推荐最适合的治疗方案。
* 此外，AI还可以用于患者监测，通过实时监测患者的生理指标，及时发现异常情况并采取措施。

请基于以上上下文信息，回答用户的问题。
</code></pre></div>
<ol>
<li>
<p>提交给LLM生成回答</p>
</li>
<li>
<p>将上述Prompt提交给LLM，LLM将基于上下文信息和用户提问生成最终的回答。</p>
</li>
</ol>
<p>三、注意事项</p>
<ol>
<li><strong>保持Prompt的简洁性</strong>：</li>
<li>
<p>Prompt应尽可能简洁明了，避免冗长和无关的信息。</p>
</li>
<li>
<p><strong>确保上下文的准确性</strong>：</p>
</li>
<li>
<p>检索到的文档片段应准确反映用户提问的意图和关键词，避免引入噪声信息。</p>
</li>
<li>
<p><strong>考虑LLM的生成能力</strong>：</p>
</li>
<li>
<p>根据所使用的LLM的特点和生成能力，适当调整Prompt的构造方式，以获得更好的回答效果。</p>
</li>
<li>
<p><strong>持续优化Prompt模板</strong>：</p>
</li>
<li>根据实际应用效果和用户反馈，不断优化Prompt模板，提高文档对话的准确性和效率。</li>
</ol>
<p>通过以上步骤和注意事项，可以构建出基于LLM+向量库的文档对话prompt模板，为文档对话系统提供有力支持。</p>
<h2 id="26-peft">2.6 大模型 参数高效微调(PEFT) 面<a class="headerlink" href="#26-peft" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>LoRA篇</strong></li>
</ul>
<p>### 2.6.1 什么是 LoRA?</p>
<p>### 2.6.2 LoRA 的思路是什么?</p>
<p>### 2.6.3 LoRA 的特点是什么?</p>
<ul>
<li><strong>QLoRA篇</strong></li>
</ul>
<p>### 2.6.4 QLoRA 的思路是怎么样的?</p>
<p>### 2.6.5 QLoRA 的特点是什么?</p>
<ul>
<li><strong>AdaLoRA篇</strong></li>
</ul>
<h3 id="266-adalora">2.6.6 AdaLoRA 的思路是怎么样的?<a class="headerlink" href="#266-adalora" title="Permanent link">&para;</a></h3>
<h3 id="267-lora">2.6.7 LoRA权重是否可以合入原模型?<a class="headerlink" href="#267-lora" title="Permanent link">&para;</a></h3>
<h3 id="268-chatglm-6b-lora">2.6.8 ChatGLM-6B LoRA后的权重多大?<a class="headerlink" href="#268-chatglm-6b-lora" title="Permanent link">&para;</a></h3>
<h3 id="269-lora">2.6.9 LoRA 微调优点是什么?<a class="headerlink" href="#269-lora" title="Permanent link">&para;</a></h3>
<h3 id="2610-lora">2.6.10 LoRA微调方法为啥能加速训练?<a class="headerlink" href="#2610-lora" title="Permanent link">&para;</a></h3>
<h3 id="2611-lora">2.6.11 如何在已有LoRA模型上继续训练?<a class="headerlink" href="#2611-lora" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>提示学习（Prompting）</strong></li>
</ul>
<h3 id="2612-p-tuning">2.6.12 为什么需要 P-tuning?<a class="headerlink" href="#2612-p-tuning" title="Permanent link">&para;</a></h3>
<p>### 2.6.13 P-tuning 思路是什么?</p>
<h3 id="2614-p-tuning">2.6.14 P-tuning 优点是什么?<a class="headerlink" href="#2614-p-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2615-p-tuning">2.6.15 P-tuning 缺点是什么?<a class="headerlink" href="#2615-p-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2616-prompt-tuning">2.6.16 为什么需要 指示微调（Prompt-tuning）?<a class="headerlink" href="#2616-prompt-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2617-prompt-tuning">2.6.17 指示微调（Prompt-tuning）思路是什么?<a class="headerlink" href="#2617-prompt-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2618-prompt-tuning">2.6.18 指示微调（Prompt-tuning）优点是什么?<a class="headerlink" href="#2618-prompt-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2619-prompt-tuning">2.6.19 指示微调（Prompt-tuning）缺点是什么?<a class="headerlink" href="#2619-prompt-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2620-prompt-tuning-prefix-tuning">2.6.20 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么?<a class="headerlink" href="#2620-prompt-tuning-prefix-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2621-prompt-tuning-fine-tuning">2.6.21 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么?<a class="headerlink" href="#2621-prompt-tuning-fine-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2622-prompting">2.6.22 提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们?<a class="headerlink" href="#2622-prompting" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>前缀微调（Prefix-tuning）篇</strong></li>
</ul>
<h3 id="2623-prefix-tuning">2.6.23 为什么需要 前缀微调（Prefix-tuning）?<a class="headerlink" href="#2623-prefix-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2624-prefix-tuning">2.6.24 前缀微调（Prefix-tuning）思路是什么?<a class="headerlink" href="#2624-prefix-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2625-prefix-tuning">2.6.25 前缀微调（Prefix-tuning）的优点是什么?<a class="headerlink" href="#2625-prefix-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2626-prefix-tuning">2.6.26 前缀微调（Prefix-tuning）的缺点是什么?<a class="headerlink" href="#2626-prefix-tuning" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>适配器微调（Adapter-tuning）篇</strong></li>
</ul>
<h3 id="2627-adapter-tuning">2.6.27 为什么 需要 适配器微调（Adapter-tuning）?<a class="headerlink" href="#2627-adapter-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2628-adapter-tuning">2.6.28 适配器微调（Adapter-tuning）思路?<a class="headerlink" href="#2628-adapter-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2629-adapter-tuning">2.6.29 适配器微调（Adapter-tuning）特点是什么?<a class="headerlink" href="#2629-adapter-tuning" title="Permanent link">&para;</a></h3>
<h3 id="2630-adapterfusion">2.6.30 AdapterFusion 思路 是什么?<a class="headerlink" href="#2630-adapterfusion" title="Permanent link">&para;</a></h3>
<p>### 2.6.31 AdapterDrop 思路 是什么?</p>
<p>### 2.6.32 AdapterDrop 特点 是什么?</p>
<p>### 2.6.33 MAM Adapter 思路 是什么?</p>
<p>### 2.6.34 MAM Adapter 特点 是什么?</p>
<h2 id="27">2.7 大模型 推理面<a class="headerlink" href="#27" title="Permanent link">&para;</a></h2>
<p>### 2.7.1 为什么大模型推理时显存涨的那么多还一直占着?</p>
<p>### 2.7.2 大模型在gpu和cpu上推理速度如何?</p>
<h3 id="273-int8fp16">2.7.3 推理速度上，int8和fp16比起来怎么样?<a class="headerlink" href="#273-int8fp16" title="Permanent link">&para;</a></h3>
<h3 id="274">2.7.4 大模型有推理能力吗?<a class="headerlink" href="#274" title="Permanent link">&para;</a></h3>
<h3 id="275">2.7.5 大模型生成时的参数怎么设置?<a class="headerlink" href="#275" title="Permanent link">&para;</a></h3>
<p>### 2.7.6 有哪些省内存的大语言模型训练/微调/推理方法?</p>
<h3 id="277">2.7.7 如何让大模型输出合规化<a class="headerlink" href="#277" title="Permanent link">&para;</a></h3>
<h3 id="278">2.7.8 应用模式变更<a class="headerlink" href="#278" title="Permanent link">&para;</a></h3>
<h2 id="28">2.8 大模型 评测面<a class="headerlink" href="#28" title="Permanent link">&para;</a></h2>
<h3 id="281">2.8.1 大模型怎么评测?<a class="headerlink" href="#281" title="Permanent link">&para;</a></h3>
<h3 id="282-honest">2.8.2 大模型的honest原则是如何实现的?<a class="headerlink" href="#282-honest" title="Permanent link">&para;</a></h3>
<h3 id="283">2.8.3 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力?<a class="headerlink" href="#283" title="Permanent link">&para;</a></h3>
<h2 id="29">2.9 大模型 强化学习面<a class="headerlink" href="#29" title="Permanent link">&para;</a></h2>
<p>### 2.9.1 奖励模型需要和基础模型一致吗?</p>
<h3 id="292-rlhf">2.9.2 RLHF 在实践过程中存在哪些不足?<a class="headerlink" href="#292-rlhf" title="Permanent link">&para;</a></h3>
<h3 id="293">2.9.3 如何解决 人工产生的偏好数据集成本较高，很难量产问题?<a class="headerlink" href="#293" title="Permanent link">&para;</a></h3>
<h3 id="294-sft-rm-ppo">2.9.4 如何解决三个阶段的训练（SFT-&gt;RM-&gt;PPO）过程较长，更新迭代较慢问题?<a class="headerlink" href="#294-sft-rm-ppo" title="Permanent link">&para;</a></h3>
<p>### 2.9.5 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高问题?</p>
<h2 id="210">2.10 大模型 软硬件配置面<a class="headerlink" href="#210" title="Permanent link">&para;</a></h2>
<h3 id="2101-ffn">2.10.1 介绍一下 FFN 块 计算公式?<a class="headerlink" href="#2101-ffn" title="Permanent link">&para;</a></h3>
<h3 id="2102-gelu">2.10.2 介绍一下 GeLU 计算公式?<a class="headerlink" href="#2102-gelu" title="Permanent link">&para;</a></h3>
<h3 id="2103-swish">2.10.3 介绍一下 Swish 计算公式?<a class="headerlink" href="#2103-swish" title="Permanent link">&para;</a></h3>
<h3 id="2104-glu-ffn">2.10.4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式?<a class="headerlink" href="#2104-glu-ffn" title="Permanent link">&para;</a></h3>
<p>### 2.10.5 介绍一下 使用 GeLU 的 GLU 块 计算公式?</p>
<p>### 2.10.6 介绍一下 使用 Swish 的 GLU 块 计算公式?</p>
<h3 id="2107-llms">2.10.7 各LLMs 都使用哪种激活函数?<a class="headerlink" href="#2107-llms" title="Permanent link">&para;</a></h3>
<h2 id="211">2.11 大模型 训练集面<a class="headerlink" href="#211" title="Permanent link">&para;</a></h2>
<h3 id="2111-sft">2.11.1 SFT（有监督微调）的数据集格式?<a class="headerlink" href="#2111-sft" title="Permanent link">&para;</a></h3>
<h3 id="2112-rm">2.11.2 RM（奖励模型）的数据格式?<a class="headerlink" href="#2112-rm" title="Permanent link">&para;</a></h3>
<h3 id="2113-ppo">2.11.3 PPO（强化学习）的数据格式?<a class="headerlink" href="#2113-ppo" title="Permanent link">&para;</a></h3>
<p>### 2.11.4 找数据集哪里找?</p>
<p>### 2.11.4 微调需要多少条数据?</p>
<p>### 2.11.5 有哪些大模型的训练集?</p>
<p>### 2.11.6 进行领域大模型预训练应用哪些数据集比较好?</p>
<p>### 2.11.7 如何给LLM注入领域知识?</p>
<p>### 2.11.8 如果想要快速体验各种模型，该怎么办?</p>
<h2 id="212-token">2.12 Token及模型参数准备篇<a class="headerlink" href="#212-token" title="Permanent link">&para;</a></h2>
<p>### 2.12.1 预训练数据 Token 重复 是否影响 模型性能?</p>
<h3 id="2122-sfttoken">2.12.2 SFT需要训练Token数?<a class="headerlink" href="#2122-sfttoken" title="Permanent link">&para;</a></h3>
<h2 id="213-alibi-attention-with-linear-biases">2.13 ALiBi (Attention with Linear Biases)篇<a class="headerlink" href="#213-alibi-attention-with-linear-biases" title="Permanent link">&para;</a></h2>
<p>### 2.13.1 ALiBi (Attention with Linear Biases) 思路是什么?</p>
<p>### 2.13.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么?有什么作用?</p>
<p>### 2.13.3 ALiBi (Attention with Linear Biases) 有什么优点?</p>
<p>### 2.13.4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用?</p>
<h2 id="214-llms">2.14 LLMs 位置编码篇<a class="headerlink" href="#214-llms" title="Permanent link">&para;</a></h2>
<p>### 2.14.1 什么是位置编码?</p>
<p>### 2.14.1.1 什么是绝对位置编码?</p>
<p>### 2.14.1.2 什么是相对位置编码?</p>
<ul>
<li><strong>旋转位置编码 RoPE篇</strong></li>
</ul>
<p>### 2.14.2 旋转位置编码 RoPE 思路是什么?</p>
<p>### 2.14.3 推导一下 旋转位置编码 RoPE ?</p>
<p>### 2.14.4 旋转位置编码 RoPE 有什么优点?</p>
<p>### 2.14.5 旋转位置编码 RoPE 被哪些 LLMs 应用?</p>
<h2 id="215">2.15 长度外推问题篇<a class="headerlink" href="#215" title="Permanent link">&para;</a></h2>
<p>### 2.15.1 什么是 长度外推问题?</p>
<h3 id="2152">2.15.2 长度外推问题 的 解决方法 有哪些?<a class="headerlink" href="#2152" title="Permanent link">&para;</a></h3>
<h2 id="216-llms-tokenizer">2.16 LLMs Tokenizer 篇<a class="headerlink" href="#216-llms-tokenizer" title="Permanent link">&para;</a></h2>
<p>### 2.16.1 Byte-Pair Encoding(BPE)篇</p>
<p>#### 2.16.1.1 Byte-Pair Encoding(BPE) 如何构建词典?</p>
<p>### 2.16.2 WordPiece 篇</p>
<p>#### 2.16.2.1 WordPiece 与 BPE 异同点是什么?</p>
<p>### 2.16.3 SentencePiece 篇</p>
<p>#### 2.16.3.1 简单介绍一下 SentencePiece 思路?</p>
<ul>
<li><strong>对比篇</strong></li>
</ul>
<p>### 2.16.4 举例介绍一下 不同 大模型LLMs 的分词方式?</p>
<p>### 2.16.5 介绍一下 不同 大模型LLMs 的分词方式 的区别?</p>
<h2 id="217-layer-normalization">2.17 Layer Normalization 篇<a class="headerlink" href="#217-layer-normalization" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Layer Norm 篇</strong></li>
</ul>
<p>### 2.17.1 Layer Norm 的计算公式写一下?</p>
<ul>
<li><strong>RMS Norm 篇 （均方根 Norm）</strong></li>
</ul>
<p>### 2.17.2 RMS Norm 的计算公式写一下?</p>
<p>### 2.17.3 RMS Norm 相比于 Layer Norm 有什么特点?</p>
<ul>
<li><strong>Deep Norm 篇</strong></li>
</ul>
<p>### 2.17.4 Deep Norm 有什么优点?</p>
<p>### 2.17.5 Deep Norm 思路?</p>
<p>### 2.17.6 写一下 Deep Norm 代码实现?</p>
<ul>
<li>
<p><strong>Layer normalization-方法篇</strong></p>
</li>
<li>
<p><strong>Layer normalization-位置篇</strong></p>
</li>
</ul>
<p>### 2.17.7 LN 在 LLMs 中的不同位置 有什么区别么?如果有，能介绍一下区别么?</p>
<ul>
<li><strong>Layer normalization 对比篇</strong></li>
</ul>
<p>### 2.17.8 LLMs 各模型分别用了 哪种 Layer normalization?</p>
<p><a href="https://github.com/Joining-AI/LLM_Interview_Prepare"><img alt="GitHub stars" src="https://img.shields.io/github/stars/Joining-AI/LLM_Interview_Prepare?style=social" /></a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Joining-AI/LLM_Interview_Prepare" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.sections", "navigation.tabs"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="../javascripts/mathjax_config.js"></script>
      
    
  </body>
</html>