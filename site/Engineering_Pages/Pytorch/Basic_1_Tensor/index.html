
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../Full_SFT_LlamaFactory/">
      
      
        <link rel="next" href="../Basic_2_Autograd/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.40">
    
    
      
        <title>Pytorch张量 Tensor - AIDIY Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tensor" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="AIDIY Wiki" class="md-header__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AIDIY Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Pytorch张量 Tensor
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  首页 Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Mathematical_Foundation/Linear_Algebra/Matrix_Decomposition/Full_Rank_Decomposition/" class="md-tabs__link">
          
  
  数学基础 Mathematical Foundation

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../ML_Pages/CNN/" class="md-tabs__link">
          
  
  经典机器学习 Classic ML

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../LLM_Pages/LLM_Architecture/LLM_Architecture/" class="md-tabs__link">
          
  
  大语言模型结构 LLM Structure

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../SFT_Pages/SFT/" class="md-tabs__link">
          
  
  指令微调 SFT

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../LlamaFactory/" class="md-tabs__link">
          
  
  框架实践 Framework

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../RLHF_Pages/DPO/" class="md-tabs__link">
          
  
  人类反馈强化学习 RLHF

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="AIDIY Wiki" class="md-nav__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AIDIY Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    首页 Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    数学基础 Mathematical Foundation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            数学基础 Mathematical Foundation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Mathematical_Foundation/Linear_Algebra/Matrix_Decomposition/Full_Rank_Decomposition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    矩阵满秩分解 Full Rank Decomposition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Mathematical_Foundation/Linear_Algebra/Matrix_Calculation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    矩阵求导 Matrix Calculation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Mathematical_Foundation/Manifold/Manifold_hypothesis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    流形假设 Manifold Hypothesis
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    经典机器学习 Classic ML
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            经典机器学习 Classic ML
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/CNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    卷积神经网络 CNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/MLP/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    多层感知机 MLP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/RNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    循环神经网络 RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/VAE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    变分自编码器 VAE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/Pytorch_CNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CNN编程练习 CNN Practice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/Pytorch_MLP/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MLP编程练习 MLP Practice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/Pytorch_RNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RNN编程练习 RNN Practice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML_Pages/Pytorch_VAE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VAE编程练习 VAE Practice
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    大语言模型结构 LLM Structure
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            大语言模型结构 LLM Structure
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Transformer架构 Architecture
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Transformer架构 Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/LLM_Architecture/LLM_Architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer经典架构 Classical Architecture
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    归一化 Normalization
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            归一化 Normalization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/Normalization/LayerNorm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    层归一化 LayerNorm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/Normalization/BatchNorm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    批归一化 BatchNorm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/Normalization/RMSNorm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    均方根归一化 RMSNorm
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/Positional_Embedding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    位置编码 Positional Embedding
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/Pre-Train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    预训练阶段 Pre-Train
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/Post-Train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    后训练阶段 Post-Train
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM_Pages/MoE_Architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MoE数学架构 Mathematics
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    指令微调 SFT
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            指令微调 SFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../SFT_Pages/SFT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    监督微调 SFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../SFT_Pages/Lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    低秩适应 Lora
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    框架实践 Framework
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            框架实践 Framework
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LlamaFactory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LlamaFactory入门 Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Full_SFT_LlamaFactory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    全量微调 Full SFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Pytorch张量 Tensor
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Pytorch张量 Tensor
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tensor" class="md-nav__link">
    <span class="md-ellipsis">
      一、张量（Tensor）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、张量（Tensor）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 张量的概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 创建张量
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 创建张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 从数据创建张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 创建全零张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 创建全一张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 创建未初始化张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 创建服从标准正态分布的张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 创建范围内的一维张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 创建均匀间隔的张量
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 张量的属性
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 张量的属性">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-dtype" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 数据类型（dtype）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-shape" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 形状（shape）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-device" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 设备（device）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 索引、切片与拼接
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 索引、切片与拼接">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 索引操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 切片操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 拼接操作
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.3 拼接操作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchcattensors-dim" class="md-nav__link">
    <span class="md-ellipsis">
      torch.cat(tensors, dim)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchstacktensors-dim" class="md-nav__link">
    <span class="md-ellipsis">
      torch.stack(tensors, dim)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 张量变换
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 张量变换">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 重塑形状
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1 重塑形状">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorviewshape" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.view(shape)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorreshapeshape" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.reshape(shape)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 转置与交换维度
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 转置与交换维度">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensortransposedim0-dim1" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.transpose(dim0, dim1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorpermutedims" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.permute(*dims)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 增减维度
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3 增减维度">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorsqueeze" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.squeeze()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorunsqueezedim" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.unsqueeze(dim)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 数学运算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 数学运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 基本运算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.1 基本运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchaddx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.add(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchsubx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.sub(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmulx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.mul(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchdivx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.div(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmatmulx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.matmul(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchpowbase-exponent" class="md-nav__link">
    <span class="md-ellipsis">
      torch.pow(base, exponent)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchexptensor" class="md-nav__link">
    <span class="md-ellipsis">
      torch.exp(tensor)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchsqrttensor" class="md-nav__link">
    <span class="md-ellipsis">
      torch.sqrt(tensor)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 汇总统计
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.2 汇总统计">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchsuminput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.sum(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmeaninput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.mean(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmaxinput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.max(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmininput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.min(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchstdinput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.std(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchvarinput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.var(input)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 梯度相关操作
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. 梯度相关操作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 标记张量需要计算梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 获取张量的梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73" class="md-nav__link">
    <span class="md-ellipsis">
      7.3 计算梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 数据管理
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. 数据管理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 将张量移动到指定设备
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 保存与加载张量
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8.2 保存与加载张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchsaveobj-f" class="md-nav__link">
    <span class="md-ellipsis">
      torch.save(obj, f)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchloadf" class="md-nav__link">
    <span class="md-ellipsis">
      torch.load(f)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9" class="md-nav__link">
    <span class="md-ellipsis">
      9. 实用技巧
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. 实用技巧">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 类型转换
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 复制与克隆
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 注意事项
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Basic_2_Autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pytorch自动微分 Autograd
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../How_to_Load_Data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pytorch数据加载 Data Loading
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../How_to_Train_Model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pytorch模型训练 Model Training
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    人类反馈强化学习 RLHF
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            人类反馈强化学习 RLHF
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/DPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    直接偏好优化 DPO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/DPO_Exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO编程练习 DPO Practice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/DPO_Problem/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO问题分析 Problems
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/DPO_Math/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO数学原理 Mathematics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/PPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    近端策略优化 PPO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/PPO_Loss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PPO损失函数 Loss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/RM_Full_FT_or_Lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    奖励模型训练方式 RM Training
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/RM_Training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    奖励模型训练 RM Training
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/GRPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    群组相对策略优化 GRPO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/MCTS_DPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    蒙特卡洛树DPO MCTS-DPO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/KTO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KL约束优化 KTO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/SimPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    简单偏好优化 SimPO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/SimPO_Exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SimPO编程练习 SimPO Practice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/RM_Exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    奖励模型练习 RM Practice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/NPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    负偏好优化 NPO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../RLHF_Pages/FPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    特征偏好优化 FPO
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tensor" class="md-nav__link">
    <span class="md-ellipsis">
      一、张量（Tensor）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、张量（Tensor）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 张量的概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 创建张量
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 创建张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 从数据创建张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 创建全零张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 创建全一张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 创建未初始化张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 创建服从标准正态分布的张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 创建范围内的一维张量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 创建均匀间隔的张量
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 张量的属性
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 张量的属性">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-dtype" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 数据类型（dtype）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-shape" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 形状（shape）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-device" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 设备（device）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 索引、切片与拼接
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 索引、切片与拼接">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 索引操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 切片操作
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 拼接操作
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.3 拼接操作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchcattensors-dim" class="md-nav__link">
    <span class="md-ellipsis">
      torch.cat(tensors, dim)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchstacktensors-dim" class="md-nav__link">
    <span class="md-ellipsis">
      torch.stack(tensors, dim)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 张量变换
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 张量变换">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 重塑形状
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1 重塑形状">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorviewshape" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.view(shape)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorreshapeshape" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.reshape(shape)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 转置与交换维度
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 转置与交换维度">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensortransposedim0-dim1" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.transpose(dim0, dim1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorpermutedims" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.permute(*dims)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 增减维度
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3 增减维度">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorsqueeze" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.squeeze()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorunsqueezedim" class="md-nav__link">
    <span class="md-ellipsis">
      tensor.unsqueeze(dim)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 数学运算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 数学运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 基本运算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.1 基本运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchaddx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.add(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchsubx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.sub(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmulx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.mul(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchdivx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.div(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmatmulx-y" class="md-nav__link">
    <span class="md-ellipsis">
      torch.matmul(x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchpowbase-exponent" class="md-nav__link">
    <span class="md-ellipsis">
      torch.pow(base, exponent)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchexptensor" class="md-nav__link">
    <span class="md-ellipsis">
      torch.exp(tensor)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchsqrttensor" class="md-nav__link">
    <span class="md-ellipsis">
      torch.sqrt(tensor)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 汇总统计
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.2 汇总统计">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchsuminput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.sum(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmeaninput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.mean(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmaxinput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.max(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchmininput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.min(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchstdinput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.std(input)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchvarinput" class="md-nav__link">
    <span class="md-ellipsis">
      torch.var(input)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 梯度相关操作
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. 梯度相关操作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 标记张量需要计算梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 获取张量的梯度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73" class="md-nav__link">
    <span class="md-ellipsis">
      7.3 计算梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 数据管理
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. 数据管理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 将张量移动到指定设备
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 保存与加载张量
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8.2 保存与加载张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchsaveobj-f" class="md-nav__link">
    <span class="md-ellipsis">
      torch.save(obj, f)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchloadf" class="md-nav__link">
    <span class="md-ellipsis">
      torch.load(f)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9" class="md-nav__link">
    <span class="md-ellipsis">
      9. 实用技巧
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. 实用技巧">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 类型转换
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 复制与克隆
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 注意事项
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>Pytorch张量 Tensor</h1>

<p><a href="https://github.com/InuyashaYang/AIDIY"><img alt="GitHub stars" src="https://img.shields.io/github/stars/InuyashaYang/AIDIY?style=social" /></a></p>
<h2 id="tensor">一、张量（Tensor）<a class="headerlink" href="#tensor" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 张量的概念<a class="headerlink" href="#1" title="Permanent link">&para;</a></h3>
<p>张量（Tensor）是 PyTorch 的核心数据结构，类似于 NumPy 的 <code>ndarray</code>，但支持 GPU 加速。张量可以表示标量（0维）、向量（1维）、矩阵（2维）以及更高维度的数据。</p>
<h3 id="2">2. 创建张量<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<p>PyTorch 提供了多种方法来创建张量：</p>
<h4 id="21">2.1 从数据创建张量<a class="headerlink" href="#21" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.tensor(data)</code> 从列表、数组等数据创建一个新的张量。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">tensor_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor_data</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1, 2, 3, 4, 5])</span>
</code></pre></div>
<h4 id="22">2.2 创建全零张量<a class="headerlink" href="#22" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.zeros(size)</code> 创建一个指定大小的张量，所有元素的值为0。</p>
<div class="highlight"><pre><span></span><code><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[0., 0., 0.],</span>
<span class="c1">#         [0., 0., 0.]])</span>
</code></pre></div>
<h4 id="23">2.3 创建全一张量<a class="headerlink" href="#23" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.ones(size)</code> 创建一个指定大小的张量，所有元素的值为1。</p>
<div class="highlight"><pre><span></span><code><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[1., 1., 1.],</span>
<span class="c1">#         [1., 1., 1.]])</span>
</code></pre></div>
<h4 id="24">2.4 创建未初始化张量<a class="headerlink" href="#24" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.empty(size)</code> 创建一个指定大小的未初始化张量，其值取决于内存的状态。</p>
<div class="highlight"><pre><span></span><code><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">empty_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">empty_tensor</span><span class="p">)</span>
<span class="c1"># 输出示例:</span>
<span class="c1"># tensor([[1.4013e-45, 0.0000e+00, 0.0000e+00],</span>
<span class="c1">#         [0.0000e+00, 0.0000e+00, 0.0000e+00]])</span>
</code></pre></div>
<h4 id="25">2.5 创建服从标准正态分布的张量<a class="headerlink" href="#25" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.randn(size)</code> 创建一个指定大小的张量，元素值从标准正态分布中随机抽取。</p>
<div class="highlight"><pre><span></span><code><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">randn_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">randn_tensor</span><span class="p">)</span>
<span class="c1"># 输出示例:</span>
<span class="c1"># tensor([[ 0.4963, -0.1383,  0.6477],</span>
<span class="c1">#         [ 1.4393,  0.3337,  0.4621]])</span>
</code></pre></div>
<h4 id="26">2.6 创建范围内的一维张量<a class="headerlink" href="#26" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.arange(start, end, step)</code> 创建一个一维张量，元素值从起始值到结束值，步长为给定的步长。</p>
<div class="highlight"><pre><span></span><code><span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">end</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">arange_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">arange_tensor</span><span class="p">)</span>
<span class="c1"># 输出: tensor([0, 1, 2, 3, 4])</span>
</code></pre></div>
<h4 id="27">2.7 创建均匀间隔的张量<a class="headerlink" href="#27" title="Permanent link">&para;</a></h4>
<p>使用 <code>torch.linspace(start, end, steps)</code> 创建一个在指定范围内均匀间隔的一维张量。</p>
<div class="highlight"><pre><span></span><code><span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">end</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">linspace_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linspace_tensor</span><span class="p">)</span>
<span class="c1"># 输出: tensor([0.0000, 1.2500, 2.5000, 3.7500, 5.0000])</span>
</code></pre></div>
<h3 id="3">3. 张量的属性<a class="headerlink" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31-dtype">3.1 数据类型（dtype）<a class="headerlink" href="#31-dtype" title="Permanent link">&para;</a></h4>
<p>获取张量中元素的数据类型。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="c1"># 输出: torch.int64</span>
</code></pre></div>
<h4 id="32-shape">3.2 形状（shape）<a class="headerlink" href="#32-shape" title="Permanent link">&para;</a></h4>
<p>获取张量的形状，返回一个 <code>torch.Size</code> 对象。</p>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出: torch.Size([2, 3])</span>
</code></pre></div>
<h4 id="33-device">3.3 设备（device）<a class="headerlink" href="#33-device" title="Permanent link">&para;</a></h4>
<p>获取张量所在的设备，如 <code>cpu</code> 或 <code>cuda:0</code>。</p>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># 输出: cpu</span>
</code></pre></div>
<h3 id="4">4. 索引、切片与拼接<a class="headerlink" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 索引操作<a class="headerlink" href="#41" title="Permanent link">&para;</a></h4>
<p>使用索引访问张量中的元素。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">element</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 访问第0行第1列的元素</span>
<span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
<span class="c1"># 输出: tensor(2)</span>
</code></pre></div>
<h4 id="42">4.2 切片操作<a class="headerlink" href="#42" title="Permanent link">&para;</a></h4>
<p>使用切片获取张量的子张量。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">sub_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="c1"># 获取所有行，第1列及之后的所有列</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sub_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[2, 3],</span>
<span class="c1">#         [5, 6]])</span>
</code></pre></div>
<h4 id="43">4.3 拼接操作<a class="headerlink" href="#43" title="Permanent link">&para;</a></h4>
<h5 id="torchcattensors-dim"><code>torch.cat(tensors, dim)</code><a class="headerlink" href="#torchcattensors-dim" title="Permanent link">&para;</a></h5>
<p>沿着指定维度拼接多个张量。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="n">concatenated_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 在第0维拼接</span>
<span class="nb">print</span><span class="p">(</span><span class="n">concatenated_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[1, 2],</span>
<span class="c1">#         [3, 4],</span>
<span class="c1">#         [5, 6],</span>
<span class="c1">#         [7, 8]])</span>
</code></pre></div>
<h5 id="torchstacktensors-dim"><code>torch.stack(tensors, dim)</code><a class="headerlink" href="#torchstacktensors-dim" title="Permanent link">&para;</a></h5>
<p>在新维度上堆叠多个张量。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">stacked_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 在第1维堆叠</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stacked_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[1, 4],</span>
<span class="c1">#         [2, 5],</span>
<span class="c1">#         [3, 6]])</span>
</code></pre></div>
<h3 id="5">5. 张量变换<a class="headerlink" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 重塑形状<a class="headerlink" href="#51" title="Permanent link">&para;</a></h4>
<h5 id="tensorviewshape"><code>tensor.view(shape)</code><a class="headerlink" href="#tensorviewshape" title="Permanent link">&para;</a></h5>
<p>返回给定形状的张量视图，原始张量的形状必须与新形状兼容。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">reshaped_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># 重塑为1x4</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reshaped_tensor</span><span class="p">)</span>
<span class="c1"># 输出: tensor([[1, 2, 3, 4]])</span>
</code></pre></div>
<h5 id="tensorreshapeshape"><code>tensor.reshape(shape)</code><a class="headerlink" href="#tensorreshapeshape" title="Permanent link">&para;</a></h5>
<p>改变张量的形状，返回一个具有指定形状的新张量。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">reshaped_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># 重塑为1x4</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reshaped_tensor</span><span class="p">)</span>
<span class="c1"># 输出: tensor([[1, 2, 3, 4]])</span>
</code></pre></div>
<h4 id="52">5.2 转置与交换维度<a class="headerlink" href="#52" title="Permanent link">&para;</a></h4>
<h5 id="tensortransposedim0-dim1"><code>tensor.transpose(dim0, dim1)</code><a class="headerlink" href="#tensortransposedim0-dim1" title="Permanent link">&para;</a></h5>
<p>交换张量中两个维度的位置。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">transposed_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 交换第0维和第1维</span>
<span class="nb">print</span><span class="p">(</span><span class="n">transposed_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[1, 3],</span>
<span class="c1">#         [2, 4]])</span>
</code></pre></div>
<h5 id="tensorpermutedims"><code>tensor.permute(*dims)</code><a class="headerlink" href="#tensorpermutedims" title="Permanent link">&para;</a></h5>
<p>按照指定顺序排列张量的维度。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="n">permuted_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 维度顺序变为(1, 0, 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">permuted_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出: torch.Size([2, 2, 2])</span>
</code></pre></div>
<h4 id="53">5.3 增减维度<a class="headerlink" href="#53" title="Permanent link">&para;</a></h4>
<h5 id="tensorsqueeze"><code>tensor.squeeze()</code><a class="headerlink" href="#tensorsqueeze" title="Permanent link">&para;</a></h5>
<p>删除所有长度为1的维度。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]])</span>
<span class="n">squeezed_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>  <span class="c1"># 删除多余的维度</span>
<span class="nb">print</span><span class="p">(</span><span class="n">squeezed_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[1, 2],</span>
<span class="c1">#         [3, 4]])</span>
</code></pre></div>
<h5 id="tensorunsqueezedim"><code>tensor.unsqueeze(dim)</code><a class="headerlink" href="#tensorunsqueezedim" title="Permanent link">&para;</a></h5>
<p>在指定位置增加一个长度为1的新维度。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">unsqueezed_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 在第0维增加一个新维度</span>
<span class="nb">print</span><span class="p">(</span><span class="n">unsqueezed_tensor</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[[1, 2],</span>
<span class="c1">#          [3, 4]]])</span>
</code></pre></div>
<h3 id="6">6. 数学运算<a class="headerlink" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 基本运算<a class="headerlink" href="#61" title="Permanent link">&para;</a></h4>
<h5 id="torchaddx-y"><code>torch.add(x, y)</code><a class="headerlink" href="#torchaddx-y" title="Permanent link">&para;</a></h5>
<p>对两个张量进行逐元素加法运算。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([5, 7, 9])</span>
</code></pre></div>
<h5 id="torchsubx-y"><code>torch.sub(x, y)</code><a class="headerlink" href="#torchsubx-y" title="Permanent link">&para;</a></h5>
<p>对两个张量进行逐元素减法运算。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([-3, -3, -3])</span>
</code></pre></div>
<h5 id="torchmulx-y"><code>torch.mul(x, y)</code><a class="headerlink" href="#torchmulx-y" title="Permanent link">&para;</a></h5>
<p>对两个张量进行逐元素乘法运算。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([ 4, 10, 18])</span>
</code></pre></div>
<h5 id="torchdivx-y"><code>torch.div(x, y)</code><a class="headerlink" href="#torchdivx-y" title="Permanent link">&para;</a></h5>
<p>对两个张量进行逐元素除法运算。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([0.2500, 0.4000, 0.5000])</span>
</code></pre></div>
<h5 id="torchmatmulx-y"><code>torch.matmul(x, y)</code><a class="headerlink" href="#torchmatmulx-y" title="Permanent link">&para;</a></h5>
<p>计算两个张量的矩阵乘法。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="c1"># tensor([[19, 22],</span>
<span class="c1">#         [43, 50]])</span>
</code></pre></div>
<h5 id="torchpowbase-exponent"><code>torch.pow(base, exponent)</code><a class="headerlink" href="#torchpowbase-exponent" title="Permanent link">&para;</a></h5>
<p>计算张量的幂。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">exponent</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">exponent</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1, 4, 9])</span>
</code></pre></div>
<h5 id="torchexptensor"><code>torch.exp(tensor)</code><a class="headerlink" href="#torchexptensor" title="Permanent link">&para;</a></h5>
<p>计算张量中所有元素的指数。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([  2.7183,   7.3891,  20.0855])</span>
</code></pre></div>
<h5 id="torchsqrttensor"><code>torch.sqrt(tensor)</code><a class="headerlink" href="#torchsqrttensor" title="Permanent link">&para;</a></h5>
<p>计算张量中所有元素的平方根。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1.0000, 2.0000, 3.0000])</span>
</code></pre></div>
<h4 id="62">6.2 汇总统计<a class="headerlink" href="#62" title="Permanent link">&para;</a></h4>
<h5 id="torchsuminput"><code>torch.sum(input)</code><a class="headerlink" href="#torchsuminput" title="Permanent link">&para;</a></h5>
<p>计算张量中所有元素的和。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor(10)</span>
</code></pre></div>
<h5 id="torchmeaninput"><code>torch.mean(input)</code><a class="headerlink" href="#torchmeaninput" title="Permanent link">&para;</a></h5>
<p>计算张量中所有元素的平均值。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor(2.5000)</span>
</code></pre></div>
<h5 id="torchmaxinput"><code>torch.max(input)</code><a class="headerlink" href="#torchmaxinput" title="Permanent link">&para;</a></h5>
<p>找出张量中所有元素的最大值。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor(4)</span>
</code></pre></div>
<h5 id="torchmininput"><code>torch.min(input)</code><a class="headerlink" href="#torchmininput" title="Permanent link">&para;</a></h5>
<p>找出张量中所有元素的最小值。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor(1)</span>
</code></pre></div>
<h5 id="torchstdinput"><code>torch.std(input)</code><a class="headerlink" href="#torchstdinput" title="Permanent link">&para;</a></h5>
<p>计算张量中所有元素的标准差。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor(1.29099)</span>
</code></pre></div>
<h5 id="torchvarinput"><code>torch.var(input)</code><a class="headerlink" href="#torchvarinput" title="Permanent link">&para;</a></h5>
<p>计算张量中所有元素的方差。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># 输出: tensor(1.6667)</span>
</code></pre></div>
<h3 id="7">7. 梯度相关操作<a class="headerlink" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 标记张量需要计算梯度<a class="headerlink" href="#71" title="Permanent link">&para;</a></h4>
<p>使用 <code>tensor.requires_grad_()</code> 标记张量以便在反向传播中计算梯度。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1., 2., 3.], requires_grad=True)</span>
</code></pre></div>
<h4 id="72">7.2 获取张量的梯度<a class="headerlink" href="#72" title="Permanent link">&para;</a></h4>
<p>在进行反向传播后，<code>tensor.grad</code> 会包含相对于该张量的梯度。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1., 1., 1.])</span>
</code></pre></div>
<h4 id="73">7.3 计算梯度<a class="headerlink" href="#73" title="Permanent link">&para;</a></h4>
<p>使用 <code>tensor.backward()</code> 计算张量的梯度值，前提是该张量需要计算梯度。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1., 1., 1.])</span>
</code></pre></div>
<h3 id="8">8. 数据管理<a class="headerlink" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 将张量移动到指定设备<a class="headerlink" href="#81" title="Permanent link">&para;</a></h4>
<p>使用 <code>tensor.to(device)</code> 将张量移动到指定的设备，如 GPU。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># 输出: cuda:0 或 cpu</span>
</code></pre></div>
<h4 id="82">8.2 保存与加载张量<a class="headerlink" href="#82" title="Permanent link">&para;</a></h4>
<h5 id="torchsaveobj-f"><code>torch.save(obj, f)</code><a class="headerlink" href="#torchsaveobj-f" title="Permanent link">&para;</a></h5>
<p>将对象保存到文件中。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>  <span class="c1"># 将张量保存到文件</span>
</code></pre></div>
<h5 id="torchloadf"><code>torch.load(f)</code><a class="headerlink" href="#torchloadf" title="Permanent link">&para;</a></h5>
<p>从文件中加载对象。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>  <span class="c1"># 从文件加载张量</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1, 2, 3])</span>
</code></pre></div>
<h3 id="9">9. 实用技巧<a class="headerlink" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 类型转换<a class="headerlink" href="#91" title="Permanent link">&para;</a></h4>
<p>将张量转换为不同的数据类型。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">float_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">int_tensor</span> <span class="o">=</span> <span class="n">float_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">int_tensor</span><span class="p">)</span>
<span class="c1"># 输出示例:</span>
<span class="c1"># tensor([[0, 0, 0],</span>
<span class="c1">#         [0, 0, 0]], dtype=torch.int32)</span>
</code></pre></div>
<h4 id="92">9.2 复制与克隆<a class="headerlink" href="#92" title="Permanent link">&para;</a></h4>
<p>使用 <code>clone()</code> 创建张量的独立副本。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">copy</span> <span class="o">=</span> <span class="n">original</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">copy</span><span class="p">)</span>
<span class="c1"># 输出: tensor([1, 2, 3])</span>
</code></pre></div>
<h3 id="10">10. 注意事项<a class="headerlink" href="#10" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>叶子节点</strong>：只有叶子节点（在计算图的起点）的张量才会保存梯度。如果对叶子节点进行操作生成的新张量，默认 <code>requires_grad=True</code>，但不会保存 <code>grad</code>，除非调用 <code>retain_grad()</code>。</p>
</li>
<li>
<p><strong>就地操作</strong>：某些就地操作（如 <code>x += 1</code>）可能会破坏计算图，导致 Autograd 无法正确计算梯度。建议避免在需要梯度的张量上进行就地操作。</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># 推荐</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># 不推荐</span>
<span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># 可能导致梯度计算错误</span>
</code></pre></div>
<table>
<thead>
<tr>
<th>函数</th>
<th>描述</th>
<th>示例用法</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>torch.tensor(data)</code></td>
<td>从数据创建张量</td>
<td><code>torch.tensor([1, 2, 3])</code></td>
</tr>
<tr>
<td><code>torch.zeros(size)</code></td>
<td>创建指定大小的全零张量</td>
<td><code>torch.zeros((2, 3))</code></td>
</tr>
<tr>
<td><code>torch.ones(size)</code></td>
<td>创建指定大小的全一张量</td>
<td><code>torch.ones((2, 3))</code></td>
</tr>
<tr>
<td><code>torch.empty(size)</code></td>
<td>创建未初始化的张量</td>
<td><code>torch.empty((2, 3))</code></td>
</tr>
<tr>
<td><code>torch.randn(size)</code></td>
<td>创建指定大小的标准正态分布张量</td>
<td><code>torch.randn((2, 3))</code></td>
</tr>
<tr>
<td><code>torch.arange(start, end, step)</code></td>
<td>创建一个范围内的一维张量，步长为 <code>step</code></td>
<td><code>torch.arange(0, 5, 1)</code></td>
</tr>
<tr>
<td><code>torch.linspace(start, end, steps)</code></td>
<td>创建一个范围内均匀间隔的一维张量，步数为 <code>steps</code></td>
<td><code>torch.linspace(0, 5, 5)</code></td>
</tr>
<tr>
<td><code>tensor.view(shape)</code></td>
<td>重塑张量形状</td>
<td><code>tensor.view(1, 4)</code></td>
</tr>
<tr>
<td><code>tensor.reshape(shape)</code></td>
<td>改变张量形状</td>
<td><code>tensor.reshape(1, 4)</code></td>
</tr>
<tr>
<td><code>tensor.transpose(dim0, dim1)</code></td>
<td>交换张量的两个维度</td>
<td><code>tensor.transpose(0, 1)</code></td>
</tr>
<tr>
<td><code>tensor.permute(*dims)</code></td>
<td>按指定顺序排列张量的维度</td>
<td><code>tensor.permute(1, 0, 2)</code></td>
</tr>
<tr>
<td><code>tensor.squeeze()</code></td>
<td>删除所有长度为1的维度</td>
<td><code>tensor.squeeze()</code></td>
</tr>
<tr>
<td><code>tensor.unsqueeze(dim)</code></td>
<td>在指定位置增加一个长度为1的新维度</td>
<td><code>tensor.unsqueeze(0)</code></td>
</tr>
<tr>
<td><code>torch.add(x, y)</code></td>
<td>逐元素加法</td>
<td><code>torch.add(x, y)</code></td>
</tr>
<tr>
<td><code>torch.sub(x, y)</code></td>
<td>逐元素减法</td>
<td><code>torch.sub(x, y)</code></td>
</tr>
<tr>
<td><code>torch.mul(x, y)</code></td>
<td>逐元素乘法</td>
<td><code>torch.mul(x, y)</code></td>
</tr>
<tr>
<td><code>torch.div(x, y)</code></td>
<td>逐元素除法</td>
<td><code>torch.div(x, y)</code></td>
</tr>
<tr>
<td><code>torch.matmul(x, y)</code></td>
<td>矩阵乘法</td>
<td><code>torch.matmul(x, y)</code></td>
</tr>
<tr>
<td><code>torch.pow(base, exponent)</code></td>
<td>计算张量的幂</td>
<td><code>torch.pow(base, exponent)</code></td>
</tr>
<tr>
<td><code>torch.exp(tensor)</code></td>
<td>计算张量中所有元素的指数</td>
<td><code>torch.exp(tensor)</code></td>
</tr>
<tr>
<td><code>torch.sqrt(tensor)</code></td>
<td>计算张量中所有元素的平方根</td>
<td><code>torch.sqrt(tensor)</code></td>
</tr>
<tr>
<td><code>torch.sum(input)</code></td>
<td>计算张量中所有元素的和</td>
<td><code>torch.sum(tensor)</code></td>
</tr>
<tr>
<td><code>torch.mean(input)</code></td>
<td>计算张量中所有元素的平均值</td>
<td><code>torch.mean(tensor)</code></td>
</tr>
<tr>
<td><code>torch.max(input)</code></td>
<td>找出张量中所有元素的最大值</td>
<td><code>torch.max(tensor)</code></td>
</tr>
<tr>
<td><code>torch.min(input)</code></td>
<td>找出张量中所有元素的最小值</td>
<td><code>torch.min(tensor)</code></td>
</tr>
<tr>
<td><code>torch.std(input)</code></td>
<td>计算张量中所有元素的标准差</td>
<td><code>torch.std(tensor)</code></td>
</tr>
<tr>
<td><code>torch.var(input)</code></td>
<td>计算张量中所有元素的方差</td>
<td><code>torch.var(tensor)</code></td>
</tr>
<tr>
<td><code>tensor.requires_grad_()</code></td>
<td>标记张量需要计算梯度</td>
<td><code>tensor.requires_grad_()</code></td>
</tr>
<tr>
<td><code>tensor.backward()</code></td>
<td>计算张量的梯度值</td>
<td><code>tensor.backward()</code></td>
</tr>
<tr>
<td><code>tensor.to(device)</code></td>
<td>将张量移动到指定设备（如GPU）</td>
<td><code>tensor.to('cuda')</code></td>
</tr>
<tr>
<td><code>torch.save(obj, f)</code></td>
<td>将对象保存到文件中</td>
<td><code>torch.save(tensor, 'file.pt')</code></td>
</tr>
<tr>
<td><code>torch.load(f)</code></td>
<td>从文件中加载对象</td>
<td><code>torch.load('file.pt')</code></td>
</tr>
<tr>
<td><code>clone()</code></td>
<td>创建张量的独立副本</td>
<td><code>copy = original.clone()</code></td>
</tr>
</tbody>
</table>
<script src="https://giscus.app/client.js"
        data-repo="InuyashaYang/AIDIY"
        data-repo-id="R_kgDOM1VVTQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOM1VVTc4Ckls_"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Joining-AI/LLM_Interview_Prepare" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "navigation.sections", "navigation.tabs"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.525ec568.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="../../../javascripts/mathjax_config.js"></script>
      
    
  </body>
</html>