<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>节点和边信息</title>
    <style>
        body, html {
            width: 100%;
            height: 100%;
            margin: 0;
            padding: 0;
            overflow: hidden;
            display: flex;
        }
        svg {
            flex: 1;
            display: block;
        }
        .node circle {
            stroke: #fff;
            stroke-width: 1.5px;
        }
        .edge {
            stroke: #999;
            stroke-opacity: 0.6;
        }
        text {
            font-size: 11px;
            pointer-events: none;
        }
        #sidebar {
            width: 300px;
            height: 100%;
            background-color: #f0f0f0;
            border-left: 1px solid #ddd;
            padding: 20px;
            box-shadow: -2px 0 5px rgba(0,0,0,0.1);
            overflow-y: auto;
            transition: transform 0.3s ease;
            position: fixed;
            top: 0;
            right: 0;
            transform: translateX(100%);
        }
        #sidebar.show {
            transform: translateX(0);
        }
        #toggleSidebar, #reset {
            position: absolute;
            top: 10px;
            background-color: #007bff;
            color: white;
            border: none;
            padding: 10px;
            cursor: pointer;
            z-index: 100;
            border-radius: 4px;
        }
        #toggleSidebar {
            right: 10px;
        }
        #reset {
            right: 130px;
        }
        .selected-node {
            fill: gold;
            stroke: #fff;
            stroke-width: 1.5px;
        }
        .neighbor-missing-node {
            fill: green;
        }
    </style>
</head>
<body>
    <svg id="canvas"></svg>
    <div id="sidebar">
        <h2 id="sidebar-title">节点信息</h2>
        <div id="sidebar-content">
            <p>点击一个节点查看详细信息</p>
            <h3>定义</h3>
            <p id="definition"></p>
            <h3>适用场景</h3>
            <p id="applicable_scenarios"></p>
            <h3>核心思想</h3>
            <p id="core_idea"></p>
            <h3>优势特点</h3>
            <ul id="advantages"></ul>
            <h3>局限性</h3>
            <ul id="limitations"></ul>
            <h3>应用领域</h3>
            <p id="application_fields"></p>
            <h3>关键组件</h3>
            <ul id="key_components"></ul>
            <h3>相关模型/算法</h3>
            <ul id="related_models"></ul>
            <h3>关系</h3>
            <ul id="relations"></ul>
        </div>
    </div>
    <button id="toggleSidebar">隐藏信息</button>
    <button id="reset">重置视图</button>

    <!-- 加载 D3.js 和主脚本 -->
    <script>
        /**
         * 尝试加载 D3.js 的函数
         * @param {Array} urls - D3.js 的多个 CDN 源
         * @param {Function} callback - D3.js 成功加载后的回调函数
         */
        function loadD3(urls, callback) {
            console.log("尝试加载 D3.js...");
            // 如果 D3.js 已经加载，直接执行回调
            if (typeof d3 !== 'undefined') {
                console.log("D3.js 已经加载。");
                callback();
                return;
            }

            // 如果没有更多的 URL 进行尝试，显示错误信息
            if (urls.length === 0) {
                console.error('所有CDN源都加载失败！');
                alert('无法加载D3.js，请检查网络连接或联系管理员。');
                return;
            }

            const currentUrl = urls[0];
            console.log(`尝试从 ${currentUrl} 加载 D3.js`);
            const script = document.createElement('script');
            script.src = currentUrl;
            script.onload = function() {
                if (typeof d3 !== 'undefined') {
                    console.log('D3.js 成功从:', currentUrl, '加载！');
                    callback();
                } else {
                    console.warn('D3.js 未定义，从', currentUrl, '加载失败，尝试下一个CDN源...');
                    loadD3(urls.slice(1), callback);
                }
            };
            script.onerror = function() {
                console.warn('加载失败:', currentUrl, '，尝试下一个CDN源...');
                loadD3(urls.slice(1), callback);
            };
            document.head.appendChild(script);
        }

        // 定义多个CDN源，并将本地文件作为最后的备选
        const d3Urls = [
            'https://cdn.jsdelivr.net/npm/d3@7/dist/d3.min.js',
            'https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js',
            'https://unpkg.com/d3@7/dist/d3.min.js',
            'https://d3js.org/d3.v7.min.js',
            'd3.min.js' // 本地文件
        ];

        /**
         * 在 D3.js 加载完成后执行的主函数
         */
         function main() {
            console.log("D3.js 已加载，执行主函数。");
            try {
                // 嵌入实体和关系数据，确保使用 tojson 和 safe 过滤器
                const entities = {
    "过拟合": {
        "模型/算法名称": "过拟合",
        "定义": "模型在训练数据上学习过度导致泛化能力下降",
        "适用场景": "机器学习模型训练过程中",
        "核心思想": "模型过度拟合训练数据中的噪声和细节",
        "优势特点": "无",
        "局限性": "泛化性能差,无法适应新数据",
        "应用领域": "机器学习模型评估与优化",
        "关键组件": "正则化,交叉验证,数据增强"
    },
    "交叉验证": {
        "模型/算法名称": "交叉验证",
        "定义": "一种评估模型泛化能力的统计方法",
        "适用场景": "模型选择和超参数调优",
        "核心思想": "通过数据分割验证模型性能",
        "优势特点": "减少过拟合风险,提高结果可靠性",
        "局限性": "计算成本较高,数据分割影响结果",
        "应用领域": "机器学习,数据挖掘",
        "关键组件": "训练集,验证集,评估指标"
    },
    "负对数似然损失函数": {
        "模型/算法名称": "负对数似然损失函数",
        "定义": "通过取对数似然函数的负值构造的损失函数",
        "适用场景": "分类问题特别是二分类问题",
        "核心思想": "最大化似然函数等价于最小化负对数似然",
        "优势特点": "与交叉熵损失等价,优化目标明确",
        "局限性": "对异常值敏感,需要概率模型支持",
        "应用领域": "逻辑回归,神经网络分类任务",
        "关键组件": "对数变换,概率模型,梯度下降"
    },
    "正则化": {
        "模型/算法名称": "正则化",
        "定义": "通过添加约束项防止模型过拟合的技术",
        "适用场景": "模型复杂度过高或训练数据不足时",
        "核心思想": "在损失函数中加入惩罚项限制参数",
        "优势特点": "提高泛化能力,控制模型复杂度",
        "局限性": "可能欠拟合,需调整正则化系数",
        "应用领域": "机器学习,深度学习,统计建模",
        "关键组件": "正则项,惩罚系数,损失函数"
    },
    "Softmax 函数": {
        "模型/算法名称": "Softmax 函数",
        "定义": "将实数向量映射为概率分布的函数",
        "适用场景": "多类别分类问题",
        "核心思想": "通过指数变换和归一化输出类别概率",
        "优势特点": "输出概率解释性强,可微性好",
        "局限性": "对异常值敏感,计算复杂度较高",
        "应用领域": "神经网络分类器,逻辑回归",
        "关键组件": "指数运算,归一化因子"
    },
    "Sigmoid函数": {
        "模型/算法名称": "Sigmoid函数",
        "定义": "一种S型曲线函数,将实数映射到(0,1)区间",
        "适用场景": "二分类问题中的概率预测",
        "核心思想": "通过非线性变换将输入压缩到概率区间",
        "优势特点": "输出平滑可微,概率解释性强",
        "局限性": "容易梯度消失,输出非零中心化",
        "应用领域": "逻辑回归,神经网络激活函数",
        "关键组件": "指数运算,归一化处理"
    },
    "Adam": {
        "模型/算法名称": "Adam",
        "定义": "一种自适应矩估计的优化算法",
        "适用场景": "大规模数据训练和深度学习任务",
        "核心思想": "结合动量法和自适应学习率调整",
        "优势特点": "高效收敛,参数自适应调整,计算资源节省",
        "局限性": "超参数敏感,可能收敛到次优点",
        "应用领域": "深度学习模型训练",
        "关键组件": "动量项,自适应学习率,偏差校正"
    },
    "批量梯度下降": {
        "模型/算法名称": "批量梯度下降",
        "定义": "一种使用整个数据集计算梯度的优化算法",
        "适用场景": "中小规模数据集或凸优化问题",
        "核心思想": "通过全局梯度信息更新模型参数",
        "优势特点": "更新方向稳定,易于收敛到全局最优",
        "局限性": "计算成本高,大规模数据效率低",
        "应用领域": "机器学习模型训练,参数优化",
        "关键组件": "损失函数,学习率,全局梯度"
    },
    "L2 正则化": {
        "模型/算法名称": "L2 正则化",
        "定义": "通过在损失函数中加入参数平方和的惩罚项来防止过拟合",
        "适用场景": "机器学习模型训练中的过拟合控制",
        "核心思想": "限制参数值大小以降低模型复杂度",
        "优势特点": "计算简单,参数平滑,防止极端参数值",
        "局限性": "可能导致欠拟合,需要调整正则化强度",
        "应用领域": "线性回归,逻辑回归,神经网络",
        "关键组件": "正则化项,惩罚系数,梯度更新规则"
    },
    "对数似然函数": {
        "模型/算法名称": "对数似然函数",
        "定义": "用于评估模型参数在给定数据下的概率的函数",
        "适用场景": "统计建模、参数估计、分类问题",
        "核心思想": "通过最大化似然函数来估计最优参数",
        "优势特点": "数学性质良好,便于优化,理论支持强",
        "局限性": "对模型假设敏感,可能过拟合",
        "应用领域": "机器学习,统计学,数据科学",
        "关键组件": "概率模型,对数变换,优化算法"
    },
    "强化学习": {
        "模型/算法名称": "强化学习",
        "定义": "通过与环境交互学习最优策略的机器学习方法",
        "适用场景": "决策制定、游戏AI、机器人控制",
        "核心思想": "最大化累积奖励信号来指导行为",
        "优势特点": "适应动态环境,无需监督信号,长期规划",
        "局限性": "训练不稳定,样本效率低,奖励设计困难",
        "应用领域": "自动驾驶、金融交易、工业控制",
        "关键组件": "智能体,环境,奖励函数,价值函数"
    },
    "提前停止": {
        "模型/算法名称": "提前停止",
        "定义": "在验证集性能不再提升时终止训练过程",
        "适用场景": "防止模型在训练数据上过度拟合",
        "核心思想": "通过监控验证集表现控制训练轮次",
        "优势特点": "简单有效,无需额外计算开销",
        "局限性": "依赖验证集质量,可能过早停止",
        "应用领域": "深度学习模型训练过程",
        "关键组件": "验证集,性能评估指标"
    },
    "生成对抗网络": {
        "模型/算法名称": "生成对抗网络",
        "定义": "一种通过生成器与判别器对抗训练生成数据的深度学习模型",
        "适用场景": "数据生成、图像合成、风格迁移",
        "核心思想": "生成器与判别器在对抗中共同优化",
        "优势特点": "无需显式建模,生成样本质量高",
        "局限性": "训练不稳定,模式崩溃风险",
        "应用领域": "图像生成、数据增强、艺术创作",
        "关键组件": "生成器网络,判别器网络"
    },
    "数据增强": {
        "模型/算法名称": "数据增强",
        "定义": "通过增加训练数据的多样性提升模型泛化能力",
        "适用场景": "训练数据不足或多样性不足时",
        "核心思想": "生成具有语义一致性的新数据样本",
        "优势特点": "减少过拟合,提高模型鲁棒性",
        "局限性": "可能引入无效噪声,依赖领域知识",
        "应用领域": "计算机视觉,自然语言处理",
        "关键组件": "变换方法,生成策略"
    },
    "动量法": {
        "模型/算法名称": "动量法",
        "定义": "一种通过累积历史梯度来加速优化的梯度下降变体",
        "适用场景": "高维非凸优化问题,尤其是深度学习",
        "核心思想": "利用动量项平滑梯度更新方向",
        "优势特点": "加速收敛,减少震荡",
        "局限性": "需调节动量系数,可能 overshooting",
        "应用领域": "神经网络训练,强化学习",
        "关键组件": "动量系数,历史梯度累积项"
    },
    "最大似然估计": {
        "模型/算法名称": "最大似然估计",
        "定义": "通过最大化似然函数来估计模型参数的方法",
        "适用场景": "参数估计、统计建模、分类问题",
        "核心思想": "寻找使观测数据出现概率最大的参数值",
        "优势特点": "理论完备,计算直观,适用于多种分布",
        "局限性": "依赖模型假设,可能过拟合,需要足够数据",
        "应用领域": "机器学习,统计学,经济学",
        "关键组件": "似然函数,对数变换,优化算法"
    },
    "伯努利分布": {
        "模型/算法名称": "伯努利分布",
        "定义": "描述单次二值试验结果的离散概率分布",
        "适用场景": "二分类问题、单次独立试验建模",
        "核心思想": "用单一参数描述事件成功或失败的概率",
        "优势特点": "形式简单,参数易于解释,计算高效",
        "局限性": "仅适用于独立单次试验,无法建模复杂依赖",
        "应用领域": "统计学、机器学习、质量检测",
        "关键组件": "成功概率参数,二值随机变量"
    },
    "交叉熵损失": {
        "模型/算法名称": "交叉熵损失",
        "定义": "衡量模型预测概率分布与真实分布差异的损失函数",
        "适用场景": "分类任务,特别是二分类和多分类问题",
        "核心思想": "最小化预测概率与真实标签之间的信息差异",
        "优势特点": "梯度计算高效,适用于概率输出模型",
        "局限性": "对噪声标签敏感,可能过拟合",
        "应用领域": "逻辑回归,神经网络分类器",
        "关键组件": "对数运算,概率分布比较"
    },
    "Huber 损失": {
        "模型/算法名称": "Huber 损失",
        "定义": "一种对噪声标签不敏感的鲁棒损失函数",
        "适用场景": "处理含噪声或不完整数据的回归问题",
        "核心思想": "结合均方误差和绝对误差的优点",
        "优势特点": "对异常值不敏感,数值稳定性好",
        "局限性": "需要调整超参数δ,计算略复杂",
        "应用领域": "机器学习,统计建模,强化学习",
        "关键组件": "超参数δ,分段函数设计"
    },
    "梯度下降": {
        "模型/算法名称": "梯度下降",
        "定义": "通过迭代调整参数以最小化目标函数的优化算法",
        "适用场景": "大规模参数优化问题",
        "核心思想": "沿负梯度方向更新参数逐步逼近最优解",
        "优势特点": "实现简单,适用于高维空间",
        "局限性": "可能陷入局部最优,收敛速度不稳定",
        "应用领域": "机器学习模型训练,深度学习",
        "关键组件": "学习率,梯度计算,参数更新规则"
    },
    "半监督学习": {
        "模型/算法名称": "半监督学习",
        "定义": "利用少量标注数据和大量未标注数据进行训练的机器学习方法",
        "适用场景": "标注数据稀缺但未标注数据丰富的任务",
        "核心思想": "通过未标注数据挖掘潜在结构辅助模型训练",
        "优势特点": "减少对标注数据的依赖,提升模型泛化能力",
        "局限性": "未标注数据质量影响大,训练过程更复杂",
        "应用领域": "自然语言处理,计算机视觉,语音识别",
        "关键组件": "标注数据,未标注数据,一致性正则化"
    },
    "Sigmoid 分类器": {
        "模型/算法名称": "Sigmoid 分类器",
        "定义": "使用Sigmoid函数将线性组合映射为概率的分类模型",
        "适用场景": "二分类问题,概率预测任务",
        "核心思想": "通过Sigmoid函数将输入线性变换转换为0-1概率",
        "优势特点": "输出概率解释性强,计算梯度简单",
        "局限性": "仅适用于线性可分问题,易受异常值影响",
        "应用领域": "逻辑回归,神经网络输出层",
        "关键组件": "Sigmoid函数,线性权重参数"
    },
    "自监督学习": {
        "模型/算法名称": "自监督学习",
        "定义": "利用数据本身生成监督信号进行训练的机器学习方法",
        "适用场景": "数据标注成本高或标注稀缺的情况",
        "核心思想": "从未标注数据中自动构造监督任务",
        "优势特点": "减少人工标注依赖,利用大量无标签数据",
        "局限性": "预训练任务设计复杂,可能引入偏差",
        "应用领域": "自然语言处理,计算机视觉,语音识别",
        "关键组件": "预训练任务设计,特征提取器,下游任务适配"
    },
    "Dropout": {
        "模型/算法名称": "Dropout",
        "定义": "一种通过随机丢弃神经元防止过拟合的正则化技术",
        "适用场景": "深度神经网络训练",
        "核心思想": "前向传播时随机屏蔽部分神经元以破坏协同适应性",
        "优势特点": "减少过拟合,提升泛化能力,实现神经元独立决策",
        "局限性": "训练时间延长,可能损失部分有效信息",
        "应用领域": "图像识别,自然语言处理,语音识别",
        "关键组件": "丢弃率,随机屏蔽机制,神经元激活状态"
    }
};  // 使用特殊占位符
                const relations = {
    "0": {
        "出发节点": "Softmax 函数",
        "到达节点": "交叉验证",
        "关系名称": "优化验证关系",
        "关系解释": "交叉验证用于评估和优化包含Softmax函数的分类模型的泛化性能,防止过拟合",
        "关系强度": 7
    },
    "1": {
        "出发节点": "正则化",
        "到达节点": "Softmax 函数",
        "关系名称": "优化关系",
        "关系解释": "正则化通过限制模型参数的复杂度来优化Softmax函数的输出,防止过拟合,从而提高模型的泛化能力",
        "关系强度": 8
    },
    "2": {
        "出发节点": "批量梯度下降",
        "到达节点": "梯度下降",
        "关系名称": "特例关系",
        "关系解释": "批量梯度下降是梯度下降的一种特例,其中每次参数更新使用全部训练数据计算梯度",
        "关系强度": 8
    },
    "3": {
        "出发节点": "梯度下降",
        "到达节点": "动量法",
        "关系名称": "扩展关系",
        "关系解释": "动量法在梯度下降的基础上引入了动量项,通过累积历史梯度信息来加速收敛并减少震荡",
        "关系强度": 8
    },
    "4": {
        "出发节点": "批量梯度下降",
        "到达节点": "生成对抗网络",
        "关系名称": "优化基础关系",
        "关系解释": "批量梯度下降作为优化算法的基础方法,为生成对抗网络中的判别器和生成器参数更新提供核心优化框架",
        "关系强度": 8
    },
    "5": {
        "出发节点": "正则化",
        "到达节点": "交叉验证",
        "关系名称": "协同优化关系",
        "关系解释": "正则化通过约束模型复杂度防止过拟合,而交叉验证通过评估模型在验证集上的表现来选择最优正则化强度,二者协同优化模型泛化能力",
        "关系强度": 8
    },
    "6": {
        "出发节点": "批量梯度下降",
        "到达节点": "Dropout",
        "关系名称": "互补关系",
        "关系解释": "批量梯度下降通过全局参数更新优化模型,而Dropout通过随机丢弃神经元防止过拟合,二者在训练过程中分别从优化和正则化角度提升模型性能",
        "关系强度": 7
    },
    "7": {
        "出发节点": "Sigmoid函数",
        "到达节点": "梯度下降",
        "关系名称": "计算基础关系",
        "关系解释": "Sigmoid函数作为二分类模型的概率输出函数,其导数形式直接决定了梯度下降中参数更新的计算表达式",
        "关系强度": 8
    },
    "8": {
        "出发节点": "正则化",
        "到达节点": "提前停止",
        "关系名称": "互补关系",
        "关系解释": "正则化和提前停止都是缓解过拟合的方法,正则化通过限制参数复杂度防止过拟合,而提前停止通过监控验证集性能在适当时机终止训练.两者从不同角度共同解决过拟合问题,形成互补策略.",
        "关系强度": 8
    },
    "9": {
        "出发节点": "正则化",
        "到达节点": "L2 正则化",
        "关系名称": "特例关系",
        "关系解释": "L2 正则化是正则化的一种具体形式,通过在损失函数中添加参数的L2范数作为惩罚项来实现正则化",
        "关系强度": 8
    },
    "10": {
        "出发节点": "梯度下降",
        "到达节点": "对数似然函数",
        "关系名称": "优化关系",
        "关系解释": "梯度下降通过最小化负对数似然函数来优化模型参数,从而间接最大化原始对数似然函数",
        "关系强度": 9
    },
    "11": {
        "出发节点": "Softmax 函数",
        "到达节点": "数据增强",
        "关系名称": "互补关系",
        "关系解释": "Softmax 函数用于多类别分类中计算概率分布,而数据增强通过增加训练数据的多样性来帮助模型学习更泛化的特征,从而间接优化Softmax函数的输出效果",
        "关系强度": 7
    },
    "12": {
        "出发节点": "梯度下降",
        "到达节点": "伯努利分布",
        "关系名称": "优化关系",
        "关系解释": "梯度下降用于优化伯努利分布的最大似然估计参数",
        "关系强度": 8
    },
    "13": {
        "出发节点": "Huber 损失",
        "到达节点": "Sigmoid 分类器",
        "关系名称": "鲁棒性增强关系",
        "关系解释": "Huber 损失通过减少异常值对梯度的影响,增强了 Sigmoid 分类器在噪声数据下的鲁棒性",
        "关系强度": 7
    },
    "14": {
        "出发节点": "批量梯度下降",
        "到达节点": "Adam",
        "关系名称": "扩展关系",
        "关系解释": "Adam通过结合动量法和自适应学习率机制,扩展了批量梯度下降的优化能力,使其更适应不同参数的特性和大规模数据训练",
        "关系强度": 8
    },
    "15": {
        "出发节点": "Sigmoid函数",
        "到达节点": "负对数似然损失函数",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid函数作为概率输出函数,是负对数似然损失函数的核心组成部分,用于计算二分类问题的概率预测",
        "关系强度": 8
    },
    "16": {
        "出发节点": "Huber 损失",
        "到达节点": "动量法",
        "关系名称": "互补关系",
        "关系解释": "Huber损失通过鲁棒性处理噪声数据,动量法通过梯度平滑加速收敛,二者在优化过程中分别从损失函数设计和梯度更新策略角度提升模型性能",
        "关系强度": 7
    },
    "17": {
        "出发节点": "Softmax 函数",
        "到达节点": "L2 正则化",
        "关系名称": "优化关系",
        "关系解释": "L2 正则化通过限制模型参数的复杂度来优化使用Softmax函数的模型,防止过拟合,从而提高模型的泛化能力",
        "关系强度": 8
    },
    "18": {
        "出发节点": "Huber 损失",
        "到达节点": "Dropout",
        "关系名称": "互补关系",
        "关系解释": "Huber损失通过鲁棒性处理噪声数据,Dropout通过随机失活防止过拟合,二者在提升模型泛化能力上具有互补作用",
        "关系强度": 7
    },
    "19": {
        "出发节点": "提前停止",
        "到达节点": "交叉验证",
        "关系名称": "依赖关系",
        "关系解释": "提前停止依赖于交叉验证提供的验证集性能来决策训练终止时机",
        "关系强度": 8
    },
    "20": {
        "出发节点": "Huber 损失",
        "到达节点": "自监督学习",
        "关系名称": "组件关系",
        "关系解释": "Huber 损失作为自监督学习中损失函数的一种选择,用于提升模型对噪声数据的鲁棒性",
        "关系强度": 7
    },
    "21": {
        "出发节点": "正则化",
        "到达节点": "过拟合",
        "关系名称": "抑制关系",
        "关系解释": "正则化通过在似然函数中加入正则项来限制模型参数的复杂度,从而有效抑制模型在训练数据上的过拟合现象",
        "关系强度": 8
    },
    "22": {
        "出发节点": "Sigmoid函数",
        "到达节点": "对数似然函数",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid函数作为概率转换的核心组件,为对数似然函数提供了二分类概率的建模基础,两者共同构成逻辑回归的统计框架",
        "关系强度": 8
    },
    "23": {
        "出发节点": "批量梯度下降",
        "到达节点": "动量法",
        "关系名称": "扩展关系",
        "关系解释": "动量法在批量梯度下降的基础上引入了动量项,通过累积历史梯度信息来加速收敛并减少震荡",
        "关系强度": 8
    },
    "24": {
        "出发节点": "梯度下降",
        "到达节点": "负对数似然损失函数",
        "关系名称": "优化关系",
        "关系解释": "梯度下降通过迭代更新参数来最小化负对数似然损失函数,从而实现模型参数的最大似然估计",
        "关系强度": 9
    },
    "25": {
        "出发节点": "Sigmoid函数",
        "到达节点": "最大似然估计",
        "关系名称": "基础关系",
        "关系解释": "Sigmoid函数作为二分类概率输出的基础函数,为最大似然估计提供了概率建模的基础框架",
        "关系强度": 8
    },
    "26": {
        "出发节点": "Softmax 函数",
        "到达节点": "交叉熵损失",
        "关系名称": "基础关系",
        "关系解释": "Softmax 函数将模型输出转化为概率分布,交叉熵损失基于这些概率分布计算模型预测与真实标签之间的差异",
        "关系强度": 8
    },
    "27": {
        "出发节点": "梯度下降",
        "到达节点": "交叉熵损失",
        "关系名称": "优化关系",
        "关系解释": "梯度下降通过最小化交叉熵损失(负对数似然)来优化模型参数",
        "关系强度": 9
    },
    "28": {
        "出发节点": "交叉熵损失",
        "到达节点": "数据增强",
        "关系名称": "优化关系",
        "关系解释": "数据增强通过增加训练数据的多样性来优化交叉熵损失,帮助模型学习更泛化的特征,从而减少过拟合",
        "关系强度": 8
    },
    "29": {
        "出发节点": "提前停止",
        "到达节点": "L2 正则化",
        "关系名称": "功能互补关系",
        "关系解释": "提前停止通过终止训练防止过拟合,L2正则化通过约束参数规模防止过拟合,两者从不同角度实现正则化目标",
        "关系强度": 7
    },
    "30": {
        "出发节点": "正则化",
        "到达节点": "交叉熵损失",
        "关系名称": "优化关系",
        "关系解释": "正则化通过在交叉熵损失函数中添加正则项,限制模型参数的复杂度,从而优化模型的泛化性能并防止过拟合",
        "关系强度": 8
    },
    "31": {
        "出发节点": "梯度下降",
        "到达节点": "强化学习",
        "关系名称": "基础关系",
        "关系解释": "梯度下降是强化学习中优化策略或价值函数的基础方法,用于通过反向传播更新模型参数以最大化奖励信号",
        "关系强度": 8
    },
    "32": {
        "出发节点": "提前停止",
        "到达节点": "过拟合",
        "关系名称": "缓解关系",
        "关系解释": "提前停止通过终止训练过程来防止模型在训练数据上过度拟合,从而缓解过拟合现象",
        "关系强度": 8
    },
    "33": {
        "出发节点": "Softmax 函数",
        "到达节点": "过拟合",
        "关系名称": "缓解关系",
        "关系解释": "Softmax 函数通过将模型输出转化为概率分布,结合交叉熵损失函数,有助于模型学习更泛化的特征,从而在一定程度上缓解过拟合问题.",
        "关系强度": 7
    },
    "34": {
        "出发节点": "批量梯度下降",
        "到达节点": "自监督学习",
        "关系名称": "优化基础关系",
        "关系解释": "批量梯度下降作为优化算法为自监督学习提供了参数更新的基础方法,使模型能够通过大规模未标注数据进行表征学习",
        "关系强度": 7
    },
    "35": {
        "出发节点": "交叉熵损失",
        "到达节点": "最大似然估计",
        "关系名称": "等价关系",
        "关系解释": "交叉熵损失函数实际上是负对数似然函数,最小化交叉熵损失等价于最大化似然函数",
        "关系强度": 10
    },
    "36": {
        "出发节点": "交叉熵损失",
        "到达节点": "动量法",
        "关系名称": "优化协同关系",
        "关系解释": "动量法通过累积历史梯度信息加速交叉熵损失的优化过程,同时减少梯度震荡,提升训练稳定性",
        "关系强度": 8
    },
    "37": {
        "出发节点": "批量梯度下降",
        "到达节点": "半监督学习",
        "关系名称": "优化基础关系",
        "关系解释": "批量梯度下降为半监督学习提供了参数优化的基础方法,半监督学习可以利用批量梯度下降在有标签和无标签数据上进行联合训练",
        "关系强度": 7
    },
    "38": {
        "出发节点": "Huber 损失",
        "到达节点": "Adam",
        "关系名称": "组件关系",
        "关系解释": "Huber 损失可以作为Adam优化器的目标函数,用于提升模型在噪声数据下的鲁棒性",
        "关系强度": 7
    },
    "39": {
        "出发节点": "正则化",
        "到达节点": "数据增强",
        "关系名称": "互补关系",
        "关系解释": "正则化和数据增强都是缓解模型过拟合的方法,正则化通过约束模型参数复杂度,数据增强通过增加训练数据的多样性,二者从不同角度提升模型的泛化能力",
        "关系强度": 8
    },
    "40": {
        "出发节点": "Dropout",
        "到达节点": "强化学习",
        "关系名称": "组件关系",
        "关系解释": "Dropout作为一种正则化技术,可以集成到强化学习的神经网络模型中以防止过拟合,提升泛化能力",
        "关系强度": 7
    },
    "41": {
        "出发节点": "批量梯度下降",
        "到达节点": "L2 正则化",
        "关系名称": "优化增强关系",
        "关系解释": "L2 正则化通过向损失函数添加参数惩罚项,约束批量梯度下降的优化过程,从而提升模型泛化能力",
        "关系强度": 8
    },
    "42": {
        "出发节点": "Sigmoid函数",
        "到达节点": "交叉熵损失",
        "关系名称": "基础关系",
        "关系解释": "Sigmoid函数将线性输出转换为概率值,为交叉熵损失提供了概率输入的基础",
        "关系强度": 8
    },
    "43": {
        "出发节点": "自监督学习",
        "到达节点": "Adam",
        "关系名称": "优化关系",
        "关系解释": "Adam作为自适应优化算法常用于自监督学习的模型训练中,通过动态调整学习率提升训练效率",
        "关系强度": 8
    },
    "44": {
        "出发节点": "交叉熵损失",
        "到达节点": "伯努利分布",
        "关系名称": "基础关系",
        "关系解释": "交叉熵损失在二分类问题中的形式直接对应于伯努利分布的对数似然函数,两者在数学表达上具有一致性",
        "关系强度": 9
    },
    "45": {
        "出发节点": "Huber 损失",
        "到达节点": "生成对抗网络",
        "关系名称": "组件关系",
        "关系解释": "Huber 损失可以作为生成对抗网络中判别器的损失函数组件,用于提升模型对噪声和异常值的鲁棒性",
        "关系强度": 7
    },
    "46": {
        "出发节点": "交叉熵损失",
        "到达节点": "半监督学习",
        "关系名称": "优化基础关系",
        "关系解释": "交叉熵损失作为监督学习中的核心优化目标,为半监督学习提供了在有标签数据上的训练基础,使得半监督学习可以结合无标签数据进行扩展优化",
        "关系强度": 8
    },
    "47": {
        "出发节点": "Sigmoid函数",
        "到达节点": "伯努利分布",
        "关系名称": "参数映射关系",
        "关系解释": "Sigmoid函数将线性模型的输出映射为[0,1]区间的概率值,为伯努利分布提供成功概率参数",
        "关系强度": 8
    },
    "48": {
        "出发节点": "交叉熵损失",
        "到达节点": "Adam",
        "关系名称": "优化关系",
        "关系解释": "Adam优化算法通过自适应矩估计来优化交叉熵损失函数,调整学习率以加速收敛并提高训练稳定性",
        "关系强度": 8
    },
    "49": {
        "出发节点": "负对数似然损失函数",
        "到达节点": "对数似然函数",
        "关系名称": "对偶关系",
        "关系解释": "负对数似然损失函数通过对数似然函数取负得到,两者在优化目标上形成对偶(最大化似然等价于最小化负对数似然)",
        "关系强度": 10
    },
    "50": {
        "出发节点": "L2 正则化",
        "到达节点": "自监督学习",
        "关系名称": "组件关系",
        "关系解释": "L2 正则化可以作为自监督学习中的一种正则化手段,用于防止模型过拟合,提升泛化能力",
        "关系强度": 7
    },
    "51": {
        "出发节点": "交叉熵损失",
        "到达节点": "生成对抗网络",
        "关系名称": "优化关系",
        "关系解释": "交叉熵损失在生成对抗网络中用于衡量判别器对真实样本和生成样本的分类能力,是优化判别器的核心目标函数之一",
        "关系强度": 8
    },
    "52": {
        "出发节点": "Huber 损失",
        "到达节点": "半监督学习",
        "关系名称": "互补关系",
        "关系解释": "Huber 损失通过鲁棒性处理噪声标签,为半监督学习提供了更可靠的有监督信号基础;半监督学习则通过利用无标签数据扩展了Huber 损失的应用场景",
        "关系强度": 7
    },
    "53": {
        "出发节点": "自监督学习",
        "到达节点": "半监督学习",
        "关系名称": "基础关系",
        "关系解释": "自监督学习通过无监督方式生成伪标签,为半监督学习提供预训练特征或初始化模型,从而减少对标注数据的依赖",
        "关系强度": 8
    },
    "54": {
        "出发节点": "L2 正则化",
        "到达节点": "过拟合",
        "关系名称": "抑制关系",
        "关系解释": "L2正则化通过在似然函数中加入正则项限制参数复杂度,从而抑制模型在训练数据上的过拟合现象",
        "关系强度": 8
    },
    "55": {
        "出发节点": "批量梯度下降",
        "到达节点": "Sigmoid分类器",
        "关系名称": "优化关系",
        "关系解释": "批量梯度下降作为优化算法,用于训练Sigmoid分类器的参数,通过最小化负对数似然损失函数来优化分类器的性能",
        "关系强度": 8
    },
    "56": {
        "出发节点": "提前停止",
        "到达节点": "交叉熵损失",
        "关系名称": "优化关系",
        "关系解释": "提前停止通过监控验证集上的交叉熵损失来终止训练,防止模型过拟合,从而优化交叉熵损失在测试集上的表现",
        "关系强度": 8
    },
    "57": {
        "出发节点": "L2 正则化",
        "到达节点": "Sigmoid 分类器",
        "关系名称": "正则化关系",
        "关系解释": "L2 正则化通过向Sigmoid分类器的损失函数添加惩罚项,限制模型参数的大小,从而防止过拟合并提高泛化能力",
        "关系强度": 8
    },
    "58": {
        "出发节点": "Huber 损失",
        "到达节点": "交叉熵损失",
        "关系名称": "鲁棒性扩展关系",
        "关系解释": "Huber 损失通过结合均方误差和绝对误差的特性,为交叉熵损失提供了对噪声标签的鲁棒性扩展,使其在存在异常值时表现更稳定",
        "关系强度": 7
    },
    "59": {
        "出发节点": "L2 正则化",
        "到达节点": "数据增强",
        "关系名称": "互补关系",
        "关系解释": "L2正则化通过约束参数复杂度防止过拟合,数据增强通过增加训练数据多样性提升泛化能力,两者从不同角度共同改善模型性能",
        "关系强度": 8
    },
    "60": {
        "出发节点": "L2 正则化",
        "到达节点": "生成对抗网络",
        "关系名称": "组件关系",
        "关系解释": "L2 正则化可作为生成对抗网络中判别器的损失函数组件,通过约束参数范数提升模型泛化能力",
        "关系强度": 7
    },
    "61": {
        "出发节点": "负对数似然损失函数",
        "到达节点": "伯努利分布",
        "关系名称": "基础关系",
        "关系解释": "负对数似然损失函数在二分类问题中直接基于伯努利分布的概率模型构建,通过最大化伯努利试验的似然函数推导得出",
        "关系强度": 8
    },
    "62": {
        "出发节点": "交叉熵损失",
        "到达节点": "过拟合",
        "关系名称": "监控与缓解关系",
        "关系解释": "交叉熵损失作为训练目标函数时,其值在验证集上的持续下降而后上升可指示过拟合现象,从而触发提前停止等正则化策略",
        "关系强度": 7
    },
    "63": {
        "出发节点": "Huber 损失",
        "到达节点": "L2 正则化",
        "关系名称": "互补关系",
        "关系解释": "Huber 损失通过结合均方误差和绝对误差的优点提高对噪声的鲁棒性,而L2正则化通过限制参数大小防止过拟合.两者在提升模型泛化能力上具有互补作用,但作用机制不同.",
        "关系强度": 7
    },
    "64": {
        "出发节点": "交叉熵损失",
        "到达节点": "交叉验证",
        "关系名称": "评估优化关系",
        "关系解释": "交叉熵损失作为模型训练的优化目标,交叉验证用于评估模型在未见数据上的泛化性能,两者共同作用于模型性能的提升",
        "关系强度": 7
    },
    "65": {
        "出发节点": "交叉熵损失",
        "到达节点": "强化学习",
        "关系名称": "优化目标关系",
        "关系解释": "交叉熵损失在强化学习中常用于优化策略网络的输出分布与真实动作分布之间的差异,尤其在基于策略梯度的强化学习方法中作为核心优化目标",
        "关系强度": 8
    },
    "66": {
        "出发节点": "L2 正则化",
        "到达节点": "强化学习",
        "关系名称": "组件关系",
        "关系解释": "L2 正则化作为防止过拟合的技术,可集成到强化学习的奖励模型训练中,通过约束参数空间提升策略泛化能力",
        "关系强度": 7
    },
    "67": {
        "出发节点": "交叉熵损失",
        "到达节点": "自监督学习",
        "关系名称": "基础关系",
        "关系解释": "交叉熵损失作为自监督学习中常用的优化目标函数,为自监督学习提供了一种衡量预测分布与真实分布差异的有效方法",
        "关系强度": 8
    },
    "68": {
        "出发节点": "交叉熵损失",
        "到达节点": "Sigmoid分类器",
        "关系名称": "优化关系",
        "关系解释": "交叉熵损失作为Sigmoid分类器的标准优化目标,通过最小化负对数似然来训练分类器参数",
        "关系强度": 9
    },
    "69": {
        "出发节点": "批量梯度下降",
        "到达节点": "交叉熵损失",
        "关系名称": "优化关系",
        "关系解释": "批量梯度下降是一种优化算法,用于最小化交叉熵损失函数,通过计算整个数据集的梯度来更新模型参数",
        "关系强度": 8
    },
    "70": {
        "出发节点": "梯度下降",
        "到达节点": "最大似然估计",
        "关系名称": "优化实现关系",
        "关系解释": "梯度下降是最大化似然函数(或等价地最小化负对数似然)的数值优化方法,二者构成统计建模中参数估计与优化算法的典型范式",
        "关系强度": 8
    },
    "71": {
        "出发节点": "Dropout",
        "到达节点": "Sigmoid 分类器",
        "关系名称": "正则化关系",
        "关系解释": "Dropout作为一种正则化技术,通过在训练过程中随机丢弃部分神经元,防止Sigmoid分类器对训练数据的过拟合,从而提升其泛化能力.",
        "关系强度": 8
    },
    "72": {
        "出发节点": "Huber 损失",
        "到达节点": "梯度下降",
        "关系名称": "优化关系",
        "关系解释": "Huber 损失通过结合均方误差和绝对误差的特性,为梯度下降提供了更鲁棒的优化目标,尤其在存在噪声或异常值时能稳定梯度更新.",
        "关系强度": 8
    },
    "73": {
        "出发节点": "梯度下降",
        "到达节点": "自监督学习",
        "关系名称": "优化基础关系",
        "关系解释": "梯度下降作为核心优化算法,为自监督学习提供参数更新机制,使其能够通过无监督目标函数学习有效表示",
        "关系强度": 8
    },
    "74": {
        "出发节点": "梯度下降",
        "到达节点": "Adam",
        "关系名称": "扩展关系",
        "关系解释": "Adam通过结合动量法和自适应学习率机制扩展了梯度下降的优化能力,提升了在复杂和非平稳目标函数上的性能",
        "关系强度": 9
    },
    "75": {
        "出发节点": "负对数似然损失函数",
        "到达节点": "最大似然估计",
        "关系名称": "等价转换关系",
        "关系解释": "负对数似然损失函数是通过对最大似然估计取负对数得到的,两者在优化问题中是等价的,最小化负对数似然损失函数等价于最大化似然函数",
        "关系强度": 10
    },
    "76": {
        "出发节点": "提前停止",
        "到达节点": "数据增强",
        "关系名称": "互补关系",
        "关系解释": "提前停止和数据增强是两种不同的缓解过拟合的方法,提前停止通过监控验证集性能来终止训练,而数据增强通过增加训练数据的多样性来提高泛化能力.两者可以独立或联合使用,共同改善模型的泛化性能.",
        "关系强度": 7
    },
    "77": {
        "出发节点": "Dropout",
        "到达节点": "半监督学习",
        "关系名称": "正则化支持关系",
        "关系解释": "Dropout作为一种正则化技术,可以通过防止神经网络过拟合来提升半监督学习的泛化性能,尤其在标记数据有限时增强模型对未标记数据的利用能力",
        "关系强度": 7
    },
    "78": {
        "出发节点": "梯度下降",
        "到达节点": "L2 正则化",
        "关系名称": "扩展关系",
        "关系解释": "L2 正则化通过引入参数范数惩罚项扩展了梯度下降的优化目标,从而在参数更新过程中增加对模型复杂度的控制",
        "关系强度": 8
    },
    "79": {
        "出发节点": "Dropout",
        "到达节点": "自监督学习",
        "关系名称": "正则化支持关系",
        "关系解释": "Dropout作为一种正则化技术,通过在训练过程中随机丢弃神经元来防止过拟合,从而为自监督学习提供更鲁棒的表示学习能力",
        "关系强度": 7
    },
    "80": {
        "出发节点": "Dropout",
        "到达节点": "动量法",
        "关系名称": "互补关系",
        "关系解释": "Dropout通过随机失活神经元防止过拟合,动量法通过累积梯度方向加速收敛并减少震荡,二者在优化过程中分别从正则化和梯度更新角度提升模型性能,形成功能互补",
        "关系强度": 7
    },
    "81": {
        "出发节点": "交叉熵损失",
        "到达节点": "对数似然函数",
        "关系名称": "等价关系",
        "关系解释": "交叉熵损失与负对数似然函数在数学形式上完全等价,两者仅相差一个归一化常数.最大化对数似然函数等价于最小化交叉熵损失",
        "关系强度": 10
    },
    "82": {
        "出发节点": "负对数似然损失函数",
        "到达节点": "交叉熵损失",
        "关系名称": "归一化等价关系",
        "关系解释": "负对数似然损失函数与交叉熵损失在数学形式上完全等价,区别仅在于交叉熵损失通常包含样本数量的归一化因子(1/N),使得损失值更具可比性",
        "关系强度": 10
    },
    "83": {
        "出发节点": "L2 正则化",
        "到达节点": "交叉熵损失",
        "关系名称": "约束关系",
        "关系解释": "L2 正则化通过向交叉熵损失函数添加参数范数惩罚项,约束模型参数的复杂度以防止过拟合,从而提升泛化能力",
        "关系强度": 8
    },
    "84": {
        "出发节点": "梯度下降",
        "到达节点": "Sigmoid分类器",
        "关系名称": "优化关系",
        "关系解释": "梯度下降通过最小化负对数似然损失函数来优化Sigmoid分类器的参数,从而提高分类准确性",
        "关系强度": 8
    },
    "85": {
        "出发节点": "Huber 损失",
        "到达节点": "强化学习",
        "关系名称": "组件关系",
        "关系解释": "Huber 损失可以作为强化学习中价值函数或策略优化的损失函数,用于平衡均方误差和绝对误差,提高算法在噪声环境下的鲁棒性.",
        "关系强度": 7
    },
    "86": {
        "出发节点": "Huber 损失",
        "到达节点": "批量梯度下降",
        "关系名称": "优化适配关系",
        "关系解释": "Huber 损失通过结合均方误差和绝对误差的优点,为批量梯度下降提供了对噪声数据更鲁棒的优化目标,从而提升训练稳定性",
        "关系强度": 7
    },
    "87": {
        "出发节点": "批量梯度下降",
        "到达节点": "强化学习",
        "关系名称": "优化基础关系",
        "关系解释": "批量梯度下降作为优化算法的基础方法,为强化学习中的策略优化和值函数更新提供了参数更新的核心机制.强化学习通过结合批量梯度下降的优化能力,实现了对复杂策略或值函数的高效训练.",
        "关系强度": 7
    },
    "88": {
        "出发节点": "Dropout",
        "到达节点": "生成对抗网络",
        "关系名称": "组件关系",
        "关系解释": "Dropout可以作为生成对抗网络中判别器的正则化组件,通过随机丢弃神经元来防止过拟合,提升判别器的泛化能力",
        "关系强度": 7
    },
    "89": {
        "出发节点": "梯度下降",
        "到达节点": "半监督学习",
        "关系名称": "优化基础关系",
        "关系解释": "梯度下降作为核心优化算法,为半监督学习提供参数更新机制,使其能够有效利用未标注数据优化模型",
        "关系强度": 8
    },
    "90": {
        "出发节点": "L2 正则化",
        "到达节点": "半监督学习",
        "关系名称": "互补关系",
        "关系解释": "L2 正则化通过约束模型参数防止过拟合,而半监督学习通过利用未标注数据提升泛化能力,二者在提升模型鲁棒性上具有互补作用",
        "关系强度": 7
    },
    "91": {
        "出发节点": "Dropout",
        "到达节点": "交叉熵损失",
        "关系名称": "正则化关系",
        "关系解释": "Dropout作为一种正则化技术,通过在训练过程中随机丢弃部分神经元,减少模型对特定特征的依赖,从而间接优化交叉熵损失函数的泛化性能.",
        "关系强度": 8
    },
    "92": {
        "出发节点": "梯度下降",
        "到达节点": "Dropout",
        "关系名称": "优化协同关系",
        "关系解释": "梯度下降作为优化算法通过参数更新最小化损失函数,而Dropout作为一种正则化技术通过随机丢弃神经元防止过拟合,二者协同提升模型泛化能力.梯度下降的更新过程会受到Dropout引入的随机稀疏化影响,从而探索更鲁棒的参数空间.",
        "关系强度": 8
    },
    "93": {
        "出发节点": "Softmax 函数",
        "到达节点": "提前停止",
        "关系名称": "互补关系",
        "关系解释": "Softmax 函数用于多类别分类中输出概率分布,而提前停止是一种防止过拟合的训练策略.两者在模型训练过程中共同作用,Softmax 函数提供分类概率,提前停止优化训练过程以提高泛化能力.",
        "关系强度": 7
    },
    "94": {
        "出发节点": "L2 正则化",
        "到达节点": "动量法",
        "关系名称": "互补关系",
        "关系解释": "L2 正则化通过参数衰减防止过拟合,动量法通过梯度方向优化加速收敛,二者在模型训练中分别从参数约束和更新效率角度互补提升性能",
        "关系强度": 7
    },
    "95": {
        "出发节点": "L2 正则化",
        "到达节点": "交叉验证",
        "关系名称": "协同关系",
        "关系解释": "L2 正则化和交叉验证在缓解模型过拟合问题中具有协同作用.L2 正则化通过限制参数复杂度直接约束模型,而交叉验证通过评估模型在验证集上的表现间接指导正则化强度的选择.两者共同提升模型的泛化能力.",
        "关系强度": 8
    },
    "96": {
        "出发节点": "梯度下降",
        "到达节点": "生成对抗网络",
        "关系名称": "优化基础关系",
        "关系解释": "梯度下降作为优化算法的基础方法,为生成对抗网络中的生成器和判别器的参数更新提供了核心优化机制.生成对抗网络依赖梯度下降或其变体(如Adam)来最小化生成器和判别器的对抗损失,从而实现模型训练.",
        "关系强度": 8
    },
    "97": {
        "出发节点": "自监督学习",
        "到达节点": "强化学习",
        "关系名称": "基础关系",
        "关系解释": "自监督学习通过无监督预训练为强化学习提供表征学习基础,增强其状态表示和奖励建模能力",
        "关系强度": 8
    },
    "98": {
        "出发节点": "伯努利分布",
        "到达节点": "最大似然估计",
        "关系名称": "基础关系",
        "关系解释": "伯努利分布作为概率模型为最大似然估计提供了似然函数构建的基础框架",
        "关系强度": 8
    },
    "99": {
        "出发节点": "对数似然函数",
        "到达节点": "伯努利分布",
        "关系名称": "建模关系",
        "关系解释": "对数似然函数为伯努利分布提供概率建模框架,通过最大化似然估计分布参数",
        "关系强度": 8
    },
    "100": {
        "出发节点": "自监督学习",
        "到达节点": "动量法",
        "关系名称": "优化增强关系",
        "关系解释": "动量法通过积累历史梯度信息优化自监督学习的参数更新过程,增强模型训练的稳定性和收敛速度",
        "关系强度": 7
    },
    "101": {
        "出发节点": "对数似然函数",
        "到达节点": "最大似然估计",
        "关系名称": "优化关系",
        "关系解释": "最大似然估计通过最大化对数似然函数来估计模型参数,因为对数变换保持了原函数的单调性且便于计算",
        "关系强度": 9
    },
    "102": {
        "出发节点": "自监督学习",
        "到达节点": "Sigmoid分类器",
        "关系名称": "预训练-微调关系",
        "关系解释": "自监督学习通过无标签数据预训练特征表示,为下游任务(如Sigmoid分类器)提供初始化参数或特征提取基础",
        "关系强度": 7
    },
    "103": {
        "出发节点": "自监督学习",
        "到达节点": "生成对抗网络",
        "关系名称": "协同增强关系",
        "关系解释": "自监督学习通过无监督预训练为生成对抗网络提供更稳健的特征表示,而生成对抗网络通过对抗训练机制增强自监督学习的表征能力,二者在无监督表征学习中形成互补协同",
        "关系强度": 8
    },
    "104": {
        "出发节点": "交叉验证",
        "到达节点": "过拟合",
        "关系名称": "缓解关系",
        "关系解释": "交叉验证通过评估模型在验证集上的表现来选择合适的模型复杂度和超参数,从而防止模型在训练数据上过度拟合",
        "关系强度": 8
    },
    "105": {
        "出发节点": "半监督学习",
        "到达节点": "Sigmoid分类器",
        "关系名称": "扩展关系",
        "关系解释": "半监督学习可以扩展Sigmoid分类器的应用场景,使其能够利用未标注数据提升分类性能",
        "关系强度": 7
    },
    "106": {
        "出发节点": "强化学习",
        "到达节点": "动量法",
        "关系名称": "优化技术应用关系",
        "关系解释": "动量法作为梯度下降的优化技术被应用于强化学习的参数更新过程中,通过累积历史梯度信息加速收敛并减少震荡",
        "关系强度": 7
    },
    "107": {
        "出发节点": "强化学习",
        "到达节点": "生成对抗网络",
        "关系名称": "扩展关系",
        "关系解释": "强化学习为生成对抗网络提供了优化框架,通过奖励信号指导生成器的训练过程",
        "关系强度": 8
    },
    "108": {
        "出发节点": "半监督学习",
        "到达节点": "动量法",
        "关系名称": "优化协同关系",
        "关系解释": "动量法作为梯度下降的优化技术,可以加速半监督学习中的参数更新过程,帮助模型更高效地利用有限的标注数据和大量未标注数据",
        "关系强度": 7
    },
    "109": {
        "出发节点": "半监督学习",
        "到达节点": "生成对抗网络",
        "关系名称": "扩展关系",
        "关系解释": "生成对抗网络通过对抗训练机制扩展了半监督学习的框架,利用生成器-判别器结构实现无监督数据与有监督目标的联合优化",
        "关系强度": 8
    },
    "110": {
        "出发节点": "数据增强",
        "到达节点": "交叉验证",
        "关系名称": "协同优化关系",
        "关系解释": "数据增强通过增加训练数据的多样性提升模型泛化能力,而交叉验证通过评估模型在不同数据子集上的表现来验证其泛化性能,二者共同作用于缓解过拟合问题",
        "关系强度": 7
    },
    "111": {
        "出发节点": "Sigmoid 分类器",
        "到达节点": "生成对抗网络",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid 分类器常作为生成对抗网络中判别器的基础组件,用于输出样本属于真实分布的概率",
        "关系强度": 7
    },
    "112": {
        "出发节点": "Adam",
        "到达节点": "动量法",
        "关系名称": "扩展关系",
        "关系解释": "Adam优化器通过结合动量法和自适应学习率调整(RMSProp)扩展了动量法的功能,使其能够更高效地优化模型参数",
        "关系强度": 8
    },
    "113": {
        "出发节点": "强化学习",
        "到达节点": "Sigmoid分类器",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid分类器可以作为强化学习中奖励模型的组成部分,用于评估动作或状态的奖励概率",
        "关系强度": 7
    },
    "114": {
        "出发节点": "数据增强",
        "到达节点": "过拟合",
        "关系名称": "缓解关系",
        "关系解释": "数据增强通过增加训练数据的多样性,帮助模型学习更泛化的特征,从而缓解过拟合问题",
        "关系强度": 8
    },
    "115": {
        "出发节点": "动量法",
        "到达节点": "Sigmoid 分类器",
        "关系名称": "优化增强关系",
        "关系解释": "动量法通过引入历史梯度信息加速Sigmoid分类器的参数优化过程,减少训练震荡,提升收敛稳定性",
        "关系强度": 8
    },
    "116": {
        "出发节点": "半监督学习",
        "到达节点": "强化学习",
        "关系名称": "扩展关系",
        "关系解释": "半监督学习通过利用未标注数据扩展了强化学习的训练数据来源,从而提升其在稀疏奖励环境下的学习效率",
        "关系强度": 7
    },
    "117": {
        "出发节点": "半监督学习",
        "到达节点": "Adam",
        "关系名称": "优化支持关系",
        "关系解释": "Adam作为自适应优化算法,为半监督学习中的参数更新提供高效且稳定的优化支持,尤其在处理未标注数据时能自适应调整学习率",
        "关系强度": 7
    },
    "118": {
        "出发节点": "Adam",
        "到达节点": "生成对抗网络",
        "关系名称": "优化关系",
        "关系解释": "Adam作为自适应矩估计算法,常用于优化生成对抗网络中生成器和判别器的参数,通过动态调整学习率提升训练稳定性",
        "关系强度": 8
    },
    "119": {
        "出发节点": "动量法",
        "到达节点": "生成对抗网络",
        "关系名称": "优化组件关系",
        "关系解释": "动量法作为优化算法的一种变体,可用于加速生成对抗网络中生成器和判别器的梯度下降过程,帮助稳定训练并缓解模式崩溃问题.",
        "关系强度": 7
    },
    "120": {
        "出发节点": "Adam",
        "到达节点": "强化学习",
        "关系名称": "优化关系",
        "关系解释": "Adam作为一种自适应矩估计算法,为强化学习中的策略优化或价值函数逼近提供了高效的参数更新方法,通过动态调整学习率提升训练稳定性和收敛速度",
        "关系强度": 8
    },
    "121": {
        "出发节点": "Adam",
        "到达节点": "Sigmoid分类器",
        "关系名称": "优化关系",
        "关系解释": "Adam优化算法用于优化Sigmoid分类器的参数,通过自适应调整学习率来加速收敛并提高分类性能",
        "关系强度": 8
    },
    "122": {
        "出发节点": "梯度下降",
        "到达节点": "Huber损失",
        "关系名称": "优化关系",
        "关系解释": "梯度下降作为优化算法可用于最小化Huber损失函数,Huber损失通过结合均方误差和绝对误差的特性为梯度下降提供更鲁棒的优化目标",
        "关系强度": 8
    },
    "123": {
        "出发节点": "Sigmoid 分类器",
        "到达节点": "批量梯度下降",
        "关系名称": "优化关系",
        "关系解释": "批量梯度下降通过最小化Sigmoid分类器的负对数似然损失函数来优化其参数",
        "关系强度": 8
    },
    "124": {
        "出发节点": "Dropout",
        "到达节点": "Huber损失",
        "关系名称": "互补关系",
        "关系解释": "Dropout通过随机丢弃神经元防止过拟合,而Huber损失通过鲁棒性设计减少异常值影响,二者在提升模型泛化能力上形成互补",
        "关系强度": 7
    },
    "125": {
        "出发节点": "生成对抗网络",
        "到达节点": "Huber损失",
        "关系名称": "组件关系",
        "关系解释": "Huber损失可以作为生成对抗网络中判别器的损失函数组件,用于提升模型对噪声数据的鲁棒性",
        "关系强度": 7
    },
    "126": {
        "出发节点": "过拟合",
        "到达节点": "Softmax函数",
        "关系名称": "缓解关系",
        "关系解释": "Softmax函数在多类别分类中通过概率归一化输出,结合交叉熵损失函数和正则化方法,可以间接缓解模型过拟合问题",
        "关系强度": 7
    },
    "127": {
        "出发节点": "Adam",
        "到达节点": "Dropout",
        "关系名称": "互补关系",
        "关系解释": "Adam优化器通过自适应学习率调整加速模型训练,而Dropout通过随机失活神经元防止过拟合,二者在训练过程中分别从优化效率和模型泛化角度互补提升性能",
        "关系强度": 8
    },
    "128": {
        "出发节点": "Adam",
        "到达节点": "Huber损失",
        "关系名称": "优化适配关系",
        "关系解释": "Adam作为自适应优化算法,可以有效地优化采用Huber损失的模型,因为Huber损失对噪声数据具有鲁棒性,而Adam的自适应学习率调整机制能更好地处理这种非标准梯度分布",
        "关系强度": 7
    },
    "129": {
        "出发节点": "动量法",
        "到达节点": "Huber损失",
        "关系名称": "互补关系",
        "关系解释": "动量法通过累积梯度方向加速优化过程,而Huber损失通过鲁棒性设计减少异常值对梯度的影响,二者在优化过程中分别从梯度方向和梯度幅度两个维度提升模型稳定性",
        "关系强度": 7
    },
    "130": {
        "出发节点": "Sigmoid分类器",
        "到达节点": "L2正则化",
        "关系名称": "正则化增强关系",
        "关系解释": "L2正则化通过向Sigmoid分类器的负对数似然损失函数添加惩罚项,约束模型参数的增长,从而提升其泛化能力并防止过拟合",
        "关系强度": 8
    },
    "131": {
        "出发节点": "L2 正则化",
        "到达节点": "Dropout",
        "关系名称": "互补关系",
        "关系解释": "L2 正则化和Dropout都是用于防止模型过拟合的技术,但作用机制不同.L2 正则化通过惩罚大权重来限制模型复杂度,而Dropout通过随机丢弃神经元来减少对特定神经元的依赖.两者可以结合使用,从不同角度提升模型泛化能力.",
        "关系强度": 8
    },
    "132": {
        "出发节点": "Sigmoid 分类器",
        "到达节点": "强化学习",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid分类器可作为强化学习中奖励模型的核心组件,用于将状态或动作映射为概率形式的奖励信号",
        "关系强度": 7
    },
    "133": {
        "出发节点": "Sigmoid分类器",
        "到达节点": "动量法",
        "关系名称": "优化扩展关系",
        "关系解释": "动量法通过引入历史梯度信息优化Sigmoid分类器的参数更新过程,加速收敛并减少震荡",
        "关系强度": 7
    },
    "134": {
        "出发节点": "Sigmoid 分类器",
        "到达节点": "自监督学习",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid分类器可以作为自监督学习框架中的下游任务评估组件,用于将学习到的表征转化为概率输出",
        "关系强度": 7
    },
    "135": {
        "出发节点": "Sigmoid 分类器",
        "到达节点": "Adam",
        "关系名称": "优化关系",
        "关系解释": "Adam优化算法可用于优化Sigmoid分类器的参数,通过自适应调整学习率来加速收敛并提高训练稳定性",
        "关系强度": 8
    },
    "136": {
        "出发节点": "Sigmoid 分类器",
        "到达节点": "半监督学习",
        "关系名称": "组件关系",
        "关系解释": "Sigmoid 分类器可以作为半监督学习中的基础分类器组件,用于处理标注数据部分的概率输出,并通过伪标签或置信度传播机制扩展至未标注数据",
        "关系强度": 7
    }
};  // 使用特殊占位符

                console.log("实体数据:", entities);
                console.log("关系数据:", relations);

                // 转换 entities_dict 为 nodes，使用模型/算法名称作为 id
                const nodes = Object.values(entities).map((entity, index) => ({
                    id: entity["模型/算法名称"],
                    name: entity["模型/算法名称"],
                    definition: entity["定义"],
                    applicable_scenarios: entity["适用场景"],
                    core_idea: entity["核心思想"],
                    advantages: entity["优势特点"],
                    limitations: entity["局限性"] ,
                    application_fields: entity["应用领域"],
                    key_components: entity["关键组件"],
                    related_models: entity["相关模型/算法"],
                    color: d3.schemeCategory10[index % 10]
                }));
                console.log("生成的节点数据:", nodes);


                // 根据实体名称查找节点 ID
                function findNodeId(name) {
                    const node = nodes.find(n => n.id === name);
                    if (!node) {
                        console.warn(`未找到名称为 "${name}" 的节点。`);
                    }
                    return node ? node.id : null;
                }

                // 转换 relations_dict 为 links
                const links = Object.values(relations).map(relation => ({
                    source: findNodeId(relation["出发节点"]),
                    target: findNodeId(relation["到达节点"]),
                    relationship_type: relation["关系类型"],
                    relationship_explanation: relation["关系说明"],
                    relationship_strength: parseInt(relation["相关度"])
                })).filter(link => {
                    const valid = link.source !== null && link.target !== null;
                    if (!valid) {
                        console.warn("过滤掉无效的链接:", link);
                    }
                    return valid;
                });
                console.log("生成的链接数据:", links);

                // 识别连通性和聚类
                function findClusters(nodes, links) {
                    const clusters = [];
                    const visited = new Set();

                    function dfs(node, cluster) {
                        if (!node) {
                            console.error('未找到节点:', node);
                            return;
                        }
                        visited.add(node.id);
                        cluster.push(node);
                        links.forEach(link => {
                            if (link.source === node.id && !visited.has(link.target)) {
                                const targetNode = nodes.find(n => n.id === link.target);
                                if (targetNode) {
                                    dfs(targetNode, cluster);
                                }
                            } else if (link.target === node.id && !visited.has(link.source)) {
                                const sourceNode = nodes.find(n => n.id === link.source);
                                if (sourceNode) {
                                    dfs(sourceNode, cluster);
                                }
                            }
                        });
                    }

                    nodes.forEach(node => {
                        if (!visited.has(node.id)) {
                            const cluster = [];
                            console.log(`开始处理节点 "${node.id}" 的聚类。`);
                            dfs(node, cluster);
                            clusters.push(cluster);
                            console.log(`完成一个聚类，包含节点:`, cluster.map(n => n.id));
                        }
                    });

                    console.log("识别出的所有聚类:", clusters);
                    return clusters;
                }

                // 识别团簇并为其上色
                function colorClusters(nodes, links) {
                    console.log("开始识别和上色聚类。");
                    const clusters = findClusters(nodes, links);
                    const color = d3.scaleOrdinal(d3.schemeCategory10);

                    clusters.forEach((cluster, clusterIndex) => {
                        cluster.forEach(node => {
                            node.cluster = clusterIndex;
                            node.color = color(clusterIndex);
                        });
                    });
                    console.log("聚类及颜色设置完成。");
                }

                // 识别团簇并上色
                colorClusters(nodes, links);

                // 创建深拷贝的原始数据
                let originalNodes = JSON.parse(JSON.stringify(nodes));
                let originalLinks = JSON.parse(JSON.stringify(links));
                console.log("创建原始数据的深拷贝。");

                let selectedNodeId = null; // 记录选中的节点 ID

                // 创建 SVG 和力导向仿真
                const width = window.innerWidth; 
                const height = window.innerHeight;
                console.log(`创建 SVG，宽度: ${width}, 高度: ${height}`);
                const svgSelection = d3.select("#canvas")
                    .attr("width", width)
                    .attr("height", height);
                    
                const svgGroup = svgSelection.append("g"); // 创建一个组元素用于缩放
                console.log("添加组元素到 SVG。");

                svgSelection.call(d3.zoom() // 添加缩放和拖拽功能
                    .on("zoom", (event) => {
                        svgGroup.attr("transform", event.transform);
                        console.log("缩放事件触发，transform:", event.transform);
                    }))
                    .on("dblclick.zoom", null); // 禁用双击缩放
                console.log("添加缩放和拖拽功能。");

                const simulation = d3.forceSimulation(nodes)
                    .force("link", d3.forceLink(links).id(d => d.id).distance(150)) // 调整距离
                    .force("charge", d3.forceManyBody().strength(-30)) // 调整力的强度
                    .force("center", d3.forceCenter(width / 2, height / 2));
                console.log("初始化力导向仿真。");

                // 绘制边
                let link = svgGroup.append("g")
                    .attr("class", "links")
                    .selectAll("line")
                    .data(links)
                    .enter().append("line")
                    .attr("class", "edge")
                    .attr("stroke-width", d => Math.sqrt(d.relationship_strength));
                console.log("绘制边完成。");

                // 绘制节点
                let node = svgGroup.append("g")
                    .attr("class", "nodes")
                    .selectAll("g")
                    .data(nodes)
                    .enter().append("g")
                    .attr("class", "node")
                    .call(d3.drag()
                        .on("start", dragstarted)
                        .on("drag", dragged)
                        .on("end", dragended));
                console.log("绘制节点组完成。");

                node.append("circle")
                    .attr("r", 10)
                    .attr("fill", d => d.color);
                console.log("添加节点圆形元素完成。");

                node.append("text")
                    .attr("dy", ".35em")
                    .text(d => d.name);
                console.log("添加节点文本元素完成。");

                // 添加事件监听器
                node.on("click", nodeClicked);
                console.log("添加节点点击事件监听器完成。");

                // 节点点击事件处理函数
                function nodeClicked(event, d) {
                    console.log(`节点 "${d.id}" 被点击。`);
                    selectedNodeId = d.id; // 记录选中的节点 ID
                    const neighborNodeIds = new Set();
                    links.forEach(link => {
                        if (link.source.id === d.id) { // 修正为link.source.id
                            neighborNodeIds.add(link.target.id);
                        } else if (link.target.id === d.id) { // 修正为link.target.id
                            neighborNodeIds.add(link.source.id);
                        }
                    });
                    neighborNodeIds.add(d.id);
                    console.log(`节点 "${d.id}" 的邻居节点 IDs:`, Array.from(neighborNodeIds));

                    const filteredNodes = nodes.filter(node => neighborNodeIds.has(node.id));
                    const filteredLinks = links.filter(link => neighborNodeIds.has(link.source.id) && neighborNodeIds.has(link.target.id));
                    console.log("过滤后的节点:", filteredNodes);
                    console.log("过滤后的链接:", filteredLinks);

                    updateGraph(filteredNodes, filteredLinks);
                    console.log("更新图形完成。");

                    // 在右边栏显示选中节点的信息
                    const entity = entities[d.id];
                    if (!entity) {
                        console.warn(`未找到节点 "${d.id}" 对应的实体信息。`);
                    }
                    console.log(`显示节点 "${d.id}" 的详细信息。`);
                    d3.select("#sidebar-title").text(entity ? entity["模型/算法名称"] : "未知");
                    d3.select("#definition").text(entity ? entity["定义"] : "无");
                    d3.select("#applicable_scenarios").text(entity ? entity["适用场景"] : "无");
                    d3.select("#core_idea").text(entity ? entity["核心思想"] : "无");

                    d3.select("#advantages").text(entity ? entity["优势特点"] : "未知");
                    d3.select("#limitations").text(entity ? entity["局限性"] : "无");
                    d3.select("#application_fields").text(entity ? entity["应用领域"] : "无");
                    d3.select("#key_components").text(entity ? entity["关键组件"] : "无");
                    // 处理列表类型的数据


                    // 处理相关模型/算法
                    const relatedModels = entity ? entity["相关模型/算法"] : [];
                    updateList("#related_models", relatedModels);

                    // 处理与其他实体的关系
                    const relatedRelations = entity ? Object.values(relations).filter(relation => 
                        relation["出发节点"] === entity["模型/算法名称"] || 
                        relation["到达节点"] === entity["模型/算法名称"]
                    ) : [];
                    const relationsList = d3.select("#relations");
                    relationsList.html("");
                    relatedRelations.forEach(relation => {
                        relationsList.append("li").html(`
                            <strong>${relation["关系类型"]}:</strong> ${relation["关系说明"]} (相关度: ${relation["相关度"]})
                        `);
                    });
                    console.log("显示相关关系。");

                    d3.select("#sidebar").classed("show", true);
                    console.log("侧边栏已显示。");
                }

                // 检查节点是否有未展示的邻居
                function hasMissingNeighbors(node, newNodes) {
                    const neighborNodeIds = new Set();
                    links.forEach(link => {
                        if (link.source.id === node.id) { // 修正为link.source.id
                            neighborNodeIds.add(link.target.id);
                        } else if (link.target.id === node.id) { // 修正为link.target.id
                            neighborNodeIds.add(link.source.id);
                        }
                    });
                    const hasMissing = Array.from(neighborNodeIds).some(id => !newNodes.some(n => n.id === id));
                    if (hasMissing) {
                        console.log(`节点 "${node.id}" 有未展示的邻居节点。`);
                    }
                    return hasMissing;
                }

                // 更新图形函数
                function updateGraph(newNodes, newLinks) {
                    console.log("开始更新图形。");
                    simulation.nodes(newNodes);
                    simulation.force("link").links(newLinks);
                    console.log("更新仿真的节点和链接。");

                    // 更新链接
                    link = link.data(newLinks, d => `${d.source.id}-${d.target.id}`);
                    console.log("绑定新的链接数据。");
                    link.exit().remove();
                    console.log("移除退出的链接。");
                    link = link.enter().append("line")
                        .attr("class", "edge")
                        .attr("stroke-width", d => Math.sqrt(d.relationship_strength))
                        .merge(link);
                    console.log("添加新链接完成。");

                    // 更新节点
                    node = node.data(newNodes, d => d.id);
                    console.log("绑定新的节点数据。");
                    node.exit().remove();
                    console.log("移除退出的节点。");

                    const nodeEnter = node.enter().append("g")
                        .attr("class", "node")
                        .call(d3.drag()
                            .on("start", dragstarted)
                            .on("drag", dragged)
                            .on("end", dragended));
                    console.log("添加新节点组完成。");

                    nodeEnter.append("circle")
                        .attr("r", 10)
                        .attr("fill", d => d.color);
                    console.log("添加新节点圆形元素完成。");

                    nodeEnter.append("text")
                        .attr("dy", ".35em")
                        .text(d => d.name);
                    console.log("添加新节点文本元素完成。");

                    nodeEnter.on("click", nodeClicked);
                    console.log("添加新节点点击事件监听器完成。");

                    node = nodeEnter.merge(node);
                    console.log("合并新节点和现有节点。");

                    // 更新所有节点的文本
                    node.select("text")
                        .text(d => d.name);
                    console.log("更新所有节点的文本。");

                    // 设置选中节点和有未展示邻居节点的样式
                    node.select("circle")
                        .attr("r", d => {
                            if (d.id === selectedNodeId) {
                                return 12.5;
                            } else if (hasMissingNeighbors(d, newNodes)) {
                                return 10 * 1.2;
                            } else {
                                return 10;
                            }
                        })
                        .attr("class", d => {
                            if (d.id === selectedNodeId) {
                                return "selected-node";
                            } else if (hasMissingNeighbors(d, newNodes)) {
                                return "neighbor-missing-node";
                            } else {
                                return "";
                            }
                        })
                        .attr("fill", d => {
                            if (d.id === selectedNodeId) {
                                return "gold";
                            } else if (hasMissingNeighbors(d, newNodes)) {
                                return "green";
                            } else {
                                return d.color;
                            }
                        });
                    console.log("更新节点样式完成。");

                    simulation.alpha(1).restart();
                    console.log("重新启动仿真。");
                }

                // 力导向仿真事件
                simulation.on("tick", () => {
                    link.attr("x1", d => d.source.x)
                        .attr("y1", d => d.source.y)
                        .attr("x2", d => d.target.x)
                        .attr("y2", d => d.target.y);

                    node.attr("transform", d => `translate(${d.x},${d.y})`);
                    // 可以在这里添加日志来跟踪每个 tick 的位置，但可能会导致控制台过多日志
                    // console.log("仿真tick事件触发。");
                });
                console.log("设置力导向仿真的tick事件。");

                // 画布拖拽行为函数
                function dragstarted(event, d) {
                    if (!event.active) simulation.alphaTarget(0.3).restart();
                    d.fx = d.x;
                    d.fy = d.y;
                    console.log(`拖拽开始，节点: ${d.id}`);
                }

                function dragged(event, d) {
                    d.fx = event.x;
                    d.fy = event.y;
                    console.log(`正在拖拽节点: ${d.id} 到位置 (${event.x}, ${event.y})`);
                }

                function dragended(event, d) {
                    if (!event.active) simulation.alphaTarget(0);
                    d.fx = null;
                    d.fy = null;
                    console.log(`拖拽结束，节点: ${d.id}`);
                }

                // 重置视图
                d3.select("#reset").on("click", () => {
                    console.log("点击重置按钮，重新加载页面。");
                    location.reload(); // 重新加载页面
                });
                console.log("添加重置视图按钮事件监听器。");

                // 切换右边栏显示/隐藏
                d3.select("#toggleSidebar").on("click", () => {
                    const sidebar = d3.select("#sidebar");
                    const isShown = sidebar.classed("show");
                    sidebar.classed("show", !isShown);
                    d3.select("#toggleSidebar").text(isShown ? "显示信息" : "隐藏信息");
                    console.log(`切换侧边栏显示状态: ${isShown ? "隐藏" : "显示"}`);
                });
                console.log("添加切换侧边栏显示/隐藏按钮事件监听器。");

                // 初始化缩放行为
                const zoom = d3.zoom().on("zoom", zoomed);
                console.log("初始化缩放行为。");

                // 拖拽和缩放的行为
                function zoomed(event) {
                    svgGroup.attr("transform", event.transform);
                    console.log("缩放更新: ", event.transform);
                }

                // 应用缩放行为到整个 SVG
                d3.select("svg").call(zoom);
                console.log("应用缩放行为到 SVG。");

                // 确保 SVG 大小足够大以容纳所有节点
                function resize() {
                    const bounds = d3.select("svg").node().getBoundingClientRect();
                    const maxX = d3.max(nodes, d => d.x) + 20;
                    const maxY = d3.max(nodes, d => d.y) + 20;
                    const newWidth = Math.max(bounds.width, maxX);
                    const newHeight = Math.max(bounds.height, maxY);
                    d3.select("svg").attr("width", newWidth).attr("height", newHeight);
                    console.log(`调整SVG大小为: 宽度=${newWidth}, 高度=${newHeight}`);
                }

                // 调用 resize 函数
                resize();
                console.log("调用 resize 函数完成。");
            } catch (error) {
                console.error("在主函数执行过程中发生错误:", error);
            }
        }


        // 开始尝试加载 D3.js，并在加载完成后执行主函数
        loadD3(d3Urls, main);
    </script>
</body>
</html>
