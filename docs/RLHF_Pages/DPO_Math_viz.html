<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>节点和边信息</title>
    <style>
        body, html {
            width: 100%;
            height: 100%;
            margin: 0;
            padding: 0;
            overflow: hidden;
            display: flex;
        }
        svg {
            flex: 1;
            display: block;
        }
        .node circle {
            stroke: #fff;
            stroke-width: 1.5px;
        }
        .edge {
            stroke: #999;
            stroke-opacity: 0.6;
        }
        text {
            font-size: 11px;
            pointer-events: none;
        }
        #sidebar {
            width: 300px;
            height: 100%;
            background-color: #f0f0f0;
            border-left: 1px solid #ddd;
            padding: 20px;
            box-shadow: -2px 0 5px rgba(0,0,0,0.1);
            overflow-y: auto;
            transition: transform 0.3s ease;
            position: fixed;
            top: 0;
            right: 0;
            transform: translateX(100%);
        }
        #sidebar.show {
            transform: translateX(0);
        }
        #toggleSidebar, #reset {
            position: absolute;
            top: 10px;
            background-color: #007bff;
            color: white;
            border: none;
            padding: 10px;
            cursor: pointer;
            z-index: 100;
            border-radius: 4px;
        }
        #toggleSidebar {
            right: 10px;
        }
        #reset {
            right: 130px;
        }
        .selected-node {
            fill: gold;
            stroke: #fff;
            stroke-width: 1.5px;
        }
        .neighbor-missing-node {
            fill: green;
        }
    </style>
</head>
<body>
    <svg id="canvas"></svg>
    <div id="sidebar">
        <h2 id="sidebar-title">节点信息</h2>
        <div id="sidebar-content">
            <p>点击一个节点查看详细信息</p>
            <h3>定义</h3>
            <p id="definition"></p>
            <h3>适用场景</h3>
            <p id="applicable_scenarios"></p>
            <h3>核心思想</h3>
            <p id="core_idea"></p>
            <h3>优势特点</h3>
            <ul id="advantages"></ul>
            <h3>局限性</h3>
            <ul id="limitations"></ul>
            <h3>应用领域</h3>
            <p id="application_fields"></p>
            <h3>关键组件</h3>
            <ul id="key_components"></ul>
            <h3>相关模型/算法</h3>
            <ul id="related_models"></ul>
            <h3>关系</h3>
            <ul id="relations"></ul>
        </div>
    </div>
    <button id="toggleSidebar">隐藏信息</button>
    <button id="reset">重置视图</button>

    <!-- 加载 D3.js 和主脚本 -->
    <script>
        /**
         * 尝试加载 D3.js 的函数
         * @param {Array} urls - D3.js 的多个 CDN 源
         * @param {Function} callback - D3.js 成功加载后的回调函数
         */
        function loadD3(urls, callback) {
            console.log("尝试加载 D3.js...");
            // 如果 D3.js 已经加载，直接执行回调
            if (typeof d3 !== 'undefined') {
                console.log("D3.js 已经加载。");
                callback();
                return;
            }

            // 如果没有更多的 URL 进行尝试，显示错误信息
            if (urls.length === 0) {
                console.error('所有CDN源都加载失败！');
                alert('无法加载D3.js，请检查网络连接或联系管理员。');
                return;
            }

            const currentUrl = urls[0];
            console.log(`尝试从 ${currentUrl} 加载 D3.js`);
            const script = document.createElement('script');
            script.src = currentUrl;
            script.onload = function() {
                if (typeof d3 !== 'undefined') {
                    console.log('D3.js 成功从:', currentUrl, '加载！');
                    callback();
                } else {
                    console.warn('D3.js 未定义，从', currentUrl, '加载失败，尝试下一个CDN源...');
                    loadD3(urls.slice(1), callback);
                }
            };
            script.onerror = function() {
                console.warn('加载失败:', currentUrl, '，尝试下一个CDN源...');
                loadD3(urls.slice(1), callback);
            };
            document.head.appendChild(script);
        }

        // 定义多个CDN源，并将本地文件作为最后的备选
        const d3Urls = [
            'https://cdn.jsdelivr.net/npm/d3@7/dist/d3.min.js',
            'https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js',
            'https://unpkg.com/d3@7/dist/d3.min.js',
            'https://d3js.org/d3.v7.min.js',
            'd3.min.js' // 本地文件
        ];

        /**
         * 在 D3.js 加载完成后执行的主函数
         */
         function main() {
            console.log("D3.js 已加载，执行主函数。");
            try {
                // 嵌入实体和关系数据，确保使用 tojson 和 safe 过滤器
                const entities = {
    "条件概率分布": {
        "模型/算法名称": "条件概率分布",
        "定义": "给定输入x时输出y的概率分布",
        "适用场景": "偏好学习、强化学习中的策略建模",
        "核心思想": "通过概率分布描述不同输出的可能性",
        "优势特点": "自然处理不确定性,可解释性强",
        "局限性": "高维空间计算复杂度高",
        "应用领域": "语言模型、推荐系统、决策系统",
        "关键组件": "策略模型,参考模型,温度参数"
    },
    "参考模型": {
        "模型/算法名称": "参考模型",
        "定义": "作为基准的条件概率分布模型",
        "适用场景": "强化学习中的策略优化",
        "核心思想": "提供稳定的概率分布作为优化基准",
        "优势特点": "防止策略过度偏离,保持生成多样性",
        "局限性": "依赖初始模型质量,可能限制创新性",
        "应用领域": "语言模型对齐,偏好学习",
        "关键组件": "条件概率分布,对数概率计算"
    },
    "偏好学习": {
        "模型/算法名称": "偏好学习",
        "定义": "通过偏好数据学习模型参数的机器学习方法",
        "适用场景": "处理配对比较数据中的偏好关系",
        "核心思想": "将偏好学习转化为二分类概率最大化问题",
        "优势特点": "隐式控制KL散度,避免显式奖励建模",
        "局限性": "依赖参考模型质量,温度参数敏感",
        "应用领域": "语言模型对齐,推荐系统优化",
        "关键组件": "策略模型,参考模型,温度参数"
    },
    "策略模型": {
        "模型/算法名称": "策略模型",
        "定义": "基于条件概率分布的学习模型,用于生成响应",
        "适用场景": "偏好学习与强化学习任务",
        "核心思想": "通过优化偏好概率来调整策略",
        "优势特点": "隐式控制KL散度,避免显式奖励建模",
        "局限性": "依赖参考模型质量,温度参数敏感",
        "应用领域": "自然语言处理,对话系统",
        "关键组件": "条件概率分布,奖励函数,温度参数"
    },
    "DPO": {
        "模型/算法名称": "DPO",
        "定义": "通过偏好学习优化策略模型的算法",
        "适用场景": "语言模型微调与偏好对齐",
        "核心思想": "将偏好学习转化为二分类问题",
        "优势特点": "隐式控制KL散度,避免显式奖励建模",
        "局限性": "依赖参考模型质量,温度参数敏感",
        "应用领域": "对话系统,文本生成",
        "关键组件": "参考模型,策略模型,温度参数"
    },
    "Bradley-Terry模型": {
        "模型/算法名称": "Bradley-Terry模型",
        "定义": "用于描述配对比较中偏好关系的概率模型",
        "适用场景": "需要比较选项间相对偏好的场景",
        "核心思想": "通过潜在能力值计算选项间比较的概率",
        "优势特点": "简单直观,数学性质良好",
        "局限性": "假设严格传递性,无法处理循环偏好",
        "应用领域": "心理学,经济学,推荐系统",
        "关键组件": "潜在能力值,比较概率公式"
    },
    "温度参数": {
        "模型/算法名称": "温度参数",
        "定义": "控制策略模型保守程度的超参数",
        "适用场景": "偏好学习中的策略优化",
        "核心思想": "调节模型对新策略的探索程度",
        "优势特点": "平衡探索与利用,控制模型偏离参考策略的程度",
        "局限性": "需要手动调整,影响训练稳定性",
        "应用领域": "直接偏好优化算法",
        "关键组件": "奖励模型,参考模型"
    },
    "KL散度的约束强度": {
        "模型/算法名称": "KL散度的约束强度",
        "定义": "衡量策略模型与参考模型概率分布差异的度量",
        "适用场景": "强化学习中的策略优化过程",
        "核心思想": "通过KL散度控制策略模型偏离参考模型的程度",
        "优势特点": "保持策略稳定性,防止过度偏离参考模型",
        "局限性": "需要合理设置温度参数β",
        "应用领域": "直接偏好优化算法",
        "关键组件": "温度参数β,参考模型,策略模型"
    },
    "奖励模型": {
        "模型/算法名称": "奖励模型",
        "定义": "用于评估生成回复质量的概率模型",
        "适用场景": "偏好学习和强化学习中的策略优化",
        "核心思想": "通过比较策略模型和参考模型的对数概率差定义奖励",
        "优势特点": "无需显式训练,直接利用策略模型参数",
        "局限性": "依赖参考模型的质量,可能过拟合偏好数据",
        "应用领域": "自然语言处理中的对话系统优化",
        "关键组件": "策略模型,参考模型,温度参数"
    },
    "硬分类损失": {
        "模型/算法名称": "硬分类损失",
        "定义": "当温度参数趋近于零时DPO损失趋向的极限形式",
        "适用场景": "偏好学习中的二分类问题",
        "核心思想": "直接最大化偏好选择的确定性",
        "优势特点": "简化优化目标,计算效率高",
        "局限性": "缺乏概率解释性,可能过拟合",
        "应用领域": "语言模型偏好优化",
        "关键组件": "温度参数,奖励模型,偏好概率"
    },
    "二分类问题": {
        "模型/算法名称": "二分类问题",
        "定义": "通过偏好数据区分获胜回复和失败回复的分类任务",
        "适用场景": "处理成对偏好数据的排序学习",
        "核心思想": "利用Bradley-Terry模型建模偏好概率",
        "优势特点": "概率解释性强,数学形式简洁",
        "局限性": "假设二元独立性,忽略全局排序",
        "应用领域": "推荐系统,对话系统优化",
        "关键组件": "偏好概率函数,损失函数,梯度计算"
    },
    "KL散度": {
        "模型/算法名称": "KL散度",
        "定义": "衡量两个概率分布差异的非对称度量",
        "适用场景": "概率模型比较、信息理论分析",
        "核心思想": "通过对数概率比计算分布间信息差异",
        "优势特点": "非负性,凸性,加性,连续性",
        "局限性": "不对称性,对零概率事件敏感",
        "应用领域": "机器学习,统计推断,信息检索",
        "关键组件": "概率分布P和Q,对数比计算"
    },
    "优化过程": {
        "模型/算法名称": "优化过程",
        "定义": "通过梯度下降优化策略模型参数的过程",
        "适用场景": "偏好学习与强化学习中的策略优化",
        "核心思想": "将偏好学习转化为二分类问题并最大化偏好概率",
        "优势特点": "隐式控制KL散度,避免显式奖励建模",
        "局限性": "依赖参考模型质量,温度参数敏感",
        "应用领域": "语言模型对齐,推荐系统优化",
        "关键组件": "梯度计算,KL散度约束,温度参数"
    },
    "梯度性质": {
        "模型/算法名称": "梯度性质",
        "定义": "分析DPO损失函数对模型参数的梯度变化特性",
        "适用场景": "深度偏好优化算法的参数更新过程",
        "核心思想": "通过梯度方向控制策略模型与参考模型的偏离程度",
        "优势特点": "明确显示偏好概率与梯度权重的数学关系",
        "局限性": "依赖Bradley-Terry模型的概率假设",
        "应用领域": "强化学习中的直接偏好优化方法",
        "关键组件": "温度参数,奖励差,对数概率比"
    }
};  // 使用特殊占位符
                const relations = {
    "0": {
        "出发节点": "温度参数",
        "到达节点": "Bradley-Terry模型",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制概率分布的尖锐程度来调节Bradley-Terry模型中的偏好概率计算,影响模型对偏好差异的敏感度",
        "关系强度": 8
    },
    "1": {
        "出发节点": "温度参数",
        "到达节点": "DPO",
        "关系名称": "调节关系",
        "关系解释": "温度参数β在DPO中控制策略模型与参考模型之间的KL散度约束强度,较小的β允许更大的策略偏离,较大的β限制策略变化",
        "关系强度": 8
    },
    "2": {
        "出发节点": "温度参数",
        "到达节点": "硬分类损失",
        "关系名称": "极限关系",
        "关系解释": "当温度参数趋近于0时,DPO损失函数趋近于硬分类损失",
        "关系强度": 8
    },
    "3": {
        "出发节点": "温度参数",
        "到达节点": "策略模型",
        "关系名称": "调节关系",
        "关系解释": "温度参数控制策略模型与参考模型之间的KL散度约束强度,较小的温度参数允许策略模型更大程度地偏离参考模型,而较大的温度参数限制策略模型的偏离程度",
        "关系强度": 8
    },
    "4": {
        "出发节点": "参考模型",
        "到达节点": "KL散度",
        "关系名称": "约束关系",
        "关系解释": "参考模型通过KL散度约束策略模型的优化过程,防止其偏离参考分布过远",
        "关系强度": 8
    },
    "5": {
        "出发节点": "温度参数",
        "到达节点": "参考模型",
        "关系名称": "正则化关系",
        "关系解释": "温度参数通过控制策略模型与参考模型之间的KL散度,调节策略模型偏离参考模型的程度,起到正则化作用",
        "关系强度": 8
    },
    "6": {
        "出发节点": "参考模型",
        "到达节点": "硬分类损失",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,DPO损失函数退化为硬分类损失,此时参考模型与硬分类损失形成极限关系",
        "关系强度": 8
    },
    "7": {
        "出发节点": "参考模型",
        "到达节点": "条件概率分布",
        "关系名称": "正则化约束关系",
        "关系解释": "参考模型通过KL散度对策略模型的条件概率分布施加正则化约束,防止其过度偏离参考分布",
        "关系强度": 8
    },
    "8": {
        "出发节点": "参考模型",
        "到达节点": "策略模型",
        "关系名称": "正则化约束关系",
        "关系解释": "策略模型通过KL散度与参考模型保持概率分布相似性,温度参数β控制约束强度",
        "关系强度": 8
    },
    "9": {
        "出发节点": "温度参数",
        "到达节点": "奖励模型",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过缩放奖励模型的输出值来控制策略模型的保守程度,较小的温度参数允许更大的策略偏离参考模型,而较大的温度参数限制策略偏离",
        "关系强度": 8
    },
    "10": {
        "出发节点": "奖励模型",
        "到达节点": "直接偏好优化",
        "关系名称": "重构关系",
        "关系解释": "直接偏好优化通过将奖励模型重新参数化为策略模型与参考模型的对数概率差,从而绕过了显式奖励建模步骤",
        "关系强度": 8
    },
    "11": {
        "出发节点": "温度参数",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制策略模型的保守程度来调节KL散度的约束强度,较小的温度参数允许更大的KL散度,较大的温度参数限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "12": {
        "出发节点": "参考模型",
        "到达节点": "梯度性质",
        "关系名称": "约束关系",
        "关系解释": "参考模型通过KL散度约束梯度更新的方向和幅度,确保策略模型不会过度偏离参考分布",
        "关系强度": 8
    },
    "13": {
        "出发节点": "策略模型",
        "到达节点": "梯度性质",
        "关系名称": "优化关系",
        "关系解释": "梯度性质直接反映了策略模型在优化过程中如何调整参数以最大化偏好概率,梯度计算中的p值决定了参数更新方向",
        "关系强度": 8
    },
    "14": {
        "出发节点": "Direct Preference Optimization",
        "到达节点": "Policy Model",
        "关系名称": "优化关系",
        "关系解释": "Direct Preference Optimization通过Bradley-Terry模型和KL散度约束,直接优化策略模型的参数以对齐人类偏好",
        "关系强度": 9
    },
    "15": {
        "出发节点": "温度参数",
        "到达节点": "优化过程",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过控制KL散度的约束强度来调节优化过程的保守程度,较小的β允许更大的策略偏离(高KL散度),较大的β限制策略接近参考模型(低KL散度)",
        "关系强度": 8
    },
    "16": {
        "出发节点": "温度参数",
        "到达节点": "偏好学习",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制策略模型与参考模型之间的KL散度约束强度,调节偏好学习过程中模型保守性与探索性的平衡",
        "关系强度": 8
    },
    "17": {
        "出发节点": "KL散度",
        "到达节点": "二分类问题",
        "关系名称": "正则化关系",
        "关系解释": "KL散度在DPO框架中作为正则化项约束二分类问题的优化过程,防止策略模型过度偏离参考模型",
        "关系强度": 8
    },
    "18": {
        "出发节点": "奖励模型",
        "到达节点": "硬分类损失",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,奖励模型驱动的DPO损失函数退化为硬分类损失,表现为严格的二元分类决策边界",
        "关系强度": 8
    },
    "19": {
        "出发节点": "奖励模型",
        "到达节点": "KL散度",
        "关系名称": "量化关系",
        "关系解释": "奖励模型中的对数概率比直接对应于KL散度的定义项,用于量化策略模型与参考模型之间的分布差异",
        "关系强度": 8
    },
    "20": {
        "出发节点": "温度参数",
        "到达节点": "梯度性质",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制奖励差异的缩放比例来调节梯度更新的幅度,较小的温度参数会放大梯度更新,而较大的温度参数会抑制梯度更新",
        "关系强度": 8
    },
    "21": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "二分类问题",
        "关系名称": "基础关系",
        "关系解释": "Bradley-Terry模型为二分类问题提供了概率建模基础,通过比较两个选项的潜在能力值来预测偏好选择",
        "关系强度": 8
    },
    "22": {
        "出发节点": "条件概率分布",
        "到达节点": "优化过程",
        "关系名称": "约束关系",
        "关系解释": "条件概率分布通过KL散度约束优化过程,确保策略模型不会过度偏离参考模型",
        "关系强度": 8
    },
    "23": {
        "出发节点": "策略模型",
        "到达节点": "优化过程",
        "关系名称": "约束关系",
        "关系解释": "优化过程通过KL散度约束策略模型与参考模型之间的偏离程度,温度参数β控制约束强度",
        "关系强度": 8
    },
    "24": {
        "出发节点": "偏好学习",
        "到达节点": "Bradley-Terry模型",
        "关系名称": "基础关系",
        "关系解释": "Bradley-Terry模型为偏好学习提供了概率建模基础,通过定义选项间的二元比较概率来量化偏好关系",
        "关系强度": 8
    },
    "25": {
        "出发节点": "参考模型",
        "到达节点": "偏好学习",
        "关系名称": "正则化基础关系",
        "关系解释": "参考模型通过KL散度为偏好学习提供正则化约束,确保策略模型在优化偏好目标时不会过度偏离参考分布",
        "关系强度": 8
    },
    "26": {
        "出发节点": "KL散度",
        "到达节点": "优化过程",
        "关系名称": "约束关系",
        "关系解释": "KL散度在DPO的优化过程中作为隐式约束,控制策略模型与参考模型之间的偏离程度,温度参数β调节约束强度",
        "关系强度": 8
    },
    "27": {
        "出发节点": "硬分类损失",
        "到达节点": "梯度性质",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,DPO损失函数退化为硬分类损失,此时梯度性质表现为对错误分类的强烈惩罚",
        "关系强度": 8
    },
    "28": {
        "出发节点": "二分类问题",
        "到达节点": "优化过程",
        "关系名称": "转化关系",
        "关系解释": "DPO将偏好学习问题转化为二分类问题,并通过优化过程来最大化偏好对的概率,利用Bradley-Terry模型将偏好关系建模为概率比较",
        "关系强度": 8
    },
    "29": {
        "出发节点": "奖励模型",
        "到达节点": "二分类问题",
        "关系名称": "转化关系",
        "关系解释": "根据Bradley-Terry模型,奖励模型的输出差值被转化为二分类问题的概率估计,通过sigmoid函数实现偏好概率的计算",
        "关系强度": 8
    },
    "30": {
        "出发节点": "硬分类损失",
        "到达节点": "二分类问题",
        "关系名称": "特化关系",
        "关系解释": "硬分类损失是二分类问题中当温度参数趋近于零时的极限特例,表现为非概率性的绝对偏好判断",
        "关系强度": 8
    },
    "31": {
        "出发节点": "温度参数",
        "到达节点": "KL散度",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制策略模型的保守程度来调节其与参考模型之间的KL散度约束强度,较小的温度参数允许更大的KL散度,而较大的温度参数限制策略偏离参考模型的程度",
        "关系强度": 8
    },
    "32": {
        "出发节点": "KL散度",
        "到达节点": "策略模型",
        "关系名称": "约束关系",
        "关系解释": "KL散度作为正则项约束策略模型与参考模型之间的偏离程度,通过温度参数β控制约束强度",
        "关系强度": 8
    },
    "33": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "二分类问题",
        "关系名称": "转化关系",
        "关系解释": "DPO将偏好学习问题转化为基于Bradley-Terry模型的二分类问题,通过最大化偏好对的似然概率来优化模型",
        "关系强度": 8
    },
    "34": {
        "出发节点": "策略模型",
        "到达节点": "二分类问题",
        "关系名称": "转化关系",
        "关系解释": "DPO将策略模型的偏好学习问题转化为基于Bradley-Terry模型的二分类概率优化问题",
        "关系强度": 8
    },
    "35": {
        "出发节点": "硬分类损失",
        "到达节点": "策略模型",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,DPO损失函数退化为硬分类损失,此时策略模型完全依据偏好数据做出非黑即白的决策",
        "关系强度": 8
    },
    "36": {
        "出发节点": "偏好学习",
        "到达节点": "优化过程",
        "关系名称": "转化关系",
        "关系解释": "DPO将偏好学习问题通过Bradley-Terry模型转化为概率优化问题,利用KL散度作为约束条件指导优化过程",
        "关系强度": 8
    },
    "37": {
        "出发节点": "条件概率分布",
        "到达节点": "KL散度",
        "关系名称": "度量关系",
        "关系解释": "KL散度通过期望运算量化两个条件概率分布之间的差异程度,在DPO中用于衡量策略模型与参考模型的条件概率分布偏离程度",
        "关系强度": 9
    },
    "38": {
        "出发节点": "条件概率分布",
        "到达节点": "二分类问题",
        "关系名称": "转化关系",
        "关系解释": "通过Bradley-Terry模型将条件概率分布的比较转化为二分类问题,利用概率比值构建分类边界",
        "关系强度": 8
    },
    "39": {
        "出发节点": "温度参数",
        "到达节点": "二分类问题",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制概率分布的尖锐程度来调节二分类问题的决策边界硬度,较小的β值使分类决策更明确(硬边界),较大的β值使分类决策更平滑(软边界)",
        "关系强度": 8
    },
    "40": {
        "出发节点": "参考模型",
        "到达节点": "二分类问题",
        "关系名称": "基础关系",
        "关系解释": "参考模型提供的条件概率分布作为基准,为基于Bradley-Terry模型的二分类偏好概率计算提供基础支撑",
        "关系强度": 8
    },
    "41": {
        "出发节点": "偏好学习",
        "到达节点": "条件概率分布",
        "关系名称": "优化关系",
        "关系解释": "偏好学习通过Bradley-Terry模型将人类偏好转化为条件概率分布的优化目标,利用KL散度约束策略模型与参考模型的条件概率分布差异",
        "关系强度": 8
    },
    "42": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "优化过程",
        "关系名称": "基础关系",
        "关系解释": "Bradley-Terry模型为优化过程提供了概率建模框架,DPO的损失函数直接基于该模型的偏好概率公式构建",
        "关系强度": 8
    },
    "43": {
        "出发节点": "参考模型",
        "到达节点": "优化过程",
        "关系名称": "正则化关系",
        "关系解释": "参考模型通过KL散度约束为优化过程提供正则化,控制策略模型不过度偏离参考分布",
        "关系强度": 8
    },
    "44": {
        "出发节点": "KL散度",
        "到达节点": "梯度性质",
        "关系名称": "约束关系",
        "关系解释": "KL散度通过控制策略模型与参考模型之间的差异程度,约束了梯度更新的方向和幅度,温度参数β调节这种约束强度",
        "关系强度": 8
    },
    "45": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "硬分类损失",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,Bradley-Terry模型的概率输出会退化为硬分类决策,即选择概率最大的选项",
        "关系强度": 8
    },
    "46": {
        "出发节点": "条件概率分布",
        "到达节点": "硬分类损失",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,基于条件概率分布的DPO损失函数退化为硬分类损失,体现了概率模型在极限情况下的确定性决策特性",
        "关系强度": 8
    },
    "47": {
        "出发节点": "偏好学习",
        "到达节点": "策略模型",
        "关系名称": "优化关系",
        "关系解释": "偏好学习通过Bradley-Terry模型和KL散度约束,优化策略模型的参数使其生成的响应更符合人类偏好",
        "关系强度": 9
    },
    "48": {
        "出发节点": "偏好学习",
        "到达节点": "硬分类损失",
        "关系名称": "泛化关系",
        "关系解释": "偏好学习通过Bradley-Terry模型将硬分类损失泛化为概率形式的偏好比较,当温度参数β趋近于0时,DPO损失函数退化为硬分类损失",
        "关系强度": 8
    },
    "49": {
        "出发节点": "二分类问题",
        "到达节点": "梯度性质",
        "关系名称": "基础关系",
        "关系解释": "梯度性质是二分类问题在Bradley-Terry模型框架下的数学优化基础,通过概率比较的梯度计算实现偏好学习",
        "关系强度": 8
    },
    "50": {
        "出发节点": "KL散度",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过调节KL散度在优化过程中的影响程度来控制约束强度,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "51": {
        "出发节点": "参考模型",
        "到达节点": "直接偏好优化(DPO)",
        "关系名称": "约束优化关系",
        "关系解释": "DPO在参考模型的基础上,通过隐式控制KL散度来优化策略模型,使其在保持与参考模型相似性的同时学习人类偏好",
        "关系强度": 8
    },
    "52": {
        "出发节点": "硬分类损失",
        "到达节点": "优化过程",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,DPO损失函数退化为硬分类损失形式,此时优化过程直接最大化偏好对的分类准确率",
        "关系强度": 8
    },
    "53": {
        "出发节点": "条件概率分布",
        "到达节点": "策略模型",
        "关系名称": "参数化关系",
        "关系解释": "策略模型通过参数θ显式地定义了条件概率分布π_θ(y|x),将概率分布形式转化为可优化的参数化模型",
        "关系强度": 8
    },
    "54": {
        "出发节点": "奖励模型",
        "到达节点": "策略模型",
        "关系名称": "对数差分关系",
        "关系解释": "奖励模型被定义为策略模型与参考模型的条件概率分布的对数差,即奖励模型的值直接由策略模型和参考模型的对数概率决定",
        "关系强度": 8
    },
    "55": {
        "出发节点": "条件概率分布",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过调节条件概率分布与参考分布的KL散度来控制约束强度,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "56": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "梯度性质",
        "关系名称": "推导关系",
        "关系解释": "DPO的损失函数通过Bradley-Terry模型和KL散度推导出特定的梯度性质,该梯度性质直接影响模型参数的更新方向和强度",
        "关系强度": 8
    },
    "57": {
        "出发节点": "条件概率分布",
        "到达节点": "梯度性质",
        "关系名称": "推导关系",
        "关系解释": "梯度性质是通过对条件概率分布的对数比进行求导而得到的,反映了策略模型与参考模型之间的差异变化率",
        "关系强度": 8
    },
    "58": {
        "出发节点": "偏好学习",
        "到达节点": "直接偏好优化(DPO)",
        "关系名称": "参数化关系",
        "关系解释": "DPO通过将偏好学习问题参数化为Bradley-Terry模型的对数概率比形式,实现了无需显式奖励模型的偏好优化",
        "关系强度": 8
    },
    "59": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "Kullback-Leibler Divergence (KL散度)",
        "关系名称": "约束关系",
        "关系解释": "DPO通过温度参数β隐式地约束策略模型与参考模型之间的KL散度,控制策略偏离参考模型的程度",
        "关系强度": 8
    },
    "60": {
        "出发节点": "策略模型",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过控制策略模型与参考模型之间的KL散度来调节约束强度,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "61": {
        "出发节点": "偏好学习",
        "到达节点": "二分类问题",
        "关系名称": "转化关系",
        "关系解释": "DPO将偏好学习问题转化为基于Bradley-Terry模型的二分类概率优化问题,通过比较获胜回复和失败回复的对数概率差来构建分类目标",
        "关系强度": 8
    },
    "62": {
        "出发节点": "二分类问题",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "通过温度参数β,二分类问题的优化过程调节了策略模型与参考模型之间KL散度的约束强度.较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "63": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "Optimization Process",
        "关系名称": "隐式约束关系",
        "关系解释": "DPO通过Bradley-Terry模型和KL散度隐式地约束优化过程,控制策略模型与参考模型之间的偏离程度",
        "关系强度": 8
    },
    "64": {
        "出发节点": "参考模型",
        "到达节点": "奖励模型",
        "关系名称": "对数差分关系",
        "关系解释": "奖励模型被定义为策略模型与参考模型对数概率的差值,即rθ(x,y) = log(πθ(y|x)) - log(πref(y|x)),表明奖励模型通过比较策略模型与参考模型的输出差异来构建",
        "关系强度": 8
    },
    "65": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "策略模型",
        "关系名称": "概率框架关系",
        "关系解释": "策略模型利用Bradley-Terry模型的概率框架来建模偏好选择,将策略比较问题转化为基于能力值的概率预测问题",
        "关系强度": 8
    },
    "66": {
        "出发节点": "优化过程",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数β在优化过程中动态调节KL散度的约束强度,较小的β允许更大的KL散度(弱约束),较大的β限制策略偏离参考模型过远(强约束)",
        "关系强度": 8
    },
    "67": {
        "出发节点": "硬分类损失",
        "到达节点": "直接偏好优化",
        "关系名称": "广义化关系",
        "关系解释": "直接偏好优化通过引入Bradley-Terry概率模型和温度参数β,将硬分类损失广义化为可调节的软概率形式,同时保持KL散度约束",
        "关系强度": 8
    },
    "68": {
        "出发节点": "奖励模型",
        "到达节点": "偏好学习",
        "关系名称": "基础关系",
        "关系解释": "Bradley-Terry模型将奖励模型的输出转化为偏好概率,为偏好学习提供概率化比较框架,其中奖励差值直接决定偏好强度",
        "关系强度": 8
    },
    "69": {
        "出发节点": "偏好学习",
        "到达节点": "梯度性质",
        "关系名称": "优化关系",
        "关系解释": "梯度性质直接反映了偏好学习目标函数的优化方向,通过Bradley-Terry模型将偏好比较转化为概率最大化问题,其梯度计算明确显示了模型参数更新如何强化优选响应并弱化劣选响应",
        "关系强度": 8
    },
    "70": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "Kullback-Leibler Divergence Constraint Strength",
        "关系名称": "调节关系",
        "关系解释": "DPO通过温度参数β隐式地调节策略模型与参考模型之间的KL散度约束强度,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "71": {
        "出发节点": "梯度性质",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "梯度性质中的温度参数β直接调节KL散度的约束强度,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "72": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "在DPO框架中,Bradley-Terry模型通过温度参数β显式调节KL散度对策略模型的约束强度,β越小则KL约束越宽松,β越大则策略越保守接近参考模型",
        "关系强度": 8
    },
    "73": {
        "出发节点": "硬分类损失",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "硬分类损失通过温度参数β调节KL散度的约束强度,较小的β对应更强的硬分类特性但允许更大的KL散度,较大的β则强化KL散度约束使模型更保守",
        "关系强度": 8
    },
    "74": {
        "出发节点": "偏好学习",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "在直接偏好优化(DPO)中,温度参数β通过调节偏好学习目标函数中的奖励差异,间接控制策略模型与参考模型之间的KL散度约束强度.较小的β允许更大的KL散度(策略更自由偏离参考模型),较大的β强化KL约束(策略更保守接近参考模型)",
        "关系强度": 8
    },
    "75": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "梯度性质",
        "关系名称": "基础关系",
        "关系解释": "Bradley-Terry模型为DPO的梯度性质提供了概率建模基础,其偏好概率公式直接决定了损失函数的构造方式,进而影响梯度计算的结构",
        "关系强度": 8
    },
    "76": {
        "出发节点": "条件概率分布",
        "到达节点": "Bradley-Terry模型",
        "关系名称": "基础关系",
        "关系解释": "Bradley-Terry模型建立在条件概率分布的基础上,通过特定形式的概率比值(π_i/(π_i + π_j))来描述偏好关系,其中π_i可视为从条件概率分布中导出的潜在强度参数",
        "关系强度": 8
    },
    "77": {
        "出发节点": "奖励模型",
        "到达节点": "梯度性质",
        "关系名称": "数学表达关系",
        "关系解释": "梯度性质直接由奖励模型的数学表达式推导得出,梯度计算中的关键项(1-p)和p都来自奖励模型在Bradley-Terry框架下的概率表达",
        "关系强度": 8
    },
    "78": {
        "出发节点": "奖励模型",
        "到达节点": "Bradley-Terry模型",
        "关系名称": "实例化关系",
        "关系解释": "奖励模型通过定义r_θ(x,y)=log(π_θ(y|x)/π_ref(y|x)),将Bradley-Terry模型中的潜在能力值π_i具体实例化为策略模型与参考模型的对数概率比",
        "关系强度": 8
    },
    "79": {
        "出发节点": "奖励模型",
        "到达节点": "条件概率分布",
        "关系名称": "对数线性关系",
        "关系解释": "奖励模型被定义为条件概率分布与参考模型条件概率分布的对数比值,即r_θ(x,y) = log(π_θ(y|x)) - log(π_ref(y|x)),表明奖励与概率分布呈对数线性关系",
        "关系强度": 8
    },
    "80": {
        "出发节点": "温度参数",
        "到达节点": "条件概率分布",
        "关系名称": "调节关系",
        "关系解释": "温度参数通过控制策略模型与参考模型之间的KL散度,调节条件概率分布的保守程度.较小的温度参数允许更大的KL散度,使策略模型更偏离参考模型;较大的温度参数限制KL散度,使策略模型更接近参考模型.",
        "关系强度": 8
    },
    "81": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "直接偏好优化(Direct Preference Optimization)",
        "关系名称": "基础关系",
        "关系解释": "直接偏好优化以Bradley-Terry模型为理论基础,将其偏好概率公式作为核心组件嵌入到损失函数设计中,用于建模和优化语言模型的偏好选择行为",
        "关系强度": 9
    },
    "82": {
        "出发节点": "奖励模型",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过控制奖励模型的输出差异,间接调节策略模型与参考模型之间的KL散度约束强度.较小的β允许更大的KL散度(策略偏离参考模型更自由),较大的β强制更小的KL散度(策略需保持接近参考模型)",
        "关系强度": 8
    },
    "83": {
        "出发节点": "条件概率分布",
        "到达节点": "直接偏好优化(DPO)",
        "关系名称": "基础关系",
        "关系解释": "条件概率分布是DPO模型构建的基础,DPO通过比较策略模型和参考模型的条件概率分布来定义奖励函数,并利用Bradley-Terry模型建立偏好概率与条件概率分布之间的关系",
        "关系强度": 9
    },
    "84": {
        "出发节点": "参考模型",
        "到达节点": "Bradley-Terry模型",
        "关系名称": "概率框架关系",
        "关系解释": "参考模型通过提供归一化的对数概率差(即奖励函数r_θ(x,y) = log(π_θ(y|x)) - log(π_ref(y|x))),为Bradley-Terry模型中的偏好概率计算提供了概率框架基础.Bradley-Terry模型利用该框架将策略模型与参考模型的输出差异转化为可优化的偏好概率形式.",
        "关系强度": 8
    },
    "85": {
        "出发节点": "Bradley-Terry模型",
        "到达节点": "KL散度",
        "关系名称": "优化约束关系",
        "关系解释": "在DPO框架中,Bradley-Terry模型定义的偏好概率被用于构建损失函数,而KL散度则作为隐式约束控制策略模型与参考模型的偏离程度,两者共同构成目标函数的概率比较基础和正则化约束",
        "关系强度": 7
    },
    "86": {
        "出发节点": "参考模型",
        "到达节点": "KL散度的约束强度",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过调节参考模型与策略模型之间的KL散度约束强度,控制策略模型的保守程度.β越小,KL散度约束越弱,允许策略模型更大程度偏离参考模型;β越大,KL散度约束越强,策略模型行为越接近参考模型.",
        "关系强度": 8
    },
    "87": {
        "出发节点": "偏好学习",
        "到达节点": "KL散度",
        "关系名称": "约束关系",
        "关系解释": "在直接偏好优化(DPO)中,KL散度被用作隐式约束,确保学习到的策略模型不会过度偏离参考模型,从而保持生成结果的合理性和多样性.温度参数β调节KL散度的约束强度,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远.",
        "关系强度": 8
    },
    "88": {
        "出发节点": "优化过程",
        "到达节点": "梯度性质",
        "关系名称": "基础关系",
        "关系解释": "梯度性质直接反映了优化过程中目标函数的变化趋势,是优化过程的基础数学表现.DPO的梯度计算揭示了模型参数更新如何通过Bradley-Terry概率和KL散度约束来调整策略模型与参考模型的差异.",
        "关系强度": 8
    },
    "89": {
        "出发节点": "奖励模型",
        "到达节点": "优化过程",
        "关系名称": "基础关系",
        "关系解释": "奖励模型为优化过程提供了基础的目标函数,通过定义奖励函数来指导策略模型的优化方向.具体来说,奖励模型将策略模型和参考模型的输出差异量化为奖励值,这些奖励值随后被用于构建优化目标(如DPO Loss),从而驱动策略模型的学习过程.",
        "关系强度": 8
    },
    "90": {
        "出发节点": "Direct Preference Optimization",
        "到达节点": "Reference Model",
        "关系名称": "正则化约束关系",
        "关系解释": "DPO通过KL散度隐式地约束策略模型不要过度偏离参考模型的概率分布,参考模型作为策略优化的基准和正则化项",
        "关系强度": 8
    },
    "91": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "温度参数",
        "关系名称": "调节关系",
        "关系解释": "温度参数控制DPO策略的保守程度,较小的值允许更大的KL散度(策略更偏离参考模型),较大的值限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "92": {
        "出发节点": "优化过程",
        "到达节点": "直接偏好优化",
        "关系名称": "基础关系",
        "关系解释": "直接偏好优化的推导过程基于Bradley-Terry模型和KL散度的优化框架,通过将偏好学习转化为二分类问题并控制策略与参考模型之间的KL散度来实现优化",
        "关系强度": 8
    },
    "93": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "Preference Learning",
        "关系名称": "实例化关系",
        "关系解释": "DPO是偏好学习在生成模型领域的具体实现方法,通过Bradley-Terry模型将偏好比较转化为概率优化问题,并利用KL散度约束策略更新",
        "关系强度": 8
    },
    "94": {
        "出发节点": "KL散度",
        "到达节点": "硬分类损失",
        "关系名称": "极限关系",
        "关系解释": "当温度参数β趋近于0时,基于KL散度约束的DPO损失函数退化为硬分类损失,体现了KL散度在极限条件下的特殊表现形式",
        "关系强度": 8
    },
    "95": {
        "出发节点": "梯度性质",
        "到达节点": "DPO",
        "关系名称": "优化关系",
        "关系解释": "梯度性质描述了DPO损失函数的优化方向和强度,通过控制策略模型与参考模型之间的KL散度来优化偏好学习",
        "关系强度": 8
    },
    "96": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "硬分类损失",
        "关系名称": "温度参数控制的渐进逼近关系",
        "关系解释": "当温度参数β趋近于0时,DPO损失函数的行为趋近于硬分类损失,实现从概率化偏好学习到确定性分类的渐进过渡",
        "关系强度": 8
    },
    "97": {
        "出发节点": "KL散度",
        "到达节点": "直接偏好优化(DPO)",
        "关系名称": "约束关系",
        "关系解释": "DPO通过温度参数β隐式地约束策略模型与参考模型之间的KL散度,β调节KL散度的约束强度,实现奖励最大化与策略保守性的平衡",
        "关系强度": 8
    },
    "98": {
        "出发节点": "Direct Preference Optimization",
        "到达节点": "Bradley-Terry模型",
        "关系名称": "扩展关系",
        "关系解释": "Direct Preference Optimization将Bradley-Terry模型从静态配对比较扩展到动态策略优化领域,通过引入参考模型和KL散度约束,使其适用于语言模型的偏好学习",
        "关系强度": 8
    },
    "99": {
        "出发节点": "策略模型",
        "到达节点": "DPO",
        "关系名称": "优化框架关系",
        "关系解释": "DPO通过Bradley-Terry模型和KL散度约束,为策略模型提供了一种直接偏好优化的数学框架,将策略优化转化为偏好概率最大化问题",
        "关系强度": 8
    },
    "100": {
        "出发节点": "KL散度的约束强度",
        "到达节点": "直接偏好优化(DPO)",
        "关系名称": "调节关系",
        "关系解释": "温度参数β通过控制KL散度的约束强度来调节DPO的优化过程,较小的β允许更大的KL散度,较大的β限制策略偏离参考模型过远",
        "关系强度": 8
    },
    "101": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "Reward Model",
        "关系名称": "隐式替代关系",
        "关系解释": "DPO通过策略模型与参考模型的对数概率差隐式定义奖励函数,避免了显式奖励模型的训练过程",
        "关系强度": 8
    },
    "102": {
        "出发节点": "Direct Preference Optimization (DPO)",
        "到达节点": "Conditional Probability Distribution",
        "关系名称": "正则化约束关系",
        "关系解释": "DPO通过KL散度隐式地约束策略模型的条件概率分布与参考模型的条件概率分布之间的偏离程度,确保优化过程不会过度偏离参考分布",
        "关系强度": 8
    },
    "103": {
        "出发节点": "二分类问题",
        "到达节点": "直接偏好优化(DPO)",
        "关系名称": "基础关系",
        "关系解释": "DPO的核心思想是将偏好学习问题转化为基于Bradley-Terry模型的二分类问题,通过优化获胜回复与失败回复的选择概率来实现偏好学习",
        "关系强度": 8
    }
};  // 使用特殊占位符

                console.log("实体数据:", entities);
                console.log("关系数据:", relations);

                // 转换 entities_dict 为 nodes，使用模型/算法名称作为 id
                const nodes = Object.values(entities).map((entity, index) => ({
                    id: entity["模型/算法名称"],
                    name: entity["模型/算法名称"],
                    definition: entity["定义"],
                    applicable_scenarios: entity["适用场景"],
                    core_idea: entity["核心思想"],
                    advantages: entity["优势特点"],
                    limitations: entity["局限性"] ,
                    application_fields: entity["应用领域"],
                    key_components: entity["关键组件"],
                    related_models: entity["相关模型/算法"],
                    color: d3.schemeCategory10[index % 10]
                }));
                console.log("生成的节点数据:", nodes);


                // 根据实体名称查找节点 ID
                function findNodeId(name) {
                    const node = nodes.find(n => n.id === name);
                    if (!node) {
                        console.warn(`未找到名称为 "${name}" 的节点。`);
                    }
                    return node ? node.id : null;
                }

                // 转换 relations_dict 为 links
                const links = Object.values(relations).map(relation => ({
                    source: findNodeId(relation["出发节点"]),
                    target: findNodeId(relation["到达节点"]),
                    relationship_type: relation["关系类型"],
                    relationship_explanation: relation["关系说明"],
                    relationship_strength: parseInt(relation["相关度"])
                })).filter(link => {
                    const valid = link.source !== null && link.target !== null;
                    if (!valid) {
                        console.warn("过滤掉无效的链接:", link);
                    }
                    return valid;
                });
                console.log("生成的链接数据:", links);

                // 识别连通性和聚类
                function findClusters(nodes, links) {
                    const clusters = [];
                    const visited = new Set();

                    function dfs(node, cluster) {
                        if (!node) {
                            console.error('未找到节点:', node);
                            return;
                        }
                        visited.add(node.id);
                        cluster.push(node);
                        links.forEach(link => {
                            if (link.source === node.id && !visited.has(link.target)) {
                                const targetNode = nodes.find(n => n.id === link.target);
                                if (targetNode) {
                                    dfs(targetNode, cluster);
                                }
                            } else if (link.target === node.id && !visited.has(link.source)) {
                                const sourceNode = nodes.find(n => n.id === link.source);
                                if (sourceNode) {
                                    dfs(sourceNode, cluster);
                                }
                            }
                        });
                    }

                    nodes.forEach(node => {
                        if (!visited.has(node.id)) {
                            const cluster = [];
                            console.log(`开始处理节点 "${node.id}" 的聚类。`);
                            dfs(node, cluster);
                            clusters.push(cluster);
                            console.log(`完成一个聚类，包含节点:`, cluster.map(n => n.id));
                        }
                    });

                    console.log("识别出的所有聚类:", clusters);
                    return clusters;
                }

                // 识别团簇并为其上色
                function colorClusters(nodes, links) {
                    console.log("开始识别和上色聚类。");
                    const clusters = findClusters(nodes, links);
                    const color = d3.scaleOrdinal(d3.schemeCategory10);

                    clusters.forEach((cluster, clusterIndex) => {
                        cluster.forEach(node => {
                            node.cluster = clusterIndex;
                            node.color = color(clusterIndex);
                        });
                    });
                    console.log("聚类及颜色设置完成。");
                }

                // 识别团簇并上色
                colorClusters(nodes, links);

                // 创建深拷贝的原始数据
                let originalNodes = JSON.parse(JSON.stringify(nodes));
                let originalLinks = JSON.parse(JSON.stringify(links));
                console.log("创建原始数据的深拷贝。");

                let selectedNodeId = null; // 记录选中的节点 ID

                // 创建 SVG 和力导向仿真
                const width = window.innerWidth; 
                const height = window.innerHeight;
                console.log(`创建 SVG，宽度: ${width}, 高度: ${height}`);
                const svgSelection = d3.select("#canvas")
                    .attr("width", width)
                    .attr("height", height);
                    
                const svgGroup = svgSelection.append("g"); // 创建一个组元素用于缩放
                console.log("添加组元素到 SVG。");

                svgSelection.call(d3.zoom() // 添加缩放和拖拽功能
                    .on("zoom", (event) => {
                        svgGroup.attr("transform", event.transform);
                        console.log("缩放事件触发，transform:", event.transform);
                    }))
                    .on("dblclick.zoom", null); // 禁用双击缩放
                console.log("添加缩放和拖拽功能。");

                const simulation = d3.forceSimulation(nodes)
                    .force("link", d3.forceLink(links).id(d => d.id).distance(150)) // 调整距离
                    .force("charge", d3.forceManyBody().strength(-30)) // 调整力的强度
                    .force("center", d3.forceCenter(width / 2, height / 2));
                console.log("初始化力导向仿真。");

                // 绘制边
                let link = svgGroup.append("g")
                    .attr("class", "links")
                    .selectAll("line")
                    .data(links)
                    .enter().append("line")
                    .attr("class", "edge")
                    .attr("stroke-width", d => Math.sqrt(d.relationship_strength));
                console.log("绘制边完成。");

                // 绘制节点
                let node = svgGroup.append("g")
                    .attr("class", "nodes")
                    .selectAll("g")
                    .data(nodes)
                    .enter().append("g")
                    .attr("class", "node")
                    .call(d3.drag()
                        .on("start", dragstarted)
                        .on("drag", dragged)
                        .on("end", dragended));
                console.log("绘制节点组完成。");

                node.append("circle")
                    .attr("r", 10)
                    .attr("fill", d => d.color);
                console.log("添加节点圆形元素完成。");

                node.append("text")
                    .attr("dy", ".35em")
                    .text(d => d.name);
                console.log("添加节点文本元素完成。");

                // 添加事件监听器
                node.on("click", nodeClicked);
                console.log("添加节点点击事件监听器完成。");

                // 节点点击事件处理函数
                function nodeClicked(event, d) {
                    console.log(`节点 "${d.id}" 被点击。`);
                    selectedNodeId = d.id; // 记录选中的节点 ID
                    const neighborNodeIds = new Set();
                    links.forEach(link => {
                        if (link.source.id === d.id) { // 修正为link.source.id
                            neighborNodeIds.add(link.target.id);
                        } else if (link.target.id === d.id) { // 修正为link.target.id
                            neighborNodeIds.add(link.source.id);
                        }
                    });
                    neighborNodeIds.add(d.id);
                    console.log(`节点 "${d.id}" 的邻居节点 IDs:`, Array.from(neighborNodeIds));

                    const filteredNodes = nodes.filter(node => neighborNodeIds.has(node.id));
                    const filteredLinks = links.filter(link => neighborNodeIds.has(link.source.id) && neighborNodeIds.has(link.target.id));
                    console.log("过滤后的节点:", filteredNodes);
                    console.log("过滤后的链接:", filteredLinks);

                    updateGraph(filteredNodes, filteredLinks);
                    console.log("更新图形完成。");

                    // 在右边栏显示选中节点的信息
                    const entity = entities[d.id];
                    if (!entity) {
                        console.warn(`未找到节点 "${d.id}" 对应的实体信息。`);
                    }
                    console.log(`显示节点 "${d.id}" 的详细信息。`);
                    d3.select("#sidebar-title").text(entity ? entity["模型/算法名称"] : "未知");
                    d3.select("#definition").text(entity ? entity["定义"] : "无");
                    d3.select("#applicable_scenarios").text(entity ? entity["适用场景"] : "无");
                    d3.select("#core_idea").text(entity ? entity["核心思想"] : "无");

                    d3.select("#advantages").text(entity ? entity["优势特点"] : "未知");
                    d3.select("#limitations").text(entity ? entity["局限性"] : "无");
                    d3.select("#application_fields").text(entity ? entity["应用领域"] : "无");
                    d3.select("#key_components").text(entity ? entity["关键组件"] : "无");
                    // 处理列表类型的数据


                    // 处理相关模型/算法
                    const relatedModels = entity ? entity["相关模型/算法"] : [];
                    updateList("#related_models", relatedModels);

                    // 处理与其他实体的关系
                    const relatedRelations = entity ? Object.values(relations).filter(relation => 
                        relation["出发节点"] === entity["模型/算法名称"] || 
                        relation["到达节点"] === entity["模型/算法名称"]
                    ) : [];
                    const relationsList = d3.select("#relations");
                    relationsList.html("");
                    relatedRelations.forEach(relation => {
                        relationsList.append("li").html(`
                            <strong>${relation["关系类型"]}:</strong> ${relation["关系说明"]} (相关度: ${relation["相关度"]})
                        `);
                    });
                    console.log("显示相关关系。");

                    d3.select("#sidebar").classed("show", true);
                    console.log("侧边栏已显示。");
                }

                // 检查节点是否有未展示的邻居
                function hasMissingNeighbors(node, newNodes) {
                    const neighborNodeIds = new Set();
                    links.forEach(link => {
                        if (link.source.id === node.id) { // 修正为link.source.id
                            neighborNodeIds.add(link.target.id);
                        } else if (link.target.id === node.id) { // 修正为link.target.id
                            neighborNodeIds.add(link.source.id);
                        }
                    });
                    const hasMissing = Array.from(neighborNodeIds).some(id => !newNodes.some(n => n.id === id));
                    if (hasMissing) {
                        console.log(`节点 "${node.id}" 有未展示的邻居节点。`);
                    }
                    return hasMissing;
                }

                // 更新图形函数
                function updateGraph(newNodes, newLinks) {
                    console.log("开始更新图形。");
                    simulation.nodes(newNodes);
                    simulation.force("link").links(newLinks);
                    console.log("更新仿真的节点和链接。");

                    // 更新链接
                    link = link.data(newLinks, d => `${d.source.id}-${d.target.id}`);
                    console.log("绑定新的链接数据。");
                    link.exit().remove();
                    console.log("移除退出的链接。");
                    link = link.enter().append("line")
                        .attr("class", "edge")
                        .attr("stroke-width", d => Math.sqrt(d.relationship_strength))
                        .merge(link);
                    console.log("添加新链接完成。");

                    // 更新节点
                    node = node.data(newNodes, d => d.id);
                    console.log("绑定新的节点数据。");
                    node.exit().remove();
                    console.log("移除退出的节点。");

                    const nodeEnter = node.enter().append("g")
                        .attr("class", "node")
                        .call(d3.drag()
                            .on("start", dragstarted)
                            .on("drag", dragged)
                            .on("end", dragended));
                    console.log("添加新节点组完成。");

                    nodeEnter.append("circle")
                        .attr("r", 10)
                        .attr("fill", d => d.color);
                    console.log("添加新节点圆形元素完成。");

                    nodeEnter.append("text")
                        .attr("dy", ".35em")
                        .text(d => d.name);
                    console.log("添加新节点文本元素完成。");

                    nodeEnter.on("click", nodeClicked);
                    console.log("添加新节点点击事件监听器完成。");

                    node = nodeEnter.merge(node);
                    console.log("合并新节点和现有节点。");

                    // 更新所有节点的文本
                    node.select("text")
                        .text(d => d.name);
                    console.log("更新所有节点的文本。");

                    // 设置选中节点和有未展示邻居节点的样式
                    node.select("circle")
                        .attr("r", d => {
                            if (d.id === selectedNodeId) {
                                return 12.5;
                            } else if (hasMissingNeighbors(d, newNodes)) {
                                return 10 * 1.2;
                            } else {
                                return 10;
                            }
                        })
                        .attr("class", d => {
                            if (d.id === selectedNodeId) {
                                return "selected-node";
                            } else if (hasMissingNeighbors(d, newNodes)) {
                                return "neighbor-missing-node";
                            } else {
                                return "";
                            }
                        })
                        .attr("fill", d => {
                            if (d.id === selectedNodeId) {
                                return "gold";
                            } else if (hasMissingNeighbors(d, newNodes)) {
                                return "green";
                            } else {
                                return d.color;
                            }
                        });
                    console.log("更新节点样式完成。");

                    simulation.alpha(1).restart();
                    console.log("重新启动仿真。");
                }

                // 力导向仿真事件
                simulation.on("tick", () => {
                    link.attr("x1", d => d.source.x)
                        .attr("y1", d => d.source.y)
                        .attr("x2", d => d.target.x)
                        .attr("y2", d => d.target.y);

                    node.attr("transform", d => `translate(${d.x},${d.y})`);
                    // 可以在这里添加日志来跟踪每个 tick 的位置，但可能会导致控制台过多日志
                    // console.log("仿真tick事件触发。");
                });
                console.log("设置力导向仿真的tick事件。");

                // 画布拖拽行为函数
                function dragstarted(event, d) {
                    if (!event.active) simulation.alphaTarget(0.3).restart();
                    d.fx = d.x;
                    d.fy = d.y;
                    console.log(`拖拽开始，节点: ${d.id}`);
                }

                function dragged(event, d) {
                    d.fx = event.x;
                    d.fy = event.y;
                    console.log(`正在拖拽节点: ${d.id} 到位置 (${event.x}, ${event.y})`);
                }

                function dragended(event, d) {
                    if (!event.active) simulation.alphaTarget(0);
                    d.fx = null;
                    d.fy = null;
                    console.log(`拖拽结束，节点: ${d.id}`);
                }

                // 重置视图
                d3.select("#reset").on("click", () => {
                    console.log("点击重置按钮，重新加载页面。");
                    location.reload(); // 重新加载页面
                });
                console.log("添加重置视图按钮事件监听器。");

                // 切换右边栏显示/隐藏
                d3.select("#toggleSidebar").on("click", () => {
                    const sidebar = d3.select("#sidebar");
                    const isShown = sidebar.classed("show");
                    sidebar.classed("show", !isShown);
                    d3.select("#toggleSidebar").text(isShown ? "显示信息" : "隐藏信息");
                    console.log(`切换侧边栏显示状态: ${isShown ? "隐藏" : "显示"}`);
                });
                console.log("添加切换侧边栏显示/隐藏按钮事件监听器。");

                // 初始化缩放行为
                const zoom = d3.zoom().on("zoom", zoomed);
                console.log("初始化缩放行为。");

                // 拖拽和缩放的行为
                function zoomed(event) {
                    svgGroup.attr("transform", event.transform);
                    console.log("缩放更新: ", event.transform);
                }

                // 应用缩放行为到整个 SVG
                d3.select("svg").call(zoom);
                console.log("应用缩放行为到 SVG。");

                // 确保 SVG 大小足够大以容纳所有节点
                function resize() {
                    const bounds = d3.select("svg").node().getBoundingClientRect();
                    const maxX = d3.max(nodes, d => d.x) + 20;
                    const maxY = d3.max(nodes, d => d.y) + 20;
                    const newWidth = Math.max(bounds.width, maxX);
                    const newHeight = Math.max(bounds.height, maxY);
                    d3.select("svg").attr("width", newWidth).attr("height", newHeight);
                    console.log(`调整SVG大小为: 宽度=${newWidth}, 高度=${newHeight}`);
                }

                // 调用 resize 函数
                resize();
                console.log("调用 resize 函数完成。");
            } catch (error) {
                console.error("在主函数执行过程中发生错误:", error);
            }
        }


        // 开始尝试加载 D3.js，并在加载完成后执行主函数
        loadD3(d3Urls, main);
    </script>
</body>
</html>
