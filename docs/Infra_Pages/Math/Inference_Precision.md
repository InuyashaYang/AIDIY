

## 1.FP16表示方法

FP16使用16位二进制数表示浮点数，格式如下：

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |   5 bits   |  10 bits   |

数值计算公式：$(-1)^S \times 2^{E-15} \times (1+M)$

### 数值计算方法

标准格式下的计算公式：

$(-1)^S \times 2^{E-15} \times (1+M/1024)$

其中：
- S: 符号位（0表示正数，1表示负数）
- E: 指数位（范围00001-11110，对应-14到+15）
- M: 尾数位（隐含首位1，实际精度为11位）


## 特殊值

| 指数 (E) | 尾数 (M) | 含义 |
|:--------:|:--------:|:----:|
|   00000  |    0     | $\pm 0$ |
|   00000  |   非0    | 非规格化数 |
|   11111  |    0     | $\pm \infty$ |
|   11111  |   非0    | NaN |

## 分辨率

FP16的分辨率随数值大小变化：

| 数值范围 | 最小精度 |
|:--------:|:--------:|
| [0.5, 1) | $2^{-11} \approx 4.88 \times 10^{-4}$ |
| [1, 2)   | $2^{-10} \approx 9.77 \times 10^{-4}$ |
| [2, 4)   | $2^{-9} \approx 1.95 \times 10^{-3}$ |

## 取值范围

- 最小规格化正数：$2^{-14} \approx 6.10 \times 10^{-5}$
- 最小非规格化正数：$2^{-24} \approx 5.96 \times 10^{-8}$
- 最大正数：$(2-2^{-10}) \times 2^{15} \approx 65504$

FP16的取值范围约为 $\pm [5.96 \times 10^{-8}, 65504]$


### PyTorch中的表示

```python
torch.finfo(torch.float16)
# 结果
finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, 
      smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)
```



## 2.FP32 (单精度浮点数)

### 表示方法

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |   8 bits   |  23 bits   |

### 数值计算方法

$(-1)^S \times 2^{E-127} \times (1+M/2^{23})$

### 特殊值

| 指数 (E) | 尾数 (M) | 含义 |
|:--------:|:--------:|:----:|
|   00000000  |    0     | $\pm 0$ |
|   00000000  |   非0    | 非规格化数 |
|   11111111  |    0     | $\pm \infty$ |
|   11111111  |   非0    | NaN |

### 取值范围

- 最小规格化正数：$2^{-126} \approx 1.18 \times 10^{-38}$
- 最大正数：$(2-2^{-23}) \times 2^{127} \approx 3.40 \times 10^{38}$

### PyTorch中的表示

```python
torch.finfo(torch.float32)
# 结果
finfo(resolution=1e-06, min=-3.4028234663852886e+38, max=3.4028234663852886e+38, 
      eps=1.1920928955078125e-07, smallest_normal=1.1754943508222875e-38, 
      tiny=1.1754943508222875e-38, dtype=float32)
```

## 3.FP64 (双精度浮点数)

### 表示方法

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |  11 bits   |  52 bits   |

### 数值计算方法

$(-1)^S \times 2^{E-1023} \times (1+M/2^{52})$

### 特殊值

| 指数 (E) | 尾数 (M) | 含义 |
|:--------:|:--------:|:----:|
| 00000000000 |    0     | $\pm 0$ |
| 00000000000 |   非0    | 非规格化数 |
| 11111111111 |    0     | $\pm \infty$ |
| 11111111111 |   非0    | NaN |

### 取值范围

- 最小规格化正数：$2^{-1022} \approx 2.22 \times 10^{-308}$
- 最大正数：$(2-2^{-52}) \times 2^{1023} \approx 1.80 \times 10^{308}$

### PyTorch中的表示

```python
torch.finfo(torch.float64)
# 结果
finfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, 
      eps=2.220446049250313e-16, smallest_normal=2.2250738585072014e-308, 
      tiny=2.2250738585072014e-308, dtype=float64)
```

## 4.BF16 (Brain Floating Point)

### 表示方法

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |   8 bits   |   7 bits   |

### 数值计算方法

$(-1)^S \times 2^{E-127} \times (1+M/2^{7})$

### 取值范围

- 最小规格化正数：$2^{-126} \approx 1.18 \times 10^{-38}$
- 最大正数：$(2-2^{-7}) \times 2^{127} \approx 3.39 \times 10^{38}$

### PyTorch中的表示

```python
torch.finfo(torch.bfloat16)
# 结果
finfo(resolution=0.01, min=-3.3895313892515355e+38, max=3.3895313892515355e+38, 
      eps=0.0078125, smallest_normal=1.1754943508222875e-38, 
      tiny=1.1754943508222875e-38, dtype=bfloat16)
```


## FP8 (8位浮点数)

FP8是一种新兴的8位浮点数格式，主要用于深度学习中的推理和训练。它有两种主要变体：E4M3和E5M2。

### E4M3 (主要用于激活值和梯度)

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |   4 bits   |   3 bits   |

### E5M2 (主要用于权重)

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |   5 bits   |   2 bits   |

### 数值计算方法

对于E4M3：$(-1)^S \times 2^{E-7} \times (1+M/2^3)$

对于E5M2：$(-1)^S \times 2^{E-15} \times (1+M/2^2)$

### 特点

1. 极小的内存占用，每个数只需8位。
2. 相比FP16，可以进一步提高计算速度和减少内存带宽需求。
3. 精度较低，需要特殊的训练技巧来维持模型性能。

### 取值范围（近似值）

- E4M3: $\pm[2^{-8}, 448]$
- E5M2: $\pm[2^{-16}, 57344]$

### 应用

主要用于深度学习模型的训练和推理，特别是在需要极致效率的场景，如边缘计算设备。

## TF32 (TensorFloat-32)

TF32是NVIDIA在Ampere架构GPU中引入的一种内部浮点格式。

### 表示方法

| 符号位 (S) | 指数位 (E) | 尾数位 (M) |
|:----------:|:----------:|:----------:|
|   1 bit    |   8 bits   |  10 bits   |

### 数值计算方法

$(-1)^S \times 2^{E-127} \times (1+M/2^{10})$

### 特点

1. 在内存中以FP32格式存储，但在计算时内部使用19位表示。
2. 结合了FP32的范围和BF16的精度。
3. 专为深度学习设计，在Tensor Core上可以显著加速矩阵乘法运算。

### 取值范围

与FP32相同：$\pm[1.18 \times 10^{-38}, 3.40 \times 10^{38}]$

### 精度

精度介于FP16和FP32之间，比BF16略高。

### 应用

1. 主要用于加速深度学习训练，特别是在支持TF32的NVIDIA GPU上。
2. 可以在不修改现有FP32模型的情况下获得性能提升。
3. 对大多数深度学习应用来说，精度损失通常可以忽略不计。

### PyTorch中的使用

```python
# 启用TF32（默认情况下在支持的设备上已启用）
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```


<script src="https://giscus.app/client.js"
        data-repo="InuyashaYang/AIDIY"
        data-repo-id="R_kgDOM1VVTQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOM1VVTc4Ckls_"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
