# 批归一化（Batch Normalization）简介与推导

## 简介

**批归一化（Batch Normalization）** 是一种在深度学习模型中用于规范化神经网络层输入的方法。它通过对一个小批次（mini-batch）内的激活值进行归一化处理，旨在加速训练过程、提高模型稳定性，并提升模型的泛化能力。BatchNorm 能有效缓解内部协变量偏移（Internal Covariate Shift）的问题，从而允许使用更高的学习率，加快模型收敛速度，并减少对初始化的敏感性。

## 工作原理

批归一化的核心思想是在每个小批次内，对神经网络层的激活值进行归一化操作。具体步骤包括计算小批次的均值与方差、执行归一化以及应用可学习的缩放和平移参数。

### 归一化步骤

对于一个输入矩阵 $ \mathbf{X} \in \mathbb{R}^{N \times D} $，其中 $N$ 是批次大小，$D$ 是特征维度，BatchNorm 的计算过程如下：

1. **计算批次均值 ($\mu$)**：
    $$
    \mu = \frac{1}{N} \sum_{i=1}^{N} x_i
    $$
   
2. **计算批次方差 ($\sigma^2$)**：
    $$
    \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
    $$
   
3. **归一化**：
    $$
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
    $$
    其中，$ \epsilon $ 是一个极小的常数，防止分母为零。
   
4. **缩放和平移**：
    $$
    y_i = \gamma \cdot \hat{x}_i + \beta
    $$
    这里，$ \gamma $ 和 $ \beta $ 是可学习的参数，用于调整归一化后的输出。

### 最终公式

将上述步骤综合起来，BatchNorm 的数学表达式为：

$$
\text{BatchNorm}(\mathbf{X}) = \gamma \cdot \frac{\mathbf{X} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

## 常用符号

| 符号             | 含义             |
| ---------------- | ---------------- |
| $ \mathbf{X} $   | 输入矩阵         |
| $ N $            | 批次大小         |
| $ D $            | 特征维度         |
| $ \mu $          | 批次均值         |
| $ \sigma^2 $     | 批次方差         |
| $ \gamma $       | 缩放参数         |
| $ \beta $        | 平移参数         |
| $ \epsilon $     | 稳定常数         |

## 一个具体的例子：

假设我们有一个 2x4 的矩阵作为输入，即一个批次包含 2 个样本，每个样本的特征维度是 4：

```
X = [[ 1.0,  2.0,  3.0,  4.0],
     [5.0,  6.0,  7.0,  8.0]]
```

为了简化计算，我们先设 $\epsilon = 0$，并且假设缩放参数 $\gamma$ 为全1向量，平移参数 $\beta$ 为全0向量。

**步骤 1: 计算批次的均值 ($\mu$)**

```
μ = [ (1 + 5)/2, (2 + 6)/2, (3 + 7)/2, (4 + 8)/2 ] = [3.0, 4.0, 5.0, 6.0]
```

**步骤 2: 计算批次的方差 ($\sigma^2$)**

```
σ² = [ ((1 - 3)² + (5 - 3)²)/2,
        ((2 - 4)² + (6 - 4)²)/2,
        ((3 - 5)² + (7 - 5)²)/2,
        ((4 - 6)² + (8 - 6)²)/2 ]

    = [ (4 + 4)/2, (4 + 4)/2, (4 + 4)/2, (4 + 4)/2 ]
    = [4.0, 4.0, 4.0, 4.0]
```

**步骤 3: 计算标准差 ($\sigma$)**

```
σ = √σ² = [2.0, 2.0, 2.0, 2.0]
```

**步骤 4: 进行归一化**

```
对于第一个样本：
    y₁ = [(1 - 3)/2, (2 - 4)/2, (3 - 5)/2, (4 - 6)/2]
       = [-1.0, -1.0, -1.0, -1.0]

对于第二个样本：
    y₂ = [(5 - 3)/2, (6 - 4)/2, (7 - 5)/2, (8 - 6)/2]
       = [1.0, 1.0, 1.0, 1.0]
```

**步骤 5: 应用缩放和平移 (这里 $\gamma$ 为全1, $\beta$ 为全0，所以结果不变)**

```
最终输出矩阵 Y 与归一化后的结果相同：
Y = [[-1.0, -1.0, -1.0, -1.0],
     [ 1.0,  1.0,  1.0,  1.0]]
```

**最终结果**

批归一化后的矩阵为：

```
BatchNorm(X) = [[-1.0, -1.0, -1.0, -1.0],
               [ 1.0,  1.0,  1.0,  1.0]]
```
